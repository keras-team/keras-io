# RoBERTa

Models, tokenizers, and preprocessing layers for RoBERTa,
as described in ["RoBERTa: A Robustly Optimized BERT Pretraining Approach"](https://arxiv.org/abs/1907.11692).

For a full list of available **presets**, see the
[models page](/api/keras_nlp/models).

{{toc}}
