# BERT

Models, tokenizers, and preprocessing layers for BERT,
as described in ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805).

For a full list of available **presets**, see the
[models page](/api/keras_nlp/models).

{{toc}}
