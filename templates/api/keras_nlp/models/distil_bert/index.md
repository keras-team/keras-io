# DistilBERT

Models, tokenizers, and preprocessing layers for DistilBERT,
as described in ["DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"](https://arxiv.org/abs/1910.01108).

For a full list of available **presets**, see the
[models page](/api/keras_nlp/models).

{{toc}}
