# XLM-RoBERTa

Models, tokenizers, and preprocessing layers for XLM-Roberta,
as described in ["Unsupervised Cross-lingual Representation Learning at Scale"](https://arxiv.org/abs/1911.02116).

For a full list of available **presets**, see the
[models page](/api/keras_nlp/models).

{{toc}}
