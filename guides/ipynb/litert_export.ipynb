{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Exporting Keras models to LiteRT (TensorFlow Lite)\n",
    "\n",
    "**Author:** [Rahul Kumar](https://github.com/pctablet505)<br>\n",
    "**Date created:** 2025/12/10<br>\n",
    "**Last modified:** 2025/12/10<br>\n",
    "**Description:** Complete guide to exporting Keras models for mobile and edge deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "TensorFlow Lite (LiteRT) is TensorFlow's solution for running machine learning models\n",
    "on mobile and edge devices. This guide covers everything you need to know about\n",
    "exporting Keras models to LiteRT format, including:\n",
    "\n",
    "- Basic model export\n",
    "- Different model architectures (Sequential, Functional, Subclassed)\n",
    "- Quantization for smaller models\n",
    "- Dynamic shapes support\n",
    "- Custom input signatures\n",
    "- Model validation and testing\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Keras backend to TensorFlow for LiteRT export\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Basic Model Export\n",
    "\n",
    "Let's start with a simple MNIST classifier and export it to LiteRT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a simple MNIST classifier\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Generate dummy data for demonstration\n",
    "X_train = np.random.random((1000, 28, 28))\n",
    "y_train = np.random.randint(0, 10, 1000)\n",
    "\n",
    "# Quick training (just for demonstration)\n",
    "model.fit(X_train, y_train, epochs=1, verbose=0)\n",
    "\n",
    "print(\"Model created and trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now let's export the model to LiteRT format. The `format=\"litert\"` parameter\n",
    "tells Keras to export in TensorFlow Lite format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Export to LiteRT\n",
    "model.export(\"mnist_classifier.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Model exported to mnist_classifier.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Testing the Exported Model\n",
    "\n",
    "Let's verify the exported model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load and test the exported model\n",
    "litert_available = False\n",
    "try:\n",
    "    from ai_edge_litert.interpreter import Interpreter\n",
    "\n",
    "    print(\"Using ai_edge_litert for inference\")\n",
    "    litert_available = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        from tensorflow.lite import Interpreter\n",
    "\n",
    "        print(\"Using tensorflow.lite for inference\")\n",
    "        litert_available = True\n",
    "    except ImportError:\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "\n",
    "            Interpreter = tf.lite.Interpreter\n",
    "\n",
    "            print(\"Using tf.lite.Interpreter for inference\")\n",
    "            litert_available = True\n",
    "        except (ImportError, AttributeError):\n",
    "            print(\"LiteRT interpreter not available. Skipping inference test.\")\n",
    "            print(\n",
    "                \"To test inference, install ai_edge_litert: pip install ai-edge-litert\"\n",
    "            )\n",
    "\n",
    "if litert_available:\n",
    "    interpreter = Interpreter(model_path=\"mnist_classifier.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input/output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    print(\"\\nModel Input Details:\")\n",
    "    print(f\"  Shape: {input_details[0]['shape']}\")\n",
    "    print(f\"  Type: {input_details[0]['dtype']}\")\n",
    "\n",
    "    print(\"\\nModel Output Details:\")\n",
    "    print(f\"  Shape: {output_details[0]['shape']}\")\n",
    "    print(f\"  Type: {output_details[0]['dtype']}\")\n",
    "\n",
    "    # Test inference\n",
    "    test_input = np.random.random(input_details[0][\"shape\"]).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], test_input)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    print(f\"\\nInference successful! Output shape: {output.shape}\")\n",
    "else:\n",
    "    print(\"Skipping inference test due to missing LiteRT interpreter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exporting Different Model Types\n",
    "\n",
    "Keras supports various model architectures. Let's explore how to export them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Functional API Models\n",
    "\n",
    "Functional API models offer more flexibility than Sequential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# Create functional model with multiple inputs\n",
    "input_a = Input(shape=(32,))\n",
    "input_b = Input(shape=(32,))\n",
    "\n",
    "shared_dense = Dense(64, activation=\"relu\")\n",
    "\n",
    "processed_a = shared_dense(input_a)\n",
    "processed_b = shared_dense(input_b)\n",
    "\n",
    "concatenated = concatenate([processed_a, processed_b])\n",
    "output = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "functional_model = keras.Model(inputs=[input_a, input_b], outputs=output)\n",
    "\n",
    "# Compile and export\n",
    "functional_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "functional_model.export(\"functional_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Functional model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Subclassed Models\n",
    "\n",
    "For complex architectures that require custom forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(64, activation=\"relu\")\n",
    "        self.dense2 = Dense(32, activation=\"relu\")\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "subclassed_model = CustomModel()\n",
    "subclassed_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Call the model to build it\n",
    "dummy_input = np.random.random((1, 16))\n",
    "_ = subclassed_model(dummy_input)\n",
    "\n",
    "subclassed_model.export(\"subclassed_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Subclassed model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## KerasHub Models\n",
    "\n",
    "KerasHub provides pretrained models for various tasks. Let's export some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "keras_hub_available = False\n",
    "try:\n",
    "    import keras_hub\n",
    "\n",
    "    keras_hub_available = True\n",
    "except ImportError:\n",
    "    print(\"keras-hub not available. Skipping Keras-Hub example.\")\n",
    "    print(\"To run this example, install keras-hub: pip install keras-hub\")\n",
    "\n",
    "if keras_hub_available:\n",
    "    try:\n",
    "        # Load a pretrained text model\n",
    "        # Sequence length is configured via the preprocessor\n",
    "        preprocessor = keras_hub.models.Gemma3CausalLMPreprocessor.from_preset(\n",
    "            \"gemma3_1b\", sequence_length=128\n",
    "        )\n",
    "\n",
    "        gemma_model = keras_hub.models.Gemma3CausalLM.from_preset(\n",
    "            \"gemma3_1b\", preprocessor=preprocessor, load_weights=False\n",
    "        )\n",
    "\n",
    "        # Export to LiteRT (sequence length already set)\n",
    "        gemma_model.export(\"gemma3_1b.tflite\", format=\"litert\")\n",
    "\n",
    "        print(\"Exported Keras-Hub Gemma3 1B model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load Gemma3 model: {e}\")\n",
    "        print(\"Skipping Gemma3 model export due to memory/resource constraints.\")\n",
    "\n",
    "    \"\"\"\n",
    "    For vision models, the image size is determined by the preset:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load a vision model\n",
    "        vision_model = keras_hub.models.ImageClassifier.from_preset(\n",
    "            \"resnet_50_imagenet\"\n",
    "        )\n",
    "\n",
    "        # Export (image size already set by preset)\n",
    "        vision_model.export(\"resnet.tflite\", format=\"litert\")\n",
    "\n",
    "        print(\"Exported Keras-Hub vision model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load vision model: {e}\")\n",
    "        print(\"Skipping vision model export.\")\n",
    "else:\n",
    "    print(\"Skipping Keras-Hub model export due to missing keras-hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Quantization for Smaller Models\n",
    "\n",
    "Quantization reduces model size and can improve inference speed on edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a model for quantization\n",
    "quantization_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(64, activation=\"relu\", input_shape=(784,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "quantization_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Basic quantization (reduces precision from float32 to int8)\n",
    "quantization_model.export(\n",
    "    \"model_quantized.tflite\",\n",
    "    format=\"litert\",\n",
    "    optimizations=[tf.lite.Optimize.DEFAULT],\n",
    ")\n",
    "\n",
    "print(\"Exported quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Float16 Quantization\n",
    "\n",
    "Float16 quantization offers a good balance between model size and accuracy,\n",
    "especially for GPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "quantization_model.export(\n",
    "    \"model_float16.tflite\",\n",
    "    format=\"litert\",\n",
    "    optimizations=[tf.lite.Optimize.DEFAULT],\n",
    "    target_spec={\"supported_types\": [tf.float16]},\n",
    ")\n",
    "\n",
    "print(\"Exported Float16 quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Dynamic Range Quantization\n",
    "\n",
    "Dynamic range quantization quantizes weights but keeps activations in float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "quantization_model.export(\n",
    "    \"model_dynamic_range.tflite\",\n",
    "    format=\"litert\",\n",
    "    optimizations=[tf.lite.Optimize.DEFAULT],\n",
    ")\n",
    "\n",
    "print(\"Exported dynamic range quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dynamic Shapes\n",
    "\n",
    "Dynamic shapes allow models to handle variable input sizes at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create model with dynamic batch size\n",
    "dynamic_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use None for dynamic dimensions\n",
    "dynamic_model.build((None, 784))  # None = dynamic batch size\n",
    "\n",
    "# Export with dynamic shapes\n",
    "dynamic_model.export(\"dynamic_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported model with dynamic shapes\")\n",
    "\n",
    "# Verify dynamic shapes in the exported model\n",
    "if litert_available:\n",
    "    interpreter = Interpreter(model_path=\"dynamic_model.tflite\")\n",
    "    input_details = interpreter.get_input_details()\n",
    "\n",
    "    print(f\"\\nInput shape: {input_details[0]['shape']}\")\n",
    "    print(\"Note: -1 indicates a dynamic dimension\")\n",
    "else:\n",
    "    print(\"Skipping dynamic shapes verification due to missing LiteRT interpreter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Custom Input Signatures\n",
    "\n",
    "For models with complex input requirements or multiple inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Model with custom signature using functional API\n",
    "sig_input_a = Input(shape=(10,), name=\"input_a\")\n",
    "sig_input_b = Input(shape=(10,), name=\"input_b\")\n",
    "\n",
    "# Create outputs with custom names\n",
    "sig_output1 = sig_input_a + sig_input_b  # Addition\n",
    "sig_output2 = sig_input_a * sig_input_b  # Multiplication\n",
    "\n",
    "# Create model with named inputs and outputs\n",
    "signature_model = keras.Model(\n",
    "    inputs={\"input_a\": sig_input_a, \"input_b\": sig_input_b},\n",
    "    outputs={\"output1\": sig_output1, \"output2\": sig_output2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model Validation\n",
    "\n",
    "Always verify your exported model before deploying to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_tflite_model(model_path, keras_model):\n",
    "    \"\"\"Compare TFLite model output with Keras model.\"\"\"\n",
    "    if not litert_available:\n",
    "        print(\"Skipping validation: LiteRT interpreter not available\")\n",
    "        return None\n",
    "\n",
    "    # Load TFLite model\n",
    "    interpreter = Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Generate test input\n",
    "    test_input = np.random.random((1, 28, 28)).astype(np.float32)\n",
    "\n",
    "    # Keras prediction\n",
    "    keras_output = keras_model(test_input, training=False)\n",
    "\n",
    "    # TFLite prediction\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], test_input)\n",
    "    interpreter.invoke()\n",
    "    tflite_output = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "    # Compare outputs\n",
    "    diff = np.abs(keras_output.numpy() - tflite_output).max()\n",
    "    print(f\"Maximum difference: {diff}\")\n",
    "\n",
    "    if diff < 1e-5:\n",
    "        print(\"\u2713 Model validation passed!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\u2717 Model validation failed!\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Validate our basic model\n",
    "if litert_available:\n",
    "    validate_tflite_model(\"mnist_classifier.tflite\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Advanced Export Options\n",
    "\n",
    "Keras export supports various advanced options for LiteRT conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Example with advanced options - supporting both TFLite builtins and TF ops\n",
    "model.export(\n",
    "    \"model_advanced.tflite\",\n",
    "    format=\"litert\",\n",
    "    target_spec={\n",
    "        \"supported_ops\": [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Exported model with advanced options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Test thoroughly**: Always validate exported models before deployment\n",
    "2. **Choose appropriate quantization**: Balance size vs accuracy based on your use case\n",
    "3. **Handle dynamic shapes**: Use when input sizes vary at runtime\n",
    "4. **Optimize for target hardware**: Consider GPU/CPU/NPU capabilities\n",
    "5. **Version control**: Keep track of model versions and export parameters\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "- **Import errors**: Ensure TensorFlow and ai_edge_litert are installed\n",
    "- **Shape mismatches**: Verify input shapes match model expectations\n",
    "- **Unsupported ops**: Use SELECT_TF_OPS for TensorFlow operations\n",
    "- **Memory issues**: Reduce model size with quantization\n",
    "- **Accuracy drops**: Start with float16 instead of full int8 quantization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Deploy to mobile apps using TensorFlow Lite Android/iOS SDKs\n",
    "- Optimize for specific hardware with TensorFlow Lite delegates\n",
    "- Explore model compression techniques beyond quantization\n",
    "- Consider using TensorFlow Model Optimization Toolkit for advanced optimization"
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "litert_export",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}