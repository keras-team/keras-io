{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Exporting Keras models to LiteRT (TensorFlow Lite)\n",
    "\n",
    "**Author:** [Rahul Kumar](https://github.com/pctablet505)<br>\n",
    "**Date created:** 2025/12/10<br>\n",
    "**Last modified:** 2025/12/10<br>\n",
    "**Description:** Complete guide to exporting Keras models for mobile and edge deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "LiteRT is a solution for running machine learning models\n",
    "on mobile and edge devices. This guide covers everything you need to know about\n",
    "exporting Keras models to LiteRT format, including:\n",
    "\n",
    "- Basic model export\n",
    "- Different model architectures (Sequential, Functional, Subclassed)\n",
    "- Quantization for smaller models\n",
    "- Dynamic shapes support\n",
    "- Custom input signatures\n",
    "- Model validation and testing\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Installation\n",
    "\n",
    "Install the required packages:\n",
    "```\n",
    "pip install -q keras tensorflow ai-edge-litert\n",
    "```\n",
    "\n",
    "For KerasHub models (optional):\n",
    "```\n",
    "pip install -q keras-hub\n",
    "```\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Keras backend to TensorFlow for LiteRT export\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Basic Model Export\n",
    "\n",
    "Let's start with a simple MNIST classifier and export it to LiteRT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a simple MNIST classifier\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Generate dummy data for demonstration\n",
    "x_train = np.random.random((1000, 28, 28))\n",
    "y_train = np.random.randint(0, 10, 1000)\n",
    "\n",
    "# Quick training (just for demonstration)\n",
    "model.fit(x_train, y_train, epochs=1, verbose=0)\n",
    "\n",
    "print(\"Model created and trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now let's export the model to LiteRT format. The `format=\"litert\"` parameter\n",
    "tells Keras to export in TensorFlow Lite format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Export to LiteRT\n",
    "model.export(\"mnist_classifier.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Model exported to mnist_classifier.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Testing the Exported Model\n",
    "\n",
    "Let's verify the exported model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load and test the exported model\n",
    "from ai_edge_litert.interpreter import Interpreter\n",
    "\n",
    "interpreter = Interpreter(model_path=\"mnist_classifier.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input/output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nModel Input Details:\")\n",
    "print(f\"  Shape: {input_details[0]['shape']}\")\n",
    "print(f\"  Type: {input_details[0]['dtype']}\")\n",
    "\n",
    "print(\"\\nModel Output Details:\")\n",
    "print(f\"  Shape: {output_details[0]['shape']}\")\n",
    "print(f\"  Type: {output_details[0]['dtype']}\")\n",
    "\n",
    "# Test inference\n",
    "test_input = np.random.random(input_details[0][\"shape\"]).astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0][\"index\"], test_input)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "print(f\"\\nInference successful! Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exporting Different Model Types\n",
    "\n",
    "Keras supports various model architectures. Let's explore how to export them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Functional API Models\n",
    "\n",
    "Functional API models offer more flexibility than Sequential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# Create functional model with multiple inputs\n",
    "input_a = Input(shape=(32,))\n",
    "input_b = Input(shape=(32,))\n",
    "\n",
    "shared_dense = Dense(64, activation=\"relu\")\n",
    "\n",
    "processed_a = shared_dense(input_a)\n",
    "processed_b = shared_dense(input_b)\n",
    "\n",
    "concatenated = concatenate([processed_a, processed_b])\n",
    "output = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "functional_model = keras.Model(inputs=[input_a, input_b], outputs=output)\n",
    "\n",
    "# Compile and export\n",
    "functional_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "functional_model.export(\"functional_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Functional model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Subclassed Models\n",
    "\n",
    "For complex architectures that require custom forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(64, activation=\"relu\")\n",
    "        self.dense2 = Dense(32, activation=\"relu\")\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "subclassed_model = CustomModel()\n",
    "subclassed_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Call the model to build it\n",
    "dummy_input = np.random.random((1, 16))\n",
    "_ = subclassed_model(dummy_input)\n",
    "\n",
    "subclassed_model.export(\"subclassed_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Subclassed model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## KerasHub Models\n",
    "\n",
    "KerasHub provides pretrained models for various tasks. Let's export some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "\n",
    "# Load a pretrained text model\n",
    "# Sequence length is configured via the preprocessor\n",
    "preprocessor = keras_hub.models.BertMaskedLMPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\", sequence_length=128\n",
    ")\n",
    "\n",
    "bert_model = keras_hub.models.BertMaskedLM.from_preset(\n",
    "    \"bert_tiny_en_uncased\", preprocessor=preprocessor, load_weights=False\n",
    ")\n",
    "\n",
    "# Export to LiteRT (sequence length already set)\n",
    "bert_model.export(\"bert_tiny_en_uncased.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Keras-Hub BERT Tiny model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For vision models, the image size is determined by the preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load a vision model\n",
    "vision_model = keras_hub.models.ImageClassifier.from_preset(\"resnet_50_imagenet\")\n",
    "\n",
    "# Export (image size already set by preset)\n",
    "vision_model.export(\"resnet.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Keras-Hub vision model\")\n",
    "\n",
    "# Load an object detection model\n",
    "# Image size is determined by the preset\n",
    "object_detector = keras_hub.models.ObjectDetector.from_preset(\n",
    "    \"retinanet_resnet50_fpn_coco\"\n",
    ")\n",
    "\n",
    "object_detector.export(\"detector.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Keras-Hub object detector\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Quantization for Smaller Models\n",
    "\n",
    "Quantization reduces model size and can improve inference speed on edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a model for quantization\n",
    "quantization_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(64, activation=\"relu\", input_shape=(784,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "quantization_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Export unquantized model for comparison\n",
    "quantization_model.export(\"model_unquantized.tflite\", format=\"litert\")\n",
    "print(\"Exported unquantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Dynamic Range Quantization\n",
    "\n",
    "Dynamic range quantization quantizes weights to 8-bit integers but keeps activations\n",
    "in float32. This is the default optimization when using `optimizations=[tf.lite.Optimize.DEFAULT]`.\n",
    "It provides about 4x size reduction with minimal accuracy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "quantization_model.export(\n",
    "    \"model_dynamic_range.tflite\",\n",
    "    format=\"litert\",\n",
    "    optimizations=[tf.lite.Optimize.DEFAULT],\n",
    ")\n",
    "\n",
    "print(\"Exported dynamic range quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Float16 Quantization\n",
    "\n",
    "Float16 quantization converts weights to 16-bit floating point numbers.\n",
    "It provides about 2x size reduction and is often GPU-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "quantization_model.export(\n",
    "    \"model_float16.tflite\",\n",
    "    format=\"litert\",\n",
    "    optimizations=[tf.lite.Optimize.DEFAULT],\n",
    "    target_spec={\"supported_types\": [tf.float16]},\n",
    ")\n",
    "\n",
    "print(\"Exported Float16 quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Full Integer Quantization (INT8)\n",
    "\n",
    "Full integer quantization converts both weights and activations to 8-bit integers.\n",
    "This requires a representative dataset for calibration and is ideal for\n",
    "edge devices without floating point support (e.g. microcontrollers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def representative_dataset():\n",
    "    # In practice, use real data from your validation set\n",
    "    for _ in range(100):\n",
    "        data = np.random.random((1, 784)).astype(np.float32)\n",
    "        yield [data]\n",
    "\n",
    "\n",
    "quantization_model.export(\n",
    "    \"model_int8.tflite\",\n",
    "    format=\"litert\",\n",
    "    optimizations=[tf.lite.Optimize.DEFAULT],\n",
    "    representative_dataset=representative_dataset,\n",
    "    target_spec={\"supported_ops\": [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]},\n",
    "    inference_input_type=tf.int8,\n",
    "    inference_output_type=tf.int8,\n",
    ")\n",
    "\n",
    "print(\"Exported INT8 quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return size / 1024  # Convert to KB\n",
    "\n",
    "\n",
    "print(\"\\nModel Size Comparison:\")\n",
    "print(f\"Unquantized: {get_file_size('model_unquantized.tflite'):.2f} KB\")\n",
    "print(f\"Dynamic Range: {get_file_size('model_dynamic_range.tflite'):.2f} KB\")\n",
    "print(f\"Float16: {get_file_size('model_float16.tflite'):.2f} KB\")\n",
    "print(f\"Int8: {get_file_size('model_int8.tflite'):.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dynamic Shapes\n",
    "\n",
    "Dynamic shapes allow models to handle variable input sizes at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create model with dynamic batch size\n",
    "dynamic_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use None for dynamic dimensions\n",
    "dynamic_model.build((None, 784))  # None = dynamic batch size\n",
    "\n",
    "# Export with dynamic shapes\n",
    "dynamic_model.export(\"dynamic_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported model with dynamic shapes\")\n",
    "\n",
    "# Verify dynamic shapes in the exported model\n",
    "interpreter = Interpreter(model_path=\"dynamic_model.tflite\")\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "print(f\"\\nInput shape: {input_details[0]['shape']}\")\n",
    "print(\"Note: -1 indicates a dynamic dimension\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Custom Input Signatures\n",
    "\n",
    "For models with complex input requirements or multiple inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Model with custom signature using functional API\n",
    "sig_input_a = Input(shape=(10,), name=\"input_a\")\n",
    "sig_input_b = Input(shape=(10,), name=\"input_b\")\n",
    "\n",
    "# Create outputs with custom names\n",
    "sig_output1 = sig_input_a + sig_input_b  # Addition\n",
    "sig_output2 = sig_input_a * sig_input_b  # Multiplication\n",
    "\n",
    "# Create model with named inputs and outputs\n",
    "signature_model = keras.Model(\n",
    "    inputs={\"input_a\": sig_input_a, \"input_b\": sig_input_b},\n",
    "    outputs={\"output1\": sig_output1, \"output2\": sig_output2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model Validation\n",
    "\n",
    "Always verify your exported model before deploying to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_tflite_model(model_path, keras_model):\n",
    "    \"\"\"Compare TFLite model output with Keras model.\"\"\"\n",
    "\n",
    "    # Load TFLite model\n",
    "    interpreter = Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Generate test input\n",
    "    test_input = np.random.random((1, 28, 28)).astype(np.float32)\n",
    "\n",
    "    # Keras prediction\n",
    "    keras_output = keras_model(test_input, training=False)\n",
    "\n",
    "    # TFLite prediction\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], test_input)\n",
    "    interpreter.invoke()\n",
    "    tflite_output = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "    # Compare outputs\n",
    "    np.testing.assert_allclose(keras_output.numpy(), tflite_output, atol=1e-5)\n",
    "    print(\"\u2713 Model validation passed!\")\n",
    "\n",
    "\n",
    "# Validate our basic model\n",
    "validate_tflite_model(\"mnist_classifier.tflite\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Advanced Export Options\n",
    "\n",
    "Keras export supports various advanced options for LiteRT conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Example with advanced options - supporting both TFLite builtins and TF ops\n",
    "model.export(\n",
    "    \"model_advanced.tflite\",\n",
    "    format=\"litert\",\n",
    "    target_spec={\n",
    "        \"supported_ops\": [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Exported model with advanced options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Test thoroughly**: Always validate exported models before deployment\n",
    "2. **Choose appropriate quantization**: Balance size vs accuracy based on your use case\n",
    "3. **Handle dynamic shapes**: Use when input sizes vary at runtime\n",
    "4. **Optimize for target hardware**: Consider GPU/CPU/NPU capabilities\n",
    "5. **Version control**: Keep track of model versions and export parameters\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "- **Import errors**: Ensure `tensorflow` and `ai_edge_litert` are installed.\n",
    "- **Shape mismatches**: Verify input shapes match model expectations.\n",
    "- **Unsupported ops**: Use `SELECT_TF_OPS` for TensorFlow operations:\n",
    "    ```python\n",
    "    model.export(\n",
    "        \"model.tflite\",\n",
    "        format=\"litert\",\n",
    "        target_spec={\n",
    "            \"supported_ops\": [\n",
    "                tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                tf.lite.OpsSet.SELECT_TF_OPS\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    ```\n",
    "- **Unable to infer input signature**: For Subclassed models, call the model with sample data before exporting to build it.\n",
    "- **Out of memory**: Large models may require significant RAM. Try exporting with quantization or using a machine with more RAM.\n",
    "- **Accuracy drops**: Start with float16 quantization instead of full int8 if accuracy drops significantly.\n",
    ""
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "litert_export",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}