{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# GPTQ Quantization in Keras\n",
    "\n",
    "**Author:** [Jyotinder Singh](https://x.com/Jyotinder_Singh)<br>\n",
    "**Date created:** 2025/10/16<br>\n",
    "**Last modified:** 2025/10/16<br>\n",
    "**Description:** How to run weight-only GPTQ quantization for Keras & KerasHub models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## What is GPTQ?\n",
    "\n",
    "GPTQ (\"Generative Pre-Training Quantization\") is a post-training, weight-only\n",
    "quantization method that uses a second-order approximation of the loss (via a\n",
    "Hessian estimate) to minimize the error introduced when compressing weights to\n",
    "lower precision, typically 4-bit integers.\n",
    "\n",
    "Unlike standard post-training techniques, GPTQ keeps activations in\n",
    "higher-precision and only quantizes the weights. This often preserves model\n",
    "quality in low bit-width settings while still providing large storage and\n",
    "memory savings.\n",
    "\n",
    "Keras supports GPTQ quantization for KerasHub models via the\n",
    "`keras.quantizers.GPTQConfig` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load a KerasHub model\n",
    "\n",
    "This guide uses the `Gemma3CausalLM` model from KerasHub, a small (1B\n",
    "parameter) causal language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_hub.models import Gemma3CausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "prompt = \"Keras is a\"\n",
    "\n",
    "model = Gemma3CausalLM.from_preset(\"gemma3_1b\")\n",
    "\n",
    "outputs = model.generate(prompt, max_length=30)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configure & run GPTQ quantization\n",
    "\n",
    "You can configure GPTQ quantization via the `keras.quantizers.GPTQConfig` class.\n",
    "\n",
    "The GPTQ configuration requires a calibration dataset and tokenizer, which it\n",
    "uses to estimate the Hessian and quantization error. Here, we use a small slice\n",
    "of the WikiText-2 dataset for calibration.\n",
    "\n",
    "You can tune several parameters to trade off speed, memory, and accuracy. The\n",
    "most important of these are `weight_bits` (the bit-width to quantize weights to)\n",
    "and `group_size` (the number of weights to quantize together). The group size\n",
    "controls the granularity of quantization: smaller groups typically yield better\n",
    "accuracy but are slower to quantize and may use more memory. A good starting\n",
    "point is `group_size=128` for 4-bit quantization (`weight_bits=4`).\n",
    "\n",
    "In this example, we first prepare a tiny calibration set, and then run GPTQ on\n",
    "the model using the `.quantize(...)` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Calibration slice (use a larger/representative set in practice)\n",
    "texts = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")[\"text\"]\n",
    "\n",
    "calibration_dataset = [\n",
    "    s + \".\" for text in texts for s in map(str.strip, text.split(\".\")) if s\n",
    "]\n",
    "\n",
    "gptq_config = keras.quantizers.GPTQConfig(\n",
    "    dataset=calibration_dataset,\n",
    "    tokenizer=model.preprocessor.tokenizer,\n",
    "    weight_bits=4,\n",
    "    group_size=128,\n",
    "    num_samples=256,\n",
    "    sequence_length=256,\n",
    "    hessian_damping=0.01,\n",
    "    symmetric=False,\n",
    "    activation_order=False,\n",
    ")\n",
    "\n",
    "model.quantize(\"gptq\", config=gptq_config)\n",
    "\n",
    "outputs = model.generate(prompt, max_length=30)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model Export\n",
    "\n",
    "The GPTQ quantized model can be saved to a preset and reloaded elsewhere, just\n",
    "like any other KerasHub model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.save_to_preset(\"gemma3_gptq_w4gs128_preset\")\n",
    "model_from_preset = Gemma3CausalLM.from_preset(\"gemma3_gptq_w4gs128_preset\")\n",
    "output = model_from_preset.generate(prompt, max_length=30)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Performance & Benchmarking\n",
    "\n",
    "Micro-benchmarks collected on a single NVIDIA 4070 Ti Super (16 GB).\n",
    "Baselines are FP32.\n",
    "\n",
    "Dataset: WikiText-2.\n",
    "\n",
    "\n",
    "| Model (preset)                    | Perplexity Increase % (\u2193 better) | Disk Storage Reduction \u0394 % (\u2193 better) | VRAM Reduction \u0394 % (\u2193 better) | First-token Latency \u0394 % (\u2193 better) | Throughput \u0394 % (\u2191 better) |\n",
    "| --------------------------------- | -------------------------------: | ------------------------------------: | ----------------------------: | ---------------------------------: | ------------------------: |\n",
    "| GPT2 (gpt2_base_en_cnn_dailymail) |                             1.0% |                              -50.1% \u2193 |                      -41.1% \u2193 |                            +0.7% \u2191 |                  +20.1% \u2191 |\n",
    "| OPT (opt_125m_en)                 |                            10.0% |                              -49.8% \u2193 |                      -47.0% \u2193 |                            +6.7% \u2191 |                  -15.7% \u2193 |\n",
    "| Bloom (bloom_1.1b_multi)          |                             7.0% |                              -47.0% \u2193 |                      -54.0% \u2193 |                            +1.8% \u2191 |                  -15.7% \u2193 |\n",
    "| Gemma3 (gemma3_1b)                |                             3.0% |                              -51.5% \u2193 |                      -51.8% \u2193 |                           +39.5% \u2191 |                   +5.7% \u2191 |\n",
    "\n",
    "\n",
    "Detailed benchmarking numbers and scripts are available\n",
    "[here](https://github.com/keras-team/keras/pull/21641).\n",
    "\n",
    "### Analysis\n",
    "\n",
    "There is notable reduction in disk space and VRAM usage across all models, with\n",
    "disk space savings around 50% and VRAM savings ranging from 41% to 54%. The\n",
    "reported disk savings understate the true weight compression because presets\n",
    "also include non-weight assets.\n",
    "\n",
    "Perplexity increases only marginally, indicating model quality is largely\n",
    "preserved after quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## GPTQ vs AWQ?\n",
    "\n",
    "Both GPTQ and AWQ are weight-only quantization methods that require calibration\n",
    "data. Here's how to choose between them:\n",
    "\n",
    "| Aspect | GPTQ | AWQ |\n",
    "| ------ | ---- | --- |\n",
    "| **Algorithm** | Hessian-based second-order optimization | Grid search for activation-aware scales |\n",
    "| **Quantization speed** | Slower (requires Hessian estimation) | Faster (no Hessian computation) |\n",
    "| **Bit-widths supported** | 2/3/4/8-bit | 4-bit |\n",
    "| **Accuracy** | Often slightly better on decoder LLMs | Competitive, especially on encoder models |\n",
    "| **Memory during quantization** | Higher (Hessian storage) | Lower |\n",
    "| **Calibration sensitivity** | May overfit calibration set, affecting out-of-distribution performance | Less prone to overfitting |\n",
    "\n",
    "**Choose GPTQ when:**\n",
    "\n",
    "* You need bit-widths other than 4 (e.g., 2-bit or 8-bit).\n",
    "* Maximum accuracy is critical and you can afford longer quantization time.\n",
    "* You're working with decoder-only LLMs where GPTQ may have a slight edge.\n",
    "\n",
    "**Choose AWQ when:**\n",
    "\n",
    "* You need faster quantization (AWQ is typically 2-3x faster than GPTQ).\n",
    "* Memory during quantization is constrained.\n",
    "* 4-bit is sufficient for your use case.\n",
    "* Your model will be used on diverse/out-of-distribution data (AWQ is less prone to overfitting on calibration data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Practical tips\n",
    "\n",
    "* GPTQ is a post-training technique; training after quantization is not supported.\n",
    "* Always use the model's own tokenizer for calibration.\n",
    "* Use a representative calibration set; small slices are only for demos.\n",
    "* Start with W4 group_size=128; tune per model/task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gptq_quantization_in_keras",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}