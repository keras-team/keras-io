{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# INT4 Quantization in Keras\n",
    "\n",
    "**Author:** [Jyotinder Singh](https://x.com/Jyotinder_Singh)<br>\n",
    "**Date created:** 2025/10/14<br>\n",
    "**Last modified:** 2025/10/14<br>\n",
    "**Description:** Complete guide to using INT4 quantization in Keras and KerasHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## What is INT4 quantization?\n",
    "\n",
    "Quantization lowers the numerical precision of weights and activations to reduce memory use\n",
    "and often speed up inference, at the cost of a small accuracy drop. INT4 post-training\n",
    "quantization (PTQ) stores model weights in 4-bit signed integers and dynamically quantizes\n",
    "activations to 8-bit at runtime (a W4A8 scheme). Compared with FP32 this can shrink weight\n",
    "storage ~8x (2x vs INT8) while retaining acceptable accuracy for many encoder models and\n",
    "some decoder models. Compute still leverages widely available NVIDIA INT8 Tensor Cores.\n",
    "\n",
    "4-bit is a more aggressive compression than 8-bit and may induce larger quality regressions,\n",
    "especially for large autoregressive language models.\n",
    "\n",
    "## How it works\n",
    "\n",
    "Quantization maps real values to 4-bit integers with a scale:\n",
    "\n",
    "1. Per-output-channel scale computed for each weight matrix (symmetric abs-max).\n",
    "2. Weights are quantized to values in `[-8, 7]` (4 bits) and packed two-per-byte.\n",
    "3. At inference, activations are dynamically scaled and quantized to INT8.\n",
    "4. Packed INT4 weights are unpacked to an INT8 tensor (still with INT4-range values).\n",
    "5. INT8 x INT8 matmul accumulates in INT32.\n",
    "6. Result is dequantized using `(input_scale * per_channel_kernel_scale)`.\n",
    "\n",
    "This mirrors the INT8 path described in the\n",
    "[INT8 guide](https://keras.io/guides/int8_quantization_in_keras) with some added unpack\n",
    "overhead for stronger compression.\n",
    "\n",
    "## Benefits\n",
    "* Memory / bandwidth bound models: When the implementation spends most of its time on memory I/O,\n",
    "  reducing the computation time does not reduce its overall runtime. INT4 reduces bytes\n",
    "  moved by ~8x vs FP32, improving cache behavior and reducing memory stalls;\n",
    "  this often helps more than increasing raw FLOPs.\n",
    "* Accuracy: Many architectures retain acceptable accuracy with INT4; encoder-only models\n",
    "  often fare better than decoder LLMs. Always validate on your own dataset.\n",
    "* Compute bound layers on supported hardware: 4-bit kernels are unpacked to INT8 at inference,\n",
    "  therefore, on NVIDIA GPUs, INT8 [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)\n",
    "  speed up matmul/conv, boosting throughput on compute-limited layers.\n",
    "\n",
    "### What Keras does in INT4 mode\n",
    "\n",
    "* **Mapping**: Symmetric, linear quantization with INT4 plus a floating-point scale.\n",
    "* **Weights**: per-output-channel scales to preserve accuracy.\n",
    "* **Activations**: **dynamic AbsMax** scaling computed at runtime.\n",
    "* **Graph rewrite**: Quantization is applied after weights are trained and built; the graph\n",
    "  is rewritten so you can run or save immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This guide shows how to use 4-bit (W4A8) post-training quantization in Keras:\n",
    "\n",
    "1. [Quantizing a minimal functional model](#quantizing-a-minimal-functional-model)\n",
    "2. [Saving and reloading a quantized model](#saving-and-reloading-a-quantized-model)\n",
    "3. [Quantizing a KerasHub model](#quantizing-a-kerashub-model)\n",
    "4. [When to use INT4 vs INT8](#when-should-i-use-int4-vs-int8)\n",
    "5. [Performance benchmarks](#performance--benchmarking)\n",
    "6. [Practical Tips](#practical-tips)\n",
    "7. [Limitations](#limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Quantizing a Minimal Functional Model\n",
    "\n",
    "Below we build a small functional model, capture a baseline output, quantize to INT4\n",
    "in place, and compare outputs with an MSE metric. (For real evaluation use your\n",
    "validation metric.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# Create a random number generator.\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Create a simple functional model.\n",
    "inputs = keras.Input(shape=(10,))\n",
    "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(1, name=\"target\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Baseline output with full-precision weights.\n",
    "x_eval = rng.random((32, 10)).astype(\"float32\")\n",
    "y_fp32 = model(x_eval)\n",
    "\n",
    "\n",
    "# Quantize the model in-place to INT4 (W4A8).\n",
    "model.quantize(\"int4\")\n",
    "\n",
    "# Compare outputs (MSE).\n",
    "y_int4 = model(x_eval)\n",
    "mse = keras.ops.mean(keras.ops.square(y_fp32 - y_int4))\n",
    "print(\"Full-Precision vs INT4 MSE:\", float(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The INT4 quantized model usually produces outputs close enough for many downstream\n",
    "tasks. Expect larger deltas than INT8, so always validate on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Saving and Reloading a Quantized Model\n",
    "\n",
    "You can use standard Keras saving / loading APIs. Quantization metadata (including\n",
    "scales and packed weights) is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Save the quantized model and reload to verify round-trip.\n",
    "model.save(\"int4.keras\")\n",
    "int4_reloaded = keras.saving.load_model(\"int4.keras\")\n",
    "y_int4_reloaded = int4_reloaded(x_eval)\n",
    "\n",
    "# Compare outputs (MSE).\n",
    "roundtrip_mse = keras.ops.mean(keras.ops.square(y_fp32 - y_int4_reloaded))\n",
    "print(\"MSE (INT4 vs reloaded INT4):\", float(roundtrip_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Quantizing a KerasHub Model\n",
    "\n",
    "All KerasHub models support the `.quantize(...)` API for post-training quantization,\n",
    "and follow the same workflow as above.\n",
    "\n",
    "In this example, we will:\n",
    "\n",
    "1. Load the [gemma3_1b](https://www.kaggle.com/models/keras/gemma3/keras/gemma3_1b)\n",
    "  preset from KerasHub\n",
    "2. Generate text using both the full-precision and quantized models, and compare outputs.\n",
    "3. Save both models to disk and compute storage savings.\n",
    "4. Reload the INT4 model and verify output consistency with the original quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras_hub.models import Gemma3CausalLM\n",
    "\n",
    "# Load a Gemma3 preset from KerasHub.\n",
    "gemma3 = Gemma3CausalLM.from_preset(\"gemma3_1b\")\n",
    "\n",
    "# Generate with full-precision weights.\n",
    "fp_output = gemma3.generate(\"Keras is a\", max_length=30)\n",
    "print(\"Full-precision output:\", fp_output)\n",
    "\n",
    "# Save the full-precision model to a preset.\n",
    "gemma3.save_to_preset(\"gemma3_fp32\")\n",
    "\n",
    "# Quantize to INT4.\n",
    "gemma3.quantize(\"int4\")\n",
    "\n",
    "# Generate with INT4 weights.\n",
    "output = gemma3.generate(\"Keras is a\", max_length=30)\n",
    "print(\"Quantized output:\", output)\n",
    "\n",
    "# Save INT4 model to a new preset.\n",
    "gemma3.save_to_preset(\"gemma3_int4\")\n",
    "\n",
    "# Reload and compare outputs\n",
    "gemma3_int4 = Gemma3CausalLM.from_preset(\"gemma3_int4\")\n",
    "\n",
    "output = gemma3_int4.generate(\"Keras is a\", max_length=30)\n",
    "print(\"Quantized reloaded output:\", output)\n",
    "\n",
    "\n",
    "# Compute storage savings\n",
    "def bytes_to_mib(n):\n",
    "    return n / (1024**2)\n",
    "\n",
    "\n",
    "gemma_fp32_size = os.path.getsize(\"gemma3_fp32/model.weights.h5\")\n",
    "gemma_int4_size = os.path.getsize(\"gemma3_int4/model.weights.h5\")\n",
    "\n",
    "gemma_reduction = 100.0 * (1.0 - (gemma_int4_size / max(gemma_fp32_size, 1)))\n",
    "print(f\"Gemma3: FP32 file size: {bytes_to_mib(gemma_fp32_size):.2f} MiB\")\n",
    "print(f\"Gemma3: INT4 file size: {bytes_to_mib(gemma_int4_size):.2f} MiB\")\n",
    "print(f\"Gemma3: Size reduction: {gemma_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Performance & Benchmarking\n",
    "\n",
    "Micro-benchmarks collected on a single NVIDIA L4 (22.5 GB). Baselines are FP32.\n",
    "\n",
    "### Text Classification (DistilBERT Base on SST-2)\n",
    "\n",
    "<img class=\"k-inline-icon\" src=\"https://colab.research.google.com/img/colab_favicon.ico\"/> [**View in Colab**](https://colab.research.google.com/gist/JyotinderSingh/77e874187d6da3f8280c053192f78c06/int4-quantization-micro-benchmark-distilbert.ipynb)\n",
    "\n",
    "| Metric | FP32 | INT4 | Change |\n",
    "| ------ | ---- | ---- | ------ |\n",
    "| Accuracy (↑) | 91.06% | 90.14% | -0.92pp |\n",
    "| Model Size (MB, ↓) | 255.86 | 159.49 | -37.67% |\n",
    "| Peak GPU Memory (MiB, ↓) | 1554.00 | 1243.26 | -20.00% |\n",
    "| Latency (ms/sample, ↓) | 6.43 | 5.73 | -10.83% |\n",
    "| Throughput (samples/s, ↑) | 155.60 | 174.50 | +12.15% |\n",
    "\n",
    "**Analysis**: Accuracy drop is modest (<1pp) with notable speed and memory gains;\n",
    "encoder-only models tend to retain fidelity under heavier weight compression.\n",
    "\n",
    "### Text Generation (Falcon 1B)\n",
    "\n",
    "<img class=\"k-inline-icon\" src=\"https://colab.research.google.com/img/colab_favicon.ico\"/> [**View in Colab**](https://colab.research.google.com/gist/JyotinderSingh/19ab238e0f5b29ae24c0faf4128e7d7e/int4_quantization_micro_benchmark_falcon.ipynb)\n",
    "\n",
    "| Metric | FP32 | INT4 | Change |\n",
    "| ------ | ---- | ---- | ------ |\n",
    "| Perplexity (↓) | 7.44 | 9.98 | +34.15% |\n",
    "| Model Size (GB, ↓) | 4.8884 | 0.9526 | -80.51% |\n",
    "| Peak GPU Memory (MiB, ↓) | 8021.12 | 5483.46 | -31.64% |\n",
    "| First Token Latency (ms, ↓) | 128.87 | 122.50 | -4.95% |\n",
    "| Sequence Latency (ms, ↓) | 338.29 | 181.93 | -46.22% |\n",
    "| Token Throughput (tokens/s, ↑) | 174.41 | 256.96 | +47.33% |\n",
    "\n",
    "**Analysis**: INT4 gives large size (-80%) and memory (-32%) reductions. Perplexity\n",
    "increases (expected for aggressive compression) yet sequence latency drops and\n",
    "throughput rises ~50%.\n",
    "\n",
    "### Text Generation (Gemma3 1B)\n",
    "\n",
    "<img class=\"k-inline-icon\" src=\"https://colab.research.google.com/img/colab_favicon.ico\"/> [**View in Colab**](https://colab.research.google.com/gist/JyotinderSingh/9ca7813971868d5d1a16cd7998d0e352/int4_quantization_micro_benchmark_gemma3.ipynb)\n",
    "\n",
    "| Metric | FP32 | INT4 | Change |\n",
    "| ------ | ---- | ---- | ------ |\n",
    "| Perplexity (↓) | 6.17 | 10.46 | +69.61% |\n",
    "| Model Size (GB, ↓) | 3.7303 | 1.4576 | -60.92% |\n",
    "| Peak GPU Memory (MiB, ↓) | 6844.67 | 5008.14 | -26.83% |\n",
    "| First Token Latency (ms, ↓) | 57.42 | 64.21 | +11.83% |\n",
    "| Sequence Latency (ms, ↓) | 239.78 | 161.18 | -32.78% |\n",
    "| Token Throughput (tokens/s, ↑) | 246.06 | 366.05 | +48.76% |\n",
    "\n",
    "**Analysis**: INT4 gives large size (-61%) and memory (-27%) reductions. Perplexity\n",
    "increases (expected for aggressive compression) yet sequence latency drops and\n",
    "throughput rises ~50%.\n",
    "\n",
    "### Text Generation (Llama 3.2 1B)\n",
    "\n",
    "<img class=\"k-inline-icon\" src=\"https://colab.research.google.com/img/colab_favicon.ico\"/> [**View in Colab**](https://colab.research.google.com/gist/JyotinderSingh/310f50a0ca0eba3754de41c612b3b8ef/int4_quantization_micro_benchmark_llama3.ipynb)\n",
    "\n",
    "| Metric | FP32 | INT4 | Change |\n",
    "| ------ | ---- | ---- | ------ |\n",
    "| Perplexity (↓) | 6.38 | 14.16 | +121.78% |\n",
    "| Model Size (GB, ↓) | 5.5890 | 2.4186 | -56.73% |\n",
    "| Peak GPU Memory (MiB, ↓) | 9509.49 | 6810.26 | -28.38% |\n",
    "| First Token Latency (ms, ↓) | 209.41 | 219.09 | +4.62% |\n",
    "| Sequence Latency (ms, ↓) | 322.33 | 262.15 | -18.67% |\n",
    "| Token Throughput (tokens/s, ↑) | 183.82 | 230.78 | +25.55% |\n",
    "\n",
    "**Analysis**: INT4 gives large size (-57%) and memory (-28%) reductions. Perplexity\n",
    "increases (expected for aggressive compression) yet sequence latency drops and\n",
    "throughput rises ~25%.\n",
    "\n",
    "### Text Generation (OPT 125M)\n",
    "\n",
    "<img class=\"k-inline-icon\" src=\"https://colab.research.google.com/img/colab_favicon.ico\"/> [**View in Colab**](https://colab.research.google.com/gist/JyotinderSingh/918fcdb8a1433dea12800f8ca4a240f5/int4_quantization_micro_benchmark_opt.ipynb)\n",
    "\n",
    "| Metric | FP32 | INT4 | Change |\n",
    "| ------ | ---- | ---- | ------ |\n",
    "| Perplexity (↓) | 13.85 | 21.02 | +51.79% |\n",
    "| Model Size (MB, ↓) | 468.3 | 284.0 | -39.37% |\n",
    "| Peak GPU Memory (MiB, ↓) | 1007.23 | 659.28 | -34.54% |\n",
    "| First Token Latency (ms/sample, ↓) | 95.79 | 97.87 | +2.18% |\n",
    "| Sequence Latency (ms/sample, ↓) | 60.35 | 54.64 | -9.46% |\n",
    "| Throughput (samples/s, ↑) | 973.41 | 1075.15 | +10.45% |\n",
    "\n",
    "**Analysis**: INT4 gives large size (-39%) and memory (-35%) reductions. Perplexity\n",
    "increases (expected for aggressive compression) yet sequence latency drops and\n",
    "throughput rises ~10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## When should I use INT4 vs INT8?\n",
    "\n",
    "| Goal / Constraint | Prefer INT8 | Prefer INT4 (W4A8) |\n",
    "| ----------------- | ----------- | ------------------ |\n",
    "| Minimal accuracy drop critical | ✔︎ |  |\n",
    "| Maximum compression (disk / RAM) |  | ✔︎ |\n",
    "| Bandwidth-bound inference | Possible | Often better |\n",
    "| Decoder LLM | ✔︎ | Try with eval first |\n",
    "| Encoder / classification models | ✔︎ | ✔︎ |\n",
    "| Available kernels / tooling maturity | ✔︎ | Emerging |\n",
    "\n",
    "* Start with INT8; if memory or distribution size is still a bottleneck, evaluate INT4.\n",
    "* For LLMs, measure task-specific metrics (perplexity, exact match, etc.) after INT4.\n",
    "* Combine INT4 weights + LoRA adapters for efficient fine-tuning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Practical Tips\n",
    "\n",
    "* Post-training quantization (PTQ) is a one-time operation; you cannot train a model\n",
    "  after quantizing it to INT4.\n",
    "* Always materialize weights before quantization (e.g., `build()` or a forward pass).\n",
    "* Evaluate on a representative validation set; track task metrics, not just MSE.\n",
    "* Use LoRA for further fine-tuning.\n",
    "\n",
    "## Limitations\n",
    "* Runtime unpack adds overhead (weights are decompressed layer-wise for each forward pass).\n",
    "* Large compression leads to accuracy drop (especially for decoder-only LLMs).\n",
    "* LoRA export path is lossy (dequantize -> add delta -> requantize).\n",
    "* Keras does not yet support native fused INT4 kernels; relies on unpack + INT8 matmul."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "int4_quantization_in_keras",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
