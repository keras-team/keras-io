{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Orbax Checkpointing in Keras\n",
    "\n",
    "**Author:** [Samaneh Saadat](https://github.com/SamanehSaadat/)<br>\n",
    "**Date created:** 2025/08/20<br>\n",
    "**Last modified:** 2025/08/20<br>\n",
    "**Description:** A guide on how to save Orbax checkpoints during model training with the JAX backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Orbax is the default checkpointing library recommended for JAX ecosystem\n",
    "users. It is a high-level checkpointing library which provides functionality\n",
    "for both checkpoint management and composable and extensible serialization.\n",
    "This guide explains how to do Orbax checkpointing when training a model in\n",
    "the JAX backend.\n",
    "\n",
    "Note that you should use Orbax checkpointing for multi-host training using\n",
    "Keras distribution API as the default Keras checkpointing currently does not\n",
    "support multi-host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing Orbax checkpointing library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U orbax-checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We need to set the Keras backend to JAX as this guide is intended for the\n",
    "JAX backend. Then we import Keras and other libraries needed including the\n",
    "Orbax checkpointing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import orbax.checkpoint as ocp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Orbax Callback\n",
    "\n",
    "We need to create two main utilities to manage Orbax checkpointing in Keras:\n",
    "1. `KerasOrbaxCheckpointManager`: A wrapper around\n",
    "   `orbax.checkpoint.CheckpointManager` for Keras models.\n",
    "   `KerasOrbaxCheckpointManager` uses `Model`'s `get_state_tree` and\n",
    "   `set_state_tree` APIs to save and restore the model variables.\n",
    "2. `OrbaxCheckpointCallback`: A Keras callback that uses\n",
    "   `KerasOrbaxCheckpointManager` to automatically save and restore model states\n",
    "   during training.\n",
    "\n",
    "Orbax checkpointing in Keras is as simple as copying these utilities to your\n",
    "own codebase and passing `OrbaxCheckpointCallback` to the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class KerasOrbaxCheckpointManager(ocp.CheckpointManager):\n",
    "    \"\"\"A wrapper over Orbax CheckpointManager for Keras with the JAX\n",
    "    backend.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        checkpoint_dir,\n",
    "        max_to_keep=5,\n",
    "        steps_per_epoch=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize the Keras Orbax Checkpoint Manager.\n",
    "\n",
    "        Args:\n",
    "            model: The Keras model to checkpoint.\n",
    "            checkpoint_dir: Directory path where checkpoints will be saved.\n",
    "            max_to_keep: Maximum number of checkpoints to keep in the directory.\n",
    "                Default is 5.\n",
    "            steps_per_epoch: Number of steps per epoch. Default is 1.\n",
    "            **kwargs: Additional keyword arguments to pass to Orbax's\n",
    "                CheckpointManagerOptions.\n",
    "        \"\"\"\n",
    "        options = ocp.CheckpointManagerOptions(\n",
    "            max_to_keep=max_to_keep, enable_async_checkpointing=False, **kwargs\n",
    "        )\n",
    "        self._model = model\n",
    "        self._steps_per_epoch = steps_per_epoch\n",
    "        self._checkpoint_dir = checkpoint_dir\n",
    "        super().__init__(checkpoint_dir, options=options)\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Gets the model state and metrics.\n",
    "\n",
    "        This method retrieves the complete state tree from the model and separates\n",
    "        the metrics variables from the rest of the state.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - state: A dictionary containing the model's state (weights, optimizer state, etc.)\n",
    "                - metrics: The model's metrics variables, if any\n",
    "        \"\"\"\n",
    "        state = self._model.get_state_tree().copy()\n",
    "        metrics = state.pop(\"metrics_variables\", None)\n",
    "        return state, metrics\n",
    "\n",
    "    def save_state(self, epoch):\n",
    "        \"\"\"Saves the model to the checkpoint directory.\n",
    "\n",
    "        Args:\n",
    "          epoch: The epoch number at which the state is saved.\n",
    "        \"\"\"\n",
    "        state, metrics_value = self._get_state()\n",
    "        self.save(\n",
    "            epoch * self._steps_per_epoch,\n",
    "            args=ocp.args.StandardSave(item=state),\n",
    "            metrics=metrics_value,\n",
    "        )\n",
    "\n",
    "    def restore_state(self, step=None):\n",
    "        \"\"\"Restores the model from the checkpoint directory.\n",
    "\n",
    "        Args:\n",
    "          step: The step number to restore the state from. Default=None\n",
    "            restores the latest step.\n",
    "        \"\"\"\n",
    "        step = step or self.latest_step()\n",
    "        if step is None:\n",
    "            return\n",
    "        # Restore the model state only, not metrics.\n",
    "        state, _ = self._get_state()\n",
    "        restored_state = self.restore(step, args=ocp.args.StandardRestore(item=state))\n",
    "        self._model.set_state_tree(restored_state)\n",
    "\n",
    "\n",
    "class OrbaxCheckpointCallback(keras.callbacks.Callback):\n",
    "    \"\"\"A callback for checkpointing and restoring state using Orbax.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        checkpoint_dir,\n",
    "        max_to_keep=5,\n",
    "        steps_per_epoch=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize the Orbax checkpoint callback.\n",
    "\n",
    "        Args:\n",
    "            model: The Keras model to checkpoint.\n",
    "            checkpoint_dir: Directory path where checkpoints will be saved.\n",
    "            max_to_keep: Maximum number of checkpoints to keep in the directory.\n",
    "                Default is 5.\n",
    "            steps_per_epoch: Number of steps per epoch. Default is 1.\n",
    "            **kwargs: Additional keyword arguments to pass to Orbax's\n",
    "                CheckpointManagerOptions.\n",
    "        \"\"\"\n",
    "        if keras.config.backend() != \"jax\":\n",
    "            raise ValueError(\n",
    "                f\"`OrbaxCheckpointCallback` is only supported on a \"\n",
    "                f\"`jax` backend. Provided backend is {keras.config.backend()}.\"\n",
    "            )\n",
    "        self._checkpoint_manager = KerasOrbaxCheckpointManager(\n",
    "            model, checkpoint_dir, max_to_keep, steps_per_epoch, **kwargs\n",
    "        )\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not self.model.built or not self.model.optimizer.built:\n",
    "            raise ValueError(\n",
    "                \"To use `OrbaxCheckpointCallback`, your model and \"\n",
    "                \"optimizer must be built before you call `fit()`.\"\n",
    "            )\n",
    "        latest_epoch = self._checkpoint_manager.latest_step()\n",
    "        if latest_epoch is not None:\n",
    "            print(\"Load Orbax checkpoint on_train_begin.\")\n",
    "            self._checkpoint_manager.restore_state(step=latest_epoch)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Save Orbax checkpoint on_epoch_end.\")\n",
    "        self._checkpoint_manager.save_state(epoch)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## An Orbax checkpointing example\n",
    "\n",
    "Let's look at how we can use `OrbaxCheckpointCallback` to save Orbax\n",
    "checkpoints during the training. To get started, let's define a simple model\n",
    "and a toy training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    # Create a simple model.\n",
    "    inputs = keras.Input(shape=(32,))\n",
    "    outputs = keras.layers.Dense(1, name=\"dense\")(inputs)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss=\"mean_squared_error\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "x_train = np.random.random((128, 32))\n",
    "y_train = np.random.random((128, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then, we create an Orbax checkpointing callback and pass it to the\n",
    "`callbacks` argument in the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "orbax_callback = OrbaxCheckpointCallback(\n",
    "    model,\n",
    "    checkpoint_dir=\"/tmp/ckpt\",\n",
    "    max_to_keep=1,\n",
    "    steps_per_epoch=1,\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    verbose=0,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[orbax_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now if you look at the Orbax checkpoint directory, you can see all the files\n",
    "saved as part of Orbax checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!ls -R /tmp/ckpt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "orbax_checkpoint",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}