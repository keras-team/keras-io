{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Writing Quantization-Compatible Layers in Keras\n",
    "\n",
    "**Author:** [Jyotinder Singh](https://x.com/Jyotinder_Singh)<br>\n",
    "**Date created:** 2025/10/16<br>\n",
    "**Last modified:** 2025/10/16<br>\n",
    "**Description:** Complete guide for writing quantization-compatible Keras layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## What are quantization-compatible layers?\n",
    "\n",
    "Keras lets you optimize models via post-training quantization (PTQ) by calling\n",
    "the `layer.quantize(...)` or `model.quantize(...)` APIs. Keras exposes an\n",
    "extensible framework for defining quantization-compatible layers. This lets you\n",
    "author custom layers that plug into the quantization framework, can be quantized\n",
    "to INT8 or INT4, and saved/loaded with quantization metadata.\n",
    "\n",
    "A quantization-compatible layer needs to implement a few hooks, so that it can:\n",
    "\n",
    "- Switch its variables to quantized representations.\n",
    "- Use a quantization-aware forward path at inference.\n",
    "- Save and load quantization metadata with the model.\n",
    "\n",
    "In this guide, we'll implement a simple layer that supports INT8 PTQ. The same\n",
    "patterns generalize to INT4 quantization and FP8 mixed-precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## The hooks you'll implement\n",
    "\n",
    "At minimum, your layer should define:\n",
    "\n",
    "- `quantize(mode, **kwargs)`: Converts existing variables to quantized form and\n",
    "    switches the dtype policy\n",
    "- `_int8_build(...)`: Allocates INT8 variables needed by your layer\n",
    "- `_int8_call(inputs, training=None)`: Minimal INT8 forward path\n",
    "\n",
    "We'll implement these for a very small layer called `SimpleScale`, which\n",
    "multiplies the inputs by a trainable per-feature vector (elementwise scaling on\n",
    "the last dimension). The same patterns generalize to more sophisticated layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Writing a Simple Quantization-Compatible Layer\n",
    "\n",
    "We start with a tiny layer that learns a per-feature multiplier. The\n",
    "full-precision path just computes `y = x * w`. We'll add the quantization hooks\n",
    "step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import ops, quantizers, dtype_policies\n",
    "from keras.layers import Layer, Input\n",
    "\n",
    "\n",
    "class SimpleScale(Layer):\n",
    "    \"\"\"A layer that learns a per-feature scaling factor.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self._kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_dim,),\n",
    "            initializer=\"random_uniform\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return ops.multiply(inputs, self._kernel)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The `quantize()` method\n",
    "\n",
    "PTQ is a one-time rewrite. After you train or load your FP32 layer, you call\n",
    "`layer.quantize(\"int8\")`. The layer should:\n",
    "\n",
    "1. Read its existing full-precision variables (e.g., `self._kernel`).\n",
    "2. Quantize them to INT8 values plus a quantization scale.\n",
    "3. Replace full-precision variables with INT8 storage and assign the quantized\n",
    "  data.\n",
    "4. Switch the `dtype_policy` to a quantized variant (e.g., `int8_from_float32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def quantize(self, mode, **kwargs):\n",
    "    if mode != \"int8\":\n",
    "        raise NotImplementedError(f\"Unsupported quantization mode: {mode}\")\n",
    "\n",
    "    quantized_kernel, scale = quantizers.abs_max_quantize(\n",
    "        self._kernel, axis=0, dtype=\"int8\", to_numpy=True\n",
    "    )\n",
    "    scale = ops.squeeze(scale, axis=0)\n",
    "\n",
    "    kernel_shape = self._kernel.shape\n",
    "\n",
    "    del self._kernel\n",
    "\n",
    "    # Allocate INT8 variables. Discussed in the next section.\n",
    "    self._int8_build(kernel_shape)\n",
    "\n",
    "    self._kernel.assign(quantized_kernel)\n",
    "    self.scale.assign(scale)\n",
    "\n",
    "    # `_is_quantized` should be set before changing dtype policy to inform\n",
    "    # the setter that quantized variables are initialized.\n",
    "    self._is_quantized = True\n",
    "\n",
    "    if self.dtype_policy.quantization_mode is None:\n",
    "        policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n",
    "        self.dtype_policy = policy\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Note\n",
    "\n",
    "1. The `quantize(...)` method should validate `mode` and raise a\n",
    "  `NotImplementedError` if the mode is not supported.\n",
    "2. Ensure your `quantize(...)` sets a quantized dtype policy based on the\n",
    "  prior policy, e.g., `int8_from_float32` or `int8_from_bfloat16`. This ensures\n",
    "  that the layer's `quantization_mode` is correctly set.\n",
    "\n",
    "3. The `_is_quantized` flag should be set before changing the dtype policy to\n",
    "  inform the setter that quantized variables are initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The `_int8_build(...)` method\n",
    "\n",
    "This `int8_build(...)` method is called from `quantize(...)` to initialize the\n",
    "INT8 variables. It should allocate:\n",
    "\n",
    "- `self._kernel` as an INT8 vector of shape `(input_dim,)` (the same shape as\n",
    "  the original full-precision kernel).\n",
    "- `self.scale` as the scalar quantization scale in the layer's variable dtype,\n",
    "  which is FP32 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _int8_build(self, kernel_shape):\n",
    "    (input_dim,) = kernel_shape\n",
    "    self._kernel = self.add_weight(\n",
    "        name=\"kernel\",\n",
    "        shape=(input_dim,),\n",
    "        initializer=\"zeros\",\n",
    "        dtype=\"int8\",\n",
    "        trainable=False,\n",
    "    )\n",
    "    self.scale = self.add_weight(\n",
    "        name=\"scale\",\n",
    "        initializer=\"ones\",\n",
    "        trainable=False,\n",
    "    )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Note\n",
    "\n",
    "1. INT8 variables should be created with `trainable=False`, as quantized parameters\n",
    "  are not meant to be updated during training. Subsequent fine-tuning should not\n",
    "  alter these quantized variables.\n",
    "2. If you support INT4 quantization, implement a similar `_int4_build(...)`\n",
    "  method that allocates packed INT4 storage (often packed into INT8) plus\n",
    "  per-feature scales. The original unpacked dimensions and packing axis should\n",
    "  be recorded as instance variables for use in the call path. A reference\n",
    "  implementation is available in the Keras\n",
    "  [Dense](https://github.com/keras-team/keras/blob/3c3d6adc08db627d89b5ad5e7f9b0ba3e88f2641/keras/src/layers/core/dense.py#L481-L512)\n",
    "  layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The `_int8_call(...)` method\n",
    "\n",
    "The `_int8_call(...)` method implements a minimal INT8 forward path. It uses the\n",
    "quantized variables allocated in `_int8_build(...)` and de-scales the output\n",
    "back to floating-point.\n",
    "\n",
    "The base `keras.Layer` class automatically dispatches to this method when the\n",
    "layer is quantized, without requiring you to wire it up manually.\n",
    "\n",
    "The INT8 path mirrors the float computation `y = x * w` but performs:\n",
    "\n",
    "1. Elementwise multiply using the quantized weight.\n",
    "2. De-scale back to float by dividing with the `scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _int8_call(self, inputs, training=None):\n",
    "    x = ops.multiply(inputs, self._kernel)\n",
    "    x = ops.divide(x, self.scale)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Complete `SimpleScale` class with hooks\n",
    "\n",
    "Below is the full class definition that incorporates the all the hooks shown above (`quantize`, `_int8_build`,\n",
    "`_int8_call`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleScale(Layer):\n",
    "    \"\"\"A layer that learns a per-feature scaling factor.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self._kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_dim,),\n",
    "            initializer=\"random_uniform\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return ops.multiply(inputs, self._kernel)\n",
    "\n",
    "    def quantize(self, mode, **kwargs):\n",
    "        if mode != \"int8\":\n",
    "            raise NotImplementedError(f\"Unsupported quantization mode: {mode}\")\n",
    "\n",
    "        quantized_kernel, scale = quantizers.abs_max_quantize(\n",
    "            self._kernel, axis=0, dtype=\"int8\", to_numpy=True\n",
    "        )\n",
    "        scale = ops.squeeze(scale, axis=0)\n",
    "\n",
    "        kernel_shape = self._kernel.shape\n",
    "\n",
    "        del self._kernel\n",
    "\n",
    "        self._int8_build(kernel_shape)\n",
    "\n",
    "        self._kernel.assign(quantized_kernel)\n",
    "        self.scale.assign(scale)\n",
    "\n",
    "        self._is_quantized = True\n",
    "\n",
    "        if self.dtype_policy.quantization_mode is None:\n",
    "            policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n",
    "            self.dtype_policy = policy\n",
    "\n",
    "    def _int8_build(self, kernel_shape):\n",
    "        (input_dim,) = kernel_shape\n",
    "        self._kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_dim,),\n",
    "            initializer=\"zeros\",\n",
    "            dtype=\"int8\",\n",
    "            trainable=False,\n",
    "        )\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\",\n",
    "            initializer=\"ones\",\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    def _int8_call(self, inputs, training=None):\n",
    "        x = ops.multiply(inputs, self._kernel)\n",
    "        x = ops.divide(x, self.scale)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Try it: quantize and run a forward pass\n",
    "\n",
    "Below we build the layer, then quantize to INT8 and call it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Sample inputs\n",
    "rng = np.random.default_rng()\n",
    "x = rng.random((2, 4)).astype(\"float32\")\n",
    "\n",
    "layer = SimpleScale()\n",
    "\n",
    "# Forward pass in float\n",
    "y_fp = layer(x)\n",
    "\n",
    "# Quantize to INT8 and run again\n",
    "layer.quantize(\"int8\")\n",
    "y_int8 = layer(x)\n",
    "\n",
    "print(\"SimpleScale FP32 sample:\", y_fp[0].numpy())\n",
    "print(\"SimpleScale INT8 sample:\", y_int8[0].numpy())\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Extending to INT4\n",
    "\n",
    "If you want to support INT4 quantization, add:\n",
    "\n",
    "- `_int4_build(...)`: allocate a packed 4-bit storage (often packed into int8) plus per-feature scales\n",
    "- `_int4_call(...)`: unpack at runtime and follow the same de-scale pattern\n",
    "- `quantize(\"int4\")`: quantize weights with `value_range=(-8, 7)`, then pack to int4 storage\n",
    "\n",
    "See the\n",
    "[Dense](https://github.com/keras-team/keras/blob/3c3d6adc08db627d89b5ad5e7f9b0ba3e88f2641/keras/src/layers/core/dense.py#L602-L653)\n",
    "reference for a complete packed int4 example, including how to track and use the\n",
    "original (unpacked) dimension in the call path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Adding Serialization Support\n",
    "\n",
    "Keras depends on a fixed serialization contract for saving and loading models.\n",
    "This contract is complicated by quantization, since the variables you need to\n",
    "save and load depend on the quantization mode.\n",
    "\n",
    "The framework provides two hooks for layers to customize variable serialization:\n",
    "\n",
    "- `save_own_variables(self, store)`: Write variables to `store` in a fixed\n",
    "  order.\n",
    "- `load_own_variables(self, store)`: Read variables from `store` in the same\n",
    "  order.\n",
    "\n",
    "Additionally, the `build(...)` method should also be modified to allocate the\n",
    "correct variables based on presence (or absence) of a `self.quantization_mode`.\n",
    "\n",
    "For this layer we only aim to support two modes (Non-quantized and INT8), so the\n",
    "serialization contract is:\n",
    "\n",
    "- None (no quantization): `[\"kernel\"]`\n",
    "- INT8: `[\"kernel\", \"scale\"]`\n",
    "\n",
    "The following code implements the required hooks; Keras will call them during\n",
    "`model.save(...)` and `keras.saving.load_model(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_own_variables(self, store):\n",
    "    # Write variables to `store` in a fixed order based on quantization mode.\n",
    "    # `store` is a key-value mapping provided by Keras during model.save().\n",
    "    # Values are tensors.\n",
    "    if not self.built:\n",
    "        return\n",
    "    mode = self.quantization_mode\n",
    "    idx = 0\n",
    "    if mode is None:\n",
    "        # Order: _kernel\n",
    "        store[str(idx)] = self._kernel\n",
    "    elif mode == \"int8\":\n",
    "        # Order: _kernel, scale\n",
    "        store[str(idx)] = self._kernel\n",
    "        idx += 1\n",
    "        store[str(idx)] = self.scale\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported quantization mode for save: {mode}\")\n",
    "\n",
    "\n",
    "def load_own_variables(self, store):\n",
    "    # Read variables from `store` in the same order used by\n",
    "    # `save_own_variables`. Keras calls this during\n",
    "    # `keras.saving.load_model(...)`.\n",
    "    if not self.built:\n",
    "        return\n",
    "    mode = self.quantization_mode\n",
    "    idx = 0\n",
    "    if mode is None:\n",
    "        self._kernel.assign(store[str(idx)])\n",
    "    elif mode == \"int8\":\n",
    "        self._kernel.assign(store[str(idx)])\n",
    "        idx += 1\n",
    "        self.scale.assign(store[str(idx)])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported quantization mode for load: {mode}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Modify the `build(...)` method\n",
    "\n",
    "The build method itself also needs to be aware of quantization mode. If a saved\n",
    "quantized layer is being loaded/deserialized, `self.quantization_mode` will be\n",
    "set before `build(...)` is called. In that case, we need to allocate quantized\n",
    "variables directly instead of full-precision ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build(self, input_shape):\n",
    "    input_dim = input_shape[-1]\n",
    "\n",
    "    # Quantized build path.\n",
    "    if self.quantization_mode:\n",
    "        if self.quantization_mode == \"int8\":\n",
    "            self._int8_build((input_dim,))\n",
    "    else:\n",
    "        # Regular FP32 build path.\n",
    "        self._kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_dim,),\n",
    "            initializer=\"random_uniform\",\n",
    "        )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Complete implementation with serialization\n",
    "\n",
    "The full class with serialization support looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class SimpleScale(Layer):\n",
    "    \"\"\"A layer that learns a per-feature scaling factor.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.quantization_mode:\n",
    "            if self.quantization_mode == \"int8\":\n",
    "                self._int8_build((input_dim,))\n",
    "        else:\n",
    "            self._kernel = self.add_weight(\n",
    "                name=\"kernel\",\n",
    "                shape=(input_dim,),\n",
    "                initializer=\"random_uniform\",\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return ops.multiply(inputs, self._kernel)\n",
    "\n",
    "    def quantize(self, mode, **kwargs):\n",
    "        if mode != \"int8\":\n",
    "            raise NotImplementedError(f\"Unsupported quantization mode: {mode}\")\n",
    "\n",
    "        quantized_kernel, scale = quantizers.abs_max_quantize(\n",
    "            self._kernel, axis=0, dtype=\"int8\", to_numpy=True\n",
    "        )\n",
    "        scale = ops.squeeze(scale, axis=0)\n",
    "\n",
    "        kernel_shape = self._kernel.shape\n",
    "\n",
    "        del self._kernel\n",
    "\n",
    "        self._int8_build(kernel_shape)\n",
    "\n",
    "        self._kernel.assign(quantized_kernel)\n",
    "        self.scale.assign(scale)\n",
    "\n",
    "        self._is_quantized = True\n",
    "\n",
    "        if self.dtype_policy.quantization_mode is None:\n",
    "            policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n",
    "            self.dtype_policy = policy\n",
    "\n",
    "    def _int8_build(self, kernel_shape):\n",
    "        (input_dim,) = kernel_shape\n",
    "        self._kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_dim,),\n",
    "            initializer=\"zeros\",\n",
    "            dtype=\"int8\",\n",
    "            trainable=False,\n",
    "        )\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\",\n",
    "            initializer=\"ones\",\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    def _int8_call(self, inputs, training=None):\n",
    "        x = ops.multiply(inputs, self._kernel)\n",
    "        x = ops.divide(x, self.scale)\n",
    "        return x\n",
    "\n",
    "    def save_own_variables(self, store):\n",
    "        # Write variables to `store` in a fixed order based on quantization mode.\n",
    "        # `store` is a key-value mapping provided by Keras during model.save(); values are tensors.\n",
    "        if not self.built:\n",
    "            return\n",
    "        mode = self.quantization_mode\n",
    "        idx = 0\n",
    "        if mode is None:\n",
    "            # Order: _kernel\n",
    "            store[str(idx)] = self._kernel\n",
    "        elif mode == \"int8\":\n",
    "            # Order: _kernel, scale\n",
    "            store[str(idx)] = self._kernel\n",
    "            idx += 1\n",
    "            store[str(idx)] = self.scale\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported quantization mode for save: {mode}\")\n",
    "\n",
    "    def load_own_variables(self, store):\n",
    "        # Read variables from `store` in the same order used by `save_own_variables`.\n",
    "        # Keras calls this during `keras.saving.load_model(...)`.\n",
    "        if not self.built:\n",
    "            return\n",
    "        mode = self.quantization_mode\n",
    "        idx = 0\n",
    "        if mode is None:\n",
    "            self._kernel.assign(store[str(idx)])\n",
    "        elif mode == \"int8\":\n",
    "            self._kernel.assign(store[str(idx)])\n",
    "            idx += 1\n",
    "            self.scale.assign(store[str(idx)])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported quantization mode for load: {mode}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Note\n",
    "\n",
    "The `@keras.saving.register_keras_serializable()` decorator is needed to\n",
    "register the class for serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Try it: quantize, save, and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([Input(shape=(4,)), SimpleScale()])\n",
    "model.build((None, 4))\n",
    "\n",
    "# Quantize to INT8.\n",
    "model.quantize(\"int8\")\n",
    "y_int8 = model(x)\n",
    "print(\"SimpleScale INT8 sample:\", y_int8[0].numpy())\n",
    "\n",
    "# Save and load the quantized model.\n",
    "model.save(\"simplescale_int8.keras\")\n",
    "loaded = keras.saving.load_model(\"simplescale_int8.keras\")\n",
    "\n",
    "y_loaded = loaded(x)\n",
    "print(\"Loaded INT8 sample:\", y_loaded[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Practical tips\n",
    "\n",
    "Here are concrete patterns you can reuse when making your own layers PTQ-friendly.\n",
    "\n",
    "- Build-time vs call-time responsibilities\n",
    "  - In `build(...)`, if `self.quantization_mode` is set: allocate the quantized\n",
    "    variables and skip allocating the float kernel to avoid duplicates.\n",
    "- Record any metadata you need for the call path, e.g., for INT4:\n",
    "  - The axis you packed along (e.g., `_int4_pack_axis`).\n",
    "  - The original (unpacked) length on that axis (e.g., `_original_input_dim` or\n",
    "    `_original_length_along_pack_axis`).\n",
    "- In quantized call hooks, compute with the quantized buffers and de-scale back\n",
    "  to float at the end, wherever possible. This allows you to leverage optimized\n",
    "  low-precision kernels (e.g., cuBLAS INT8 GEMM).\n",
    "\n",
    "- INT4 specifics (packed nibbles)\n",
    "  - Quantize to INT4 values in range [-8, 7] (still dtype int8), then pack two\n",
    "    4-bit integers per byte with `quantizers.pack_int4(..., axis=pack_axis)`.\n",
    "  - Store the packed kernel with `dtype=\"int8\"`. Unpack on the fly in the call\n",
    "    path with `quantizers.unpack_int4(packed, orig_len, axis=pack_axis)`.\n",
    "  - Keep the original length and pack axis so you can unpack for LoRA,\n",
    "    gradients, and serialization.\n",
    "\n",
    "- Inputs quantization and broadcasting\n",
    "  - In the forward path de-scale outputs using\n",
    "  `outputs /= (inputs_scale * kernel_scale)`; make sure both scales broadcast to\n",
    "  the output shape.\n",
    "\n",
    "- Dtype policy lifecycle\n",
    "  - During `quantize(mode)`: delete FP32 variables, allocate quantized ones,\n",
    "    assign values, then set `self._is_quantized = True` before changing the\n",
    "    dtype policy.\n",
    "  - Only change policy if the current policy has `quantization_mode is None` to\n",
    "    avoid an infinite loop.\n",
    "\n",
    "- Serialization contract\n",
    "  - Provide a fixed-order logic for variable serialization so save/load is\n",
    "    deterministic.\n",
    "  - Write variables in a fixed order per mode (e.g., None: [kernel, bias],\n",
    "    `\"int8\"`: [kernel, bias, kernel_scale], `\"int4\"`: [kernel, bias, kernel_scale]).\n",
    "\n",
    "- Validation and error handling\n",
    "  - Validate `mode` early and raise a `NotImplementedError` for unsupported\n",
    "    modes.\n",
    "  - After quantization, run a tiny smoke test and assert the output matches the\n",
    "    FP32 path and values are within a reasonable tolerance after de-scale.\n",
    "\n",
    "- Performance hygiene\n",
    "  - Avoid repeated transformations hot paths; precompute as much information\n",
    "    as possible and keep the forward-pass hooks lightweight.\n",
    "  - Keep quantized buffers `trainable=False` and prefer vectorized operations.\n",
    "\n",
    "For more advanced patterns, refer to the\n",
    "[Dense](https://github.com/keras-team/keras/blob/3c3d6adc08db627d89b5ad5e7f9b0ba3e88f2641/keras/src/layers/core/dense.py) and\n",
    "[EinsumDense](https://github.com/keras-team/keras/blob/3c3d6adc08db627d89b5ad5e7f9b0ba3e88f2641/keras/src/layers/core/einsum_dense.py)\n",
    "reference implementations."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "writing_quantization_compatible_layers",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
