{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Segment Anything in KerasHub!\n",
    "\n",
    "**Author:** Tirth Patel, Ian Stenbit, Divyashree Sreepathihalli<br><br>\n",
    "**Date created:** 2024/10/1<br><br>\n",
    "**Last modified:** 2024/10/1<br><br>\n",
    "**Description:** Segment anything using text, box, and points prompts in KerasHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The Segment Anything Model (SAM) produces high quality object masks from input prompts\n",
    "such as points or boxes, and it can be used to generate masks for all objects in an\n",
    "image. It has been trained on a\n",
    "[dataset](https://segment-anything.com/dataset/index.html) of 11 million images and 1.1\n",
    "billion masks, and has strong zero-shot performance on a variety of segmentation tasks.\n",
    "\n",
    "In this guide, we will show how to use KerasHub's implementation of the\n",
    "[Segment Anything Model](https://github.com/facebookresearch/segment-anything)\n",
    "and show how powerful TensorFlow's and JAX's performance boost is.\n",
    "\n",
    "First, let's get all our dependencies and images for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install -Uq git+https://github.com/keras-team/keras-hub.git\n",
    "!!pip install -Uq keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!wget -q https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Choose your backend\n",
    "\n",
    "With Keras 3, you can choose to use your favorite backend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import ops\n",
    "import keras_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "Let's define some helper functions for visulazing the images, prompts, and the\n",
    "segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    box = box.reshape(-1)\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def inference_resizing(image, pad=True):\n",
    "    # Compute Preprocess Shape\n",
    "    image = ops.cast(image, dtype=\"float32\")\n",
    "    old_h, old_w = image.shape[0], image.shape[1]\n",
    "    scale = 1024 * 1.0 / max(old_h, old_w)\n",
    "    new_h = old_h * scale\n",
    "    new_w = old_w * scale\n",
    "    preprocess_shape = int(new_h + 0.5), int(new_w + 0.5)\n",
    "\n",
    "    # Resize the image\n",
    "    image = ops.image.resize(image[None, ...], preprocess_shape)[0]\n",
    "\n",
    "    # Pad the shorter side\n",
    "    if pad:\n",
    "        pixel_mean = ops.array([123.675, 116.28, 103.53])\n",
    "        pixel_std = ops.array([58.395, 57.12, 57.375])\n",
    "        image = (image - pixel_mean) / pixel_std\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        pad_h = 1024 - h\n",
    "        pad_w = 1024 - w\n",
    "        image = ops.pad(image, [(0, pad_h), (0, pad_w), (0, 0)])\n",
    "        # KerasHub now rescales the images and normalizes them.\n",
    "        # Just unnormalize such that when KerasHub normalizes them\n",
    "        # again, the padded values map to 0.\n",
    "        image = image * pixel_std + pixel_mean\n",
    "    return image\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Get the pretrained SAM model\n",
    "\n",
    "We can initialize a trained SAM model using KerasHub's `from_preset` factory method. Here,\n",
    "we use the huge ViT backbone trained on the SA-1B dataset (`sam_huge_sa1b`) for\n",
    "high-quality segmentation masks. You can also use one of the `sam_large_sa1b` or\n",
    "`sam_base_sa1b` for better performance (at the cost of decreasing quality of segmentation\n",
    "masks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras_hub.models.SAMImageSegmenter.from_preset(\"sam_huge_sa1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Understanding Prompts\n",
    "\n",
    "Segment Anything allows prompting an image using points, boxes, and masks:\n",
    "\n",
    "1. Point prompts are the most basic of all: the model tries to guess the object given a\n",
    "point on an image. The point can either be a foreground point (i.e. the desired\n",
    "segmentation mask contains the point in it) or a backround point (i.e. the point lies\n",
    "outside the desired mask).\n",
    "2. Another way to prompt the model is using boxes. Given a bounding box, the model tries\n",
    "to segment the object contained in it.\n",
    "3. Finally, the model can also be prompted using a mask itself. This is useful, for\n",
    "instance, to refine the borders of a previously predicted or known segmentation mask.\n",
    "\n",
    "What makes the model incredibly powerful is the ability to combine the prompts above.\n",
    "Point, box, and mask prompts can be combined in several different ways to achieve the\n",
    "best result.\n",
    "\n",
    "Let's see the semantics of passing these prompts to the Segment Anything model in\n",
    "KerasHub. Input to the SAM model is a dictionary with keys:\n",
    "\n",
    "1. `\"images\"`: A batch of images to segment. Must be of shape `(B, 1024, 1024, 3)`.\n",
    "2. `\"points\"`: A batch of point prompts. Each point is an `(x, y)` coordinate originating\n",
    "from the top-left corner of the image. In other works, each point is of the form `(r, c)`\n",
    "where `r` and `c` are the row and column of the pixel in the image. Must be of shape `(B,\n",
    "N, 2)`.\n",
    "3. `\"labels\"`: A batch of labels for the given points. `1` represents foreground points\n",
    "and `0` represents background points. Must be of shape `(B, N)`.\n",
    "4. `\"boxes\"`: A batch of boxes. Note that the model only accepts one box per batch.\n",
    "Hence, the expected shape is `(B, 1, 2, 2)`. Each box is a collection of 2 points: the\n",
    "top left corner and the bottom right corner of the box. The points here follow the same\n",
    "semantics as the point prompts. Here the `1` in the second dimension represents the\n",
    "presence of box prompts. If the box prompts are missing, a placeholder input of shape\n",
    "`(B, 0, 2, 2)` must be passed.\n",
    "5. `\"masks\"`: A batch of masks. Just like box prompts, only one mask prompt per image is\n",
    "allowed. The shape of the input mask must be `(B, 1, 256, 256, 1)` if they are present\n",
    "and `(B, 0, 256, 256, 1)` for missing mask prompt.\n",
    "\n",
    "Placeholder prompts are only required when calling the model directly (i.e.\n",
    "`model(...)`). When calling the `predict` method, missing prompts can be omitted from the\n",
    "input dictionary.\n",
    "\n",
    "## Point prompts\n",
    "\n",
    "First, let's segment an image using point prompts. We load the image and resize it to\n",
    "shape `(1024, 1024)`, the image size the pretrained SAM model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load our image\n",
    "image = np.array(keras.utils.load_img(\"truck.jpg\"))\n",
    "image = inference_resizing(image)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(ops.convert_to_numpy(image) / 255.0)\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we will define the point on the object we want to segment. Let's try to segment the\n",
    "truck's window pane at coordinates `(284, 213)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define the input point prompt\n",
    "input_point = np.array([[284, 213.5]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(ops.convert_to_numpy(image) / 255.0)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now let's call the `predict` method of our model to get the segmentation masks.\n",
    "\n",
    "**Note**: We don't call the model directly (`model(...)`) since placeholder prompts are\n",
    "required to do so. Missing prompts are handled automatically by the predict method so we\n",
    "call it instead. Also, when no box prompts are present, the points and labels need to be\n",
    "padded with a zero point prompt and `-1` label prompt respectively. The cell below\n",
    "demonstrates how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "outputs = model.predict(\n",
    "    {\n",
    "        \"images\": image[np.newaxis, ...],\n",
    "        \"points\": np.concatenate(\n",
    "            [input_point[np.newaxis, ...], np.zeros((1, 1, 2))], axis=1\n",
    "        ),\n",
    "        \"labels\": np.concatenate(\n",
    "            [input_label[np.newaxis, ...], np.full((1, 1), fill_value=-1)], axis=1\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`SegmentAnythingModel.predict` returns two outputs. First are logits (segmentation masks)\n",
    "of shape `(1, 4, 256, 256)` and the other are the IoU confidence scores (of shape `(1,\n",
    "4)`) for each mask predicted. The pretrained SAM model predicts four masks: the first is\n",
    "the best mask the model could come up with for the given prompts, and the other 3 are the\n",
    "alternative masks which can be used in case the best prediction doesn't contain the\n",
    "desired object. The user can choose whichever mask they prefer.\n",
    "\n",
    "Let's visualize the masks returned by the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Resize the mask to our image shape i.e. (1024, 1024)\n",
    "mask = inference_resizing(outputs[\"masks\"][0][0][..., None], pad=False)[..., 0]\n",
    "# Convert the logits to a numpy array\n",
    "# and convert the logits to a boolean mask\n",
    "mask = ops.convert_to_numpy(mask) > 0.0\n",
    "iou_score = ops.convert_to_numpy(outputs[\"iou_pred\"][0][0])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(ops.convert_to_numpy(image) / 255.0)\n",
    "show_mask(mask, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.title(f\"IoU Score: {iou_score:.3f}\", fontsize=18)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As expected, the model returns a segmentation mask for the truck's window pane. But, our\n",
    "point prompt can also mean a range of other things. For example, another possible mask\n",
    "that contains our point is just the right side of the window pane or the whole truck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's also visualize the other masks the model has predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 60))\n",
    "masks, scores = outputs[\"masks\"][0][1:], outputs[\"iou_pred\"][0][1:]\n",
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    mask = inference_resizing(mask[..., None], pad=False)[..., 0]\n",
    "    mask, score = map(ops.convert_to_numpy, (mask, score))\n",
    "    mask = 1 * (mask > 0.0)\n",
    "    ax[i].imshow(ops.convert_to_numpy(image) / 255.0)\n",
    "    show_mask(mask, ax[i])\n",
    "    show_points(input_point, input_label, ax[i])\n",
    "    ax[i].set_title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=12)\n",
    "    ax[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Nice! SAM was able to capture the ambiguity of our point prompt and also returned other\n",
    "possible segmentation masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Box Prompts\n",
    "\n",
    "Now, let's see how we can prompt the model using boxes. The box is specified using two\n",
    "points, the top-left corner and the bottom-right corner of the bounding box in xyxy\n",
    "format. Let's prompt the model using a bounding box around the left front tyre of the\n",
    "truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's specify the box\n",
    "input_box = np.array([[240, 340], [400, 500]])\n",
    "\n",
    "outputs = model.predict(\n",
    "    {\"images\": image[np.newaxis, ...], \"boxes\": input_box[np.newaxis, np.newaxis, ...]}\n",
    ")\n",
    "mask = inference_resizing(outputs[\"masks\"][0][0][..., None], pad=False)[..., 0]\n",
    "mask = ops.convert_to_numpy(mask) > 0.0\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(ops.convert_to_numpy(image) / 255.0)\n",
    "show_mask(mask, plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Boom! The model perfectly segments out the left front tyre in our bounding box.\n",
    "\n",
    "## Combining prompts\n",
    "\n",
    "To get the true potential of the model out, let's combine box and point prompts and see\n",
    "what the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's specify the box\n",
    "input_box = np.array([[240, 340], [400, 500]])\n",
    "# Let's specify the point and mark it background\n",
    "input_point = np.array([[325, 425]])\n",
    "input_label = np.array([0])\n",
    "\n",
    "outputs = model.predict(\n",
    "    {\n",
    "        \"images\": image[np.newaxis, ...],\n",
    "        \"points\": input_point[np.newaxis, ...],\n",
    "        \"labels\": input_label[np.newaxis, ...],\n",
    "        \"boxes\": input_box[np.newaxis, np.newaxis, ...],\n",
    "    }\n",
    ")\n",
    "mask = inference_resizing(outputs[\"masks\"][0][0][..., None], pad=False)[..., 0]\n",
    "mask = ops.convert_to_numpy(mask) > 0.0\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(ops.convert_to_numpy(image) / 255.0)\n",
    "show_mask(mask, plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Voila! The model understood that the object we wanted to exclude from our mask was the\n",
    "rim of the tyre.\n",
    "\n",
    "## Text prompts\n",
    "\n",
    "Finally, let's see how text prompts can be used along with KerasHub's\n",
    "`SegmentAnythingModel`.\n",
    "\n",
    "For this demo, we will use the\n",
    "[offical Grounding DINO model](https://github.com/IDEA-Research/GroundingDINO).\n",
    "Grounding DINO is a model that\n",
    "takes as input a `(image, text)` pair and generates a bounding box around the object in\n",
    "the `image` described by the `text`. You can refer to the\n",
    "[paper](https://arxiv.org/abs/2303.05499) for more details on the implementation of the\n",
    "model.\n",
    "\n",
    "For this part of the demo, we will need to install the `groundingdino` package from\n",
    "source:\n",
    "\n",
    "```\n",
    "pip install -U git+https://github.com/IDEA-Research/GroundingDINO.git\n",
    "```\n",
    "\n",
    "Then, we can install the pretrained model's weights and config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install -U git+https://github.com/IDEA-Research/GroundingDINO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "!!wget -q https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/v0.1.0-alpha2/groundingdino/config/GroundingDINO_SwinT_OGC.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import Model as GroundingDINO\n",
    "\n",
    "CONFIG_PATH = \"GroundingDINO_SwinT_OGC.py\"\n",
    "WEIGHTS_PATH = \"groundingdino_swint_ogc.pth\"\n",
    "\n",
    "grounding_dino = GroundingDINO(CONFIG_PATH, WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's load an image of a dog for this part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "filepath = keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/keras-cv/test-images/mountain-dog.jpeg\"\n",
    ")\n",
    "image = np.array(keras.utils.load_img(filepath))\n",
    "image = ops.convert_to_numpy(inference_resizing(image))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image / 255.0)\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We first predict the bounding box of the object we want to segment using the Grounding\n",
    "DINO model. Then, we prompt the SAM model using the bounding box to get the segmentation\n",
    "mask.\n",
    "\n",
    "Let's try to segment out the harness of the dog. Change the image and text below to\n",
    "segment whatever you want using text from your image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's predict the bounding box for the harness of the dog\n",
    "boxes = grounding_dino.predict_with_caption(image.astype(np.uint8), \"harness\")\n",
    "boxes = np.array(boxes[0].xyxy)\n",
    "\n",
    "outputs = model.predict(\n",
    "    {\n",
    "        \"images\": np.repeat(image[np.newaxis, ...], boxes.shape[0], axis=0),\n",
    "        \"boxes\": boxes.reshape(-1, 1, 2, 2),\n",
    "    },\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And that's it! We got a segmentation mask for our text prompt using the combination of\n",
    "Gounding DINO + SAM! This is a very powerful technique to combine different models to\n",
    "expand the applications!\n",
    "\n",
    "Let's visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image / 255.0)\n",
    "\n",
    "for mask in outputs[\"masks\"]:\n",
    "    mask = inference_resizing(mask[0][..., None], pad=False)[..., 0]\n",
    "    mask = ops.convert_to_numpy(mask) > 0.0\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_box(boxes, plt.gca())\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Optimizing SAM\n",
    "\n",
    "You can use `mixed_float16` or `bfloat16` dtype policies to gain huge speedups and memory\n",
    "optimizations at releatively low precision loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load our image\n",
    "image = np.array(keras.utils.load_img(\"truck.jpg\"))\n",
    "image = inference_resizing(image)\n",
    "\n",
    "# Specify the prompt\n",
    "input_box = np.array([[240, 340], [400, 500]])\n",
    "\n",
    "# Let's first see how fast the model is with float32 dtype\n",
    "time_taken = timeit.repeat(\n",
    "    'model.predict({\"images\": image[np.newaxis, ...], \"boxes\": input_box[np.newaxis, np.newaxis, ...]}, verbose=False)',\n",
    "    repeat=3,\n",
    "    number=3,\n",
    "    globals=globals(),\n",
    ")\n",
    "print(f\"Time taken with float32 dtype: {min(time_taken) / 3:.10f}s\")\n",
    "\n",
    "# Set the dtype policy in Keras\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model = keras_hub.models.SAMImageSegmenter.from_preset(\"sam_huge_sa1b\")\n",
    "\n",
    "time_taken = timeit.repeat(\n",
    "    'model.predict({\"images\": image[np.newaxis, ...], \"boxes\": input_box[np.newaxis,np.newaxis, ...]}, verbose=False)',\n",
    "    repeat=3,\n",
    "    number=3,\n",
    "    globals=globals(),\n",
    ")\n",
    "print(f\"Time taken with float16 dtype: {min(time_taken) / 3:.10f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Here's a comparison of KerasHub's implementation with the original PyTorch\n",
    "implementation!\n",
    "\n",
    "![benchmark](https://github.com/tirthasheshpatel/segment_anything_keras/blob/main/benchmark.png?raw=true)\n",
    "\n",
    "The script used to generate the benchmarks is present\n",
    "[here](https://github.com/tirthasheshpatel/segment_anything_keras/blob/main/Segment_Anything_Benchmarks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "KerasHub's `SegmentAnythingModel` supports a variety of applications and, with the help of\n",
    "Keras 3, enables running the model on TensorFlow, JAX, and PyTorch! With the help of XLA\n",
    "in JAX and TensorFlow, the model runs several times faster than the original\n",
    "implementation. Moreover, using Keras's mixed precision support helps optimize memory use\n",
    "and computation time with just one line of code!\n",
    "\n",
    "For more advanced uses, check out the\n",
    "[Automatic Mask Generator demo](https://github.com/tirthasheshpatel/segment_anything_keras/blob/main/Segment_Anything_Automatic_Mask_Generator_Demo.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segment_anything_in_keras_hub",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}