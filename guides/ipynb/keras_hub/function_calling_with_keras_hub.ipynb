{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Function Calling with KerasHub models\n",
    "\n",
    "**Author:** [Laxmareddy Patlolla](https://github.com/laxmareddyp), [Divyashree Sreepathihalli](https://github.com/divyashreepathihalli)<br>\n",
    "**Date created:** 2025/07/08<br>\n",
    "**Last modified:** 2025/07/10<br>\n",
    "**Description:** A guide to using the function calling feature in KerasHub with Gemma 3 and Mistral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Tool calling is a powerful new feature in modern large language models that allows them to use external tools, such as Python functions, to answer questions and perform actions. Instead of just generating text, a tool-calling model can generate code that calls a function you've provided, allowing it to interact with the real world, access live data, and perform complex calculations.\n",
    "\n",
    "In this guide, we'll walk you through a simple example of tool calling with the Gemma 3 and Mistral models and KerasHub. We'll show you how to:\n",
    "\n",
    "1. Define a tool (a Python function).\n",
    "2. Tell the models about the tool.\n",
    "3. Use the model to generate code that calls the tool.\n",
    "4. Execute the code and feed the result back to the model.\n",
    "5. Get a final, natural-language response from the model.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and configure our environment. We'll be using KerasHub to download and run the language models, and we'll need to authenticate with Kaggle to access the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import ast\n",
    "import io\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "# Set backend before importing Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "USD_TO_EUR_RATE = 0.85\n",
    "\n",
    "# Set the default dtype policy to bfloat16 for improved performance and reduced memory usage on supported hardware (e.g., TPUs, some GPUs)\n",
    "keras.config.set_dtype_policy(\"bfloat16\")\n",
    "\n",
    "# Authenticate with Kaggle\n",
    "# In Google Colab, you can set KAGGLE_USERNAME and KAGGLE_KEY as secrets,\n",
    "# and kagglehub.login() will automatically detect and use them:\n",
    "# kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Loading the Model\n",
    "\n",
    "Next, we'll load the Gemma 3 model from KerasHub. We're using the `gemma3_instruct_4b` preset, which is a version of the model that has been specifically fine-tuned for instruction following and tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    gemma = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_4b\")\n",
    "    print(\"\u2705 Gemma 3 model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading Gemma 3 model: {e}\")\n",
    "    print(\"Please ensure you have the correct model preset and sufficient resources.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Defining a Tool\n",
    "\n",
    "Now, let's define a simple tool that we want our model to be able to use. For this example, we'll create a Python function called `convert` that can convert one currency to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert(amount, currency, new_currency):\n",
    "    \"\"\"Convert the currency with the latest exchange rate\n",
    "\n",
    "    Args:\n",
    "      amount: The amount of currency to convert\n",
    "      currency: The currency to convert from\n",
    "      new_currency: The currency to convert to\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if amount < 0:\n",
    "        raise ValueError(\"Amount cannot be negative\")\n",
    "\n",
    "    if not isinstance(currency, str) or not isinstance(new_currency, str):\n",
    "        raise ValueError(\"Currency codes must be strings\")\n",
    "\n",
    "    # Normalize currency codes to uppercase to handle model-generated lowercase codes\n",
    "    currency = currency.upper().strip()\n",
    "    new_currency = new_currency.upper().strip()\n",
    "\n",
    "    # In a real application, this function would call an API to get the latest\n",
    "    # exchange rate. For this example, we'll just use a fixed rate.\n",
    "    if currency == \"USD\" and new_currency == \"EUR\":\n",
    "        return amount * USD_TO_EUR_RATE\n",
    "    elif currency == \"EUR\" and new_currency == \"USD\":\n",
    "        return amount / USD_TO_EUR_RATE\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Currency conversion from {currency} to {new_currency} is not supported.\"\n",
    "        )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Telling the Model About the Tool\n",
    "\n",
    "Now that we have a tool, we need to tell the Gemma 3 model about it. We do this by providing a carefully crafted prompt that includes:\n",
    "\n",
    "1. A description of the tool calling process.\n",
    "2. The Python code for the tool, including its function signature and docstring.\n",
    "3. The user's question.\n",
    "\n",
    "Here's the prompt we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "message = '''\n",
    "<start_of_turn>user\n",
    "At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```. The python methods described below are imported and available, you can only use defined methods and must not reimplement them. The generated code should be readable and efficient. I will provide the response wrapped in ```tool_output```, use it to call more tools or generate a helpful, friendly response. When using a ```tool_call``` think step by step why and how it should be used.\n",
    "\n",
    "The following Python methods are available:\n",
    "\n",
    "```python\n",
    "def convert(amount, currency, new_currency):\n",
    "    \"\"\"Convert the currency with the latest exchange rate\n",
    "\n",
    "    Args:\n",
    "      amount: The amount of currency to convert\n",
    "      currency: The currency to convert from\n",
    "      new_currency: The currency to convert to\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "User: What is $200,000 in EUR?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Generating the Tool Call\n",
    "\n",
    "Now, let's pass this prompt to the model and see what it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(gemma.generate(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As you can see, the model has correctly identified that it can use the `convert` function to answer the question, and it has generated the corresponding Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Executing the Tool Call and Getting a Final Answer\n",
    "\n",
    "In a real application, you would now take this generated code, execute it, and feed the result back to the model. Let's create a practical example that shows how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# First, let's get the model's response\n",
    "response = gemma.generate(message)\n",
    "print(\"Model's response:\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# Extract the tool call from the response\n",
    "def extract_tool_call(response_text):\n",
    "    \"\"\"Extract tool call from the model's response.\"\"\"\n",
    "    tool_call_pattern = r\"```tool_code\\s*\\n(.*?)\\n```\"\n",
    "    match = re.search(tool_call_pattern, response_text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def capture_code_output(code_string, globals_dict=None, locals_dict=None):\n",
    "    \"\"\"\n",
    "    Executes Python code and captures any stdout output.\n",
    "\n",
    "    \u26a0\ufe0f  SECURITY WARNING \u26a0\ufe0f\n",
    "    This function uses eval() and exec() which can execute arbitrary code.\n",
    "    NEVER use this function with untrusted code in production environments.\n",
    "    Always validate and sanitize code from LLMs before execution.\n",
    "    Consider using a sandboxed environment or code analysis tools.\n",
    "\n",
    "    Args:\n",
    "        code_string (str): The code to execute (expression or statements).\n",
    "        globals_dict (dict, optional): Global variables for execution.\n",
    "        locals_dict (dict, optional): Local variables for execution.\n",
    "\n",
    "    Returns:\n",
    "        The captured stdout output if any, otherwise the return value of the expression,\n",
    "        or None if neither.\n",
    "    \"\"\"\n",
    "    if globals_dict is None:\n",
    "        globals_dict = {}\n",
    "    if locals_dict is None:\n",
    "        locals_dict = globals_dict\n",
    "\n",
    "    output = io.StringIO()\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(output):\n",
    "            try:\n",
    "                # Try to evaluate as an expression\n",
    "                result = eval(code_string, globals_dict, locals_dict)\n",
    "            except SyntaxError:\n",
    "                # If not an expression, execute as statements\n",
    "                exec(code_string, globals_dict, locals_dict)\n",
    "                result = None\n",
    "    except Exception as e:\n",
    "        return f\"Error during code execution: {e}\"\n",
    "\n",
    "    stdout_output = output.getvalue()\n",
    "    if stdout_output.strip():\n",
    "        return stdout_output\n",
    "    return result\n",
    "\n",
    "\n",
    "# Extract and execute the tool call\n",
    "tool_code = extract_tool_call(response)\n",
    "if tool_code:\n",
    "    print(f\"\\nExtracted tool call: {tool_code}\")\n",
    "    try:\n",
    "        local_vars = {\"convert\": convert}\n",
    "        tool_result = capture_code_output(tool_code, globals_dict=local_vars)\n",
    "        print(f\"Tool execution result: {tool_result}\")\n",
    "\n",
    "        # Create the next message with the tool result\n",
    "        message_with_result = f'''\n",
    "<start_of_turn>user\n",
    "At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```. The python methods described below are imported and available, you can only use defined methods and must not reimplement them. The generated code should be readable and efficient. I will provide the response wrapped in ```tool_output```, use it to call more tools or generate a helpful, friendly response. When using a ```tool_call``` think step by step why and how it should be used.\n",
    "\n",
    "The following Python methods are available:\n",
    "\n",
    "```python\n",
    "def convert(amount, currency, new_currency):\n",
    "    \"\"\"Convert the currency with the latest exchange rate\n",
    "\n",
    "    Args:\n",
    "      amount: The amount of currency to convert\n",
    "      currency: The currency to convert from\n",
    "      new_currency: The currency to convert to\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "User: What is $200,000 in EUR?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "```tool_code\n",
    "print(convert(200000, \"USD\", \"EUR\"))\n",
    "```<end_of_turn>\n",
    "<start_of_turn>user\n",
    "```tool_output\n",
    "{tool_result}\n",
    "```\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''\n",
    "\n",
    "        # Get the final response\n",
    "        final_response = gemma.generate(message_with_result)\n",
    "        print(\"\\nFinal response:\")\n",
    "        print(final_response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing tool call: {e}\")\n",
    "else:\n",
    "    print(\"No tool call found in the response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Automated Tool Call Execution Loop\n",
    "\n",
    "Let's create a more sophisticated example that shows how to automatically handle multiple tool calls in a conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def automated_tool_calling_example():\n",
    "    \"\"\"Demonstrate automated tool calling with a conversation loop.\"\"\"\n",
    "\n",
    "    conversation_history = []\n",
    "    max_turns = 5\n",
    "\n",
    "    # Initial user message\n",
    "    user_message = \"What is $500 in EUR, and then what is that amount in USD?\"\n",
    "\n",
    "    # Define base prompt outside the loop for better performance\n",
    "    base_prompt = f'''\n",
    "<start_of_turn>user\n",
    "At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```. The python methods described below are imported and available, you can only use defined methods and must not reimplement them. The generated code should be readable and efficient. I will provide the response wrapped in ```tool_output```, use it to call more tools or generate a helpful, friendly response. When using a ```tool_call``` think step by step why and how it should be used.\n",
    "\n",
    "The following Python methods are available:\n",
    "\n",
    "```python\n",
    "def convert(amount, currency, new_currency):\n",
    "    \"\"\"Convert the currency with the latest exchange rate\n",
    "\n",
    "    Args:\n",
    "      amount: The amount of currency to convert\n",
    "      currency: The currency to convert from\n",
    "      new_currency: The currency to convert to\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "User: {user_message}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''\n",
    "\n",
    "    for turn in range(max_turns):\n",
    "        print(f\"\\n--- Turn {turn + 1} ---\")\n",
    "\n",
    "        # Build conversation context by appending history to base prompt\n",
    "        context = base_prompt\n",
    "        for hist in conversation_history:\n",
    "            context += hist + \"\\n\"\n",
    "\n",
    "        # Get model response\n",
    "        response = gemma.generate(context)\n",
    "        print(f\"Model response: {response}\")\n",
    "\n",
    "        # Extract tool call\n",
    "        tool_code = extract_tool_call(response)\n",
    "\n",
    "        if tool_code:\n",
    "            print(f\"Executing: {tool_code}\")\n",
    "            try:\n",
    "                local_vars = {\"convert\": convert}\n",
    "                tool_result = capture_code_output(tool_code, globals_dict=local_vars)\n",
    "                conversation_history.append(\n",
    "                    f\"```tool_code\\n{tool_code}\\n```<end_of_turn>\"\n",
    "                )\n",
    "                conversation_history.append(\n",
    "                    f\"<start_of_turn>user\\n```tool_output\\n{tool_result}\\n```<end_of_turn>\"\n",
    "                )\n",
    "                conversation_history.append(f\"<start_of_turn>model\\n\")\n",
    "                print(f\"Tool result: {tool_result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing tool: {e}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No tool call found - conversation complete\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n--- Final Conversation ---\")\n",
    "    print(context)\n",
    "    for hist in conversation_history:\n",
    "        print(hist)\n",
    "\n",
    "\n",
    "# Run the automated example\n",
    "print(\"Running automated tool calling example:\")\n",
    "automated_tool_calling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Mistral\n",
    "\n",
    "Mistral differs from Gemma in its approach to tool calling, as it requires a specific format and defines special control tokens for this purpose. This JSON-based syntax for tool calling is also adopted by other models, such as Qwen and Llama.\n",
    "\n",
    "We will now extend the example to a more exciting use case: building a flight booking agent. This agent will be able to search for appropriate flights and book them automatically.\n",
    "\n",
    "To do this, we will first download the Mistral model using KerasHub. For agentic AI with Mistral, low-level access to tokenization is necessary due to the use of control tokens. Therefore, we will instantiate the tokenizer and model separately, and disable the preprocessor for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_hub.tokenizers.MistralTokenizer.from_preset(\n",
    "    \"kaggle://keras/mistral/keras/mistral_0.3_instruct_7b_en\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    mistral = keras_hub.models.MistralCausalLM.from_preset(\n",
    "        \"kaggle://keras/mistral/keras/mistral_0.3_instruct_7b_en\", preprocessor=None\n",
    "    )\n",
    "    print(\"\u2705 Mistral model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading Mistral model: {e}\")\n",
    "    print(\"Please ensure you have the correct model preset and sufficient resources.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we'll define functions for tokenization. The `preprocess` function will take a tokenized conversation in list form and format it correctly for the model. We'll also create an additional function, `encode_instruction`, for tokenizing text and adding instruction control tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(messages, sequence_length=8192):\n",
    "    \"\"\"Preprocess tokenized messages for the Mistral model.\n",
    "\n",
    "    Args:\n",
    "        messages: List of tokenized message sequences\n",
    "        sequence_length: Maximum sequence length for the model\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing token_ids and padding_mask\n",
    "    \"\"\"\n",
    "    concatd = np.expand_dims(np.concatenate(messages), 0)\n",
    "\n",
    "    # Truncate if the sequence is too long\n",
    "    if concatd.shape[1] > sequence_length:\n",
    "        concatd = concatd[:, :sequence_length]\n",
    "\n",
    "    # Calculate padding needed\n",
    "    padding_needed = max(0, sequence_length - concatd.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"token_ids\": np.pad(concatd, ((0, 0), (0, padding_needed))),\n",
    "        \"padding_mask\": np.expand_dims(\n",
    "            np.arange(sequence_length) < concatd.shape[1], 0\n",
    "        ).astype(int),\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_instruction(text):\n",
    "    \"\"\"Encode instruction text with Mistral control tokens.\n",
    "\n",
    "    Args:\n",
    "        text: The instruction text to encode\n",
    "\n",
    "    Returns:\n",
    "        List of tokenized sequences with instruction control tokens\n",
    "    \"\"\"\n",
    "    return [\n",
    "        [tokenizer.token_to_id(\"[INST]\")],\n",
    "        tokenizer(text),\n",
    "        [tokenizer.token_to_id(\"[/INST]\")],\n",
    "    ]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, we'll define a function, `try_parse_funccall`, to handle the model's function calls. These calls are identified by the `[TOOL_CALLS]` control token. The function will parse the subsequent data, which is in JSON format. Mistral also requires us to add a random call ID to each function call. Finally, the function will call the matching tool and encode its results using the `[TOOL_RESULTS]` control token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def try_parse_funccall(response):\n",
    "    \"\"\"Parse function calls from Mistral model response and execute tools.\n",
    "\n",
    "    Args:\n",
    "        response: Tokenized model response\n",
    "\n",
    "    Returns:\n",
    "        List of tokenized sequences including tool results\n",
    "    \"\"\"\n",
    "    # find the tool call in the response, if any\n",
    "    tool_call_id = tokenizer.token_to_id(\"[TOOL_CALLS]\")\n",
    "    pos = np.where(response == tool_call_id)[0]\n",
    "    if not len(pos):\n",
    "        return [response]\n",
    "    pos = pos[0]\n",
    "\n",
    "    try:\n",
    "        decoder = json.JSONDecoder()\n",
    "        tool_calls, _ = decoder.raw_decode(tokenizer.detokenize(response[pos + 1 :]))\n",
    "        if not isinstance(tool_calls, list) or not all(\n",
    "            isinstance(item, dict) for item in tool_calls\n",
    "        ):\n",
    "            return [response]\n",
    "\n",
    "        res = []  # Initialize result list\n",
    "        # assign a random call ID\n",
    "        for call in tool_calls:\n",
    "            call[\"id\"] = \"\".join(\n",
    "                random.choices(string.ascii_letters + string.digits, k=9)\n",
    "            )\n",
    "            if call[\"name\"] not in tools:\n",
    "                continue  # Skip unknown tools\n",
    "            res.append([tokenizer.token_to_id(\"[TOOL_RESULTS]\")])\n",
    "            res.append(\n",
    "                tokenizer(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            \"content\": tools[call[\"name\"]](**call[\"arguments\"]),\n",
    "                            \"call_id\": call[\"id\"],\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            res.append([tokenizer.token_to_id(\"[/TOOL_RESULTS]\")])\n",
    "        return res\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        # Log the error for debugging\n",
    "        print(f\"Error parsing tool call: {e}\")\n",
    "        return [response]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will extend our set of tools to include functions for currency conversion, finding flights, and booking flights. For this example, we'll use mock implementations for these functions, meaning they will return dummy data instead of interacting with real services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tools = {\n",
    "    \"convert_currency\": lambda amount, currency, new_currency: (\n",
    "        f\"{amount*USD_TO_EUR_RATE:.2f}\"\n",
    "        if currency == \"USD\" and new_currency == \"EUR\"\n",
    "        else (\n",
    "            f\"{amount/USD_TO_EUR_RATE:.2f}\"\n",
    "            if currency == \"EUR\" and new_currency == \"USD\"\n",
    "            else f\"Error: Unsupported conversion from {currency} to {new_currency}\"\n",
    "        )\n",
    "    ),\n",
    "    \"find_flights\": lambda origin, destination, date: [\n",
    "        {\"id\": 1, \"price\": \"USD 220\", \"stops\": 2, \"duration\": 4.5},\n",
    "        {\"id\": 2, \"price\": \"USD 22\", \"stops\": 1, \"duration\": 2.0},\n",
    "        {\"id\": 3, \"price\": \"USD 240\", \"stops\": 2, \"duration\": 13.2},\n",
    "    ],\n",
    "    \"book_flight\": lambda id: {\n",
    "        \"status\": \"success\",\n",
    "        \"message\": f\"Flight {id} booked successfully\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "It's crucial to inform the model about these available functions at the very beginning of the conversation. To do this, we will define the available tools in a specific JSON format, as shown in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tool_definitions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"convert_currency\",\n",
    "            \"description\": \"Convert the currency with the latest exchange rate\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"amount\": {\"type\": \"number\", \"description\": \"The amount\"},\n",
    "                    \"currency\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The currency to convert from\",\n",
    "                    },\n",
    "                    \"new_currency\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The currency to convert to\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"amount\", \"currency\", \"new_currency\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"find_flights\",\n",
    "            \"description\": \"Query price, time, number of stopovers and duration in hours for flights for a given date\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"origin\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city to depart from\",\n",
    "                    },\n",
    "                    \"destination\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The destination city\",\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The date in YYYYMMDD format\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"origin\", \"destination\", \"date\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"book_flight\",\n",
    "            \"description\": \"Book the flight with the given id\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The numeric id of the flight to book\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"id\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will define the conversation as a `messages` list. At the very beginning of this list, we need to include a Beginning-Of-Sequence (BOS) token. This is followed by the tool definitions, which must be wrapped in `[AVAILABLE_TOOLS]` and `[/AVAILABLE_TOOLS]` control tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [tokenizer.token_to_id(\"<s>\")],\n",
    "    [tokenizer.token_to_id(\"[AVAILABLE_TOOLS]\")],\n",
    "    tokenizer(json.dumps(tool_definitions)),\n",
    "    [tokenizer.token_to_id(\"[/AVAILABLE_TOOLS]\")],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's get started! We will task the model with the following: **Book the most comfortable flight from Linz to London on the 24th of July 2025, but only if it costs less than 20\u20ac as of the latest exchange rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "messages.extend(\n",
    "    encode_instruction(\n",
    "        \"Book the most comfortable flight from Linz to London on the 24th of July 2025, but only if it costs less than 20\u20ac as of the latest exchange rate.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In an agentic AI system, the model interacts with its tools through a sequence of messages. We will continue to handle these messages until the flight is successfully booked.\n",
    "For educational purposes, we will output the tool calls issued by the model; typically, a user would not see this level of detail. It's important to note that after the tool call JSON, the data must be truncated. If not, a less capable model may 'babble', outputting redundant or confused data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "flight_booked = False\n",
    "max_iterations = 10  # Prevent infinite loops\n",
    "iteration_count = 0\n",
    "\n",
    "while not flight_booked and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    # query the model\n",
    "    res = mistral.generate(\n",
    "        preprocess(messages), max_length=8192, stop_token_ids=[2], strip_prompt=True\n",
    "    )\n",
    "    # output the model's response, add separator line for legibility\n",
    "    response_text = tokenizer.detokenize(\n",
    "        res[\"token_ids\"][0, : np.argmax(~res[\"padding_mask\"])]\n",
    "    )\n",
    "    print(response_text, f\"\\n\\n\\n{'-'*100}\\n\\n\")\n",
    "\n",
    "    # Check for tool calls and track booking status\n",
    "    tool_call_id = tokenizer.token_to_id(\"[TOOL_CALLS]\")\n",
    "    pos = np.where(res[\"token_ids\"][0] == tool_call_id)[0]\n",
    "    if len(pos) > 0:\n",
    "        try:\n",
    "            decoder = json.JSONDecoder()\n",
    "            tool_calls, _ = decoder.raw_decode(\n",
    "                tokenizer.detokenize(res[\"token_ids\"][0][pos[0] + 1 :])\n",
    "            )\n",
    "            if isinstance(tool_calls, list):\n",
    "                for call in tool_calls:\n",
    "                    if isinstance(call, dict) and call.get(\"name\") == \"book_flight\":\n",
    "                        # Check if book_flight was called successfully\n",
    "                        flight_booked = True\n",
    "                        break\n",
    "        except (json.JSONDecodeError, KeyError, TypeError, ValueError):\n",
    "            pass\n",
    "\n",
    "    # perform tool calls and extend `messages`\n",
    "    messages.extend(try_parse_funccall(res[\"token_ids\"][0]))\n",
    "\n",
    "if not flight_booked:\n",
    "    print(\"Maximum iterations reached. Flight booking was not completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For understandability, here's the conversation as received by the model, i.e. when truncating after the tool calling JSON:\n",
    "\n",
    "* **User:**\n",
    "```\n",
    "Book the most comfortable flight from Linz to London on the 24th of July 2025, but only if it costs less than 20\u20ac as of the latest exchange rate.\n",
    "```\n",
    "\n",
    "* **Model:**\n",
    "```\n",
    "[{\"name\": \"find_flights\", \"arguments\": {\"origin\": \"Linz\", \"destination\": \"London\", \"date\": \"20250724\"}}]\n",
    "```\n",
    "* **Tool Output:**\n",
    "```\n",
    "[{\"id\": 1, \"price\": \"USD 220\", \"stops\": 2, \"duration\": 4.5}, {\"id\": 2, \"price\": \"USD 22\", \"stops\": 1, \"duration\": 2.0}, {\"id\": 3, \"price\": \"USD 240\", \"stops\": 2, \"duration\": 13.2}]\n",
    "```\n",
    "* **Model:**\n",
    "```\n",
    "Now let's convert the price from USD to EUR using the latest exchange rate:\n",
    "\n",
    " [{\"name\": \"convert_currency\", \"arguments\": {\"amount\": 22, \"currency\": \"USD\", \"new_currency\": \"EUR\"}}]\n",
    "```\n",
    "* **Tool Output:**\n",
    "```\n",
    "\"18.70\"\n",
    "```\n",
    "* **Model:**\n",
    "```\n",
    "The price of the flight with the id 2 in EUR is 18.70. Since it is below the 20\u20ac limit, let's book this flight:\n",
    "\n",
    " [{\"name\": \"book_flight\", \"arguments\": {\"id\": 2}}]\n",
    "```\n",
    "\n",
    "It's important to acknowledge that you might have to run the model a few times to obtain a good output as depicted above. As a 7-billion parameter model, Mistral may still make several mistakes, such as misinterpreting data, outputting malformed tool calls, or making incorrect decisions. However, the continued development in this field paves the way for increasingly powerful agentic AI in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Tool calling is a powerful feature that allows large language models to interact with the real world, access live data, and perform complex calculations. By defining a set of tools and telling the model about them, you can create sophisticated applications that go far beyond simple text generation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "function_calling_with_keras_hub",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}