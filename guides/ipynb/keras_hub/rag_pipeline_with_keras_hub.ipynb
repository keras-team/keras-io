{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# RAG Pipeline with KerasHub\n",
    "\n",
    "**Author:** [Laxmareddy Patlolla](https://github.com/laxmareddyp), [Divyashree Sreepathihalli](https://github.com/divyashreepathihalli)<br>\n",
    "**Date created:** 2025/07/22<br>\n",
    "**Last modified:** 2025/07/24<br>\n",
    "**Description:** RAG pipeline for brain MRI analysis: image retrieval, context search, and report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "`Retrieval-Augmented Generation (RAG)` is a powerful technique that combines the strengths of\n",
    "large language models with external knowledge retrieval. Instead of relying solely on the\n",
    "model's pre-trained knowledge, RAG allows the model to access and use relevant information\n",
    "from a database or knowledge base to generate more accurate and contextually relevant responses.\n",
    "\n",
    "In this guide, we'll walk you through implementing a RAG pipeline for medical image analysis\n",
    "using KerasHub models. We'll show you how to:\n",
    "\n",
    "1. Load and configure `[MobileNetV3 + Gemma3 1B text model]` and `Gemma3 4B VLM model`\n",
    "2. Process brain MRI images and extract meaningful features\n",
    "3. Implement similarity search for retrieving relevant medical reports\n",
    "4. Generate comprehensive radiology reports using retrieved contex\n",
    "5. Compare RAG approach with direct vision-language model generation\n",
    "\n",
    "This pipeline demonstrates how to build a sophisticated medical AI system that can:\n",
    "- Analyze brain MRI images using state-of-the-art vision models\n",
    "- Retrieve relevant medical context from a database\n",
    "- Generate detailed radiology reports with proper medical terminology\n",
    "- Provide diagnostic impressions and treatment recommendations\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and configure our environment. We'll be using\n",
    "KerasHub to download and run the language models, and we'll need to authenticate with\n",
    "Kaggle to access the model weights. We'll also set up the JAX backend for optimal\n",
    "performance on GPU accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Kaggle Credentials Setup\n",
    "\n",
    "If running in Google Colab, set up Kaggle API credentials using the `google.colab.userdata` module to enable downloading models and datasets from Kaggle. This step is only required in Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "keras.config.set_dtype_policy(\"bfloat16\")\n",
    "import keras_hub\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets, image\n",
    "import re\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model Loading\n",
    "\n",
    "Loads the  `vision (MobileNetV3)` model (for image feature extraction) `Gemma3 1B text` model for report generation in RAG pipeline and the `Gemma3 4B` vision-language model for report generation in direct approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load and configure vision model for feature extraction, Gemma3 VLM for report generation, and a compact text model for benchmarking.\n",
    "    Returns:\n",
    "        tuple: (vision_model, vlm_model, text_model)\n",
    "    \"\"\"\n",
    "    # Vision model for feature extraction (lightweight MobileNetV3)\n",
    "    vision_model = keras_hub.models.ImageClassifier.from_preset(\n",
    "        \"mobilenet_v3_large_100_imagenet_21k\"\n",
    "    )\n",
    "    # Gemma3 Text model for report generation in RAG Pipeline (compact)\n",
    "    text_model = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
    "    # Gemma3 VLM for report generation (original, for benchmarking)\n",
    "    vlm_model = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_4b\")\n",
    "    return vision_model, vlm_model, text_model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Image and Caption Preparation\n",
    "\n",
    "Prepares `OASIS brain MRI` images and generates captions for each image. Returns lists of image paths and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_images_and_captions(oasis, images_dir=\"images\"):\n",
    "    \"\"\"\n",
    "    Prepare OASIS brain MRI images and generate captions.\n",
    "\n",
    "    Args:\n",
    "        oasis: OASIS dataset object containing brain MRI data\n",
    "        images_dir (str): Directory to save processed images\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image_paths, captions) - Lists of image paths and corresponding captions\n",
    "    \"\"\"\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    image_paths = []\n",
    "    captions = []\n",
    "    for i, img_path in enumerate(oasis.gray_matter_maps):\n",
    "        img = image.load_img(img_path)\n",
    "        data = img.get_fdata()\n",
    "        slice_ = data[:, :, data.shape[2] // 2]\n",
    "        slice_ = (\n",
    "            (slice_ - np.min(slice_)) / (np.max(slice_) - np.min(slice_)) * 255\n",
    "        ).astype(np.uint8)\n",
    "        img_pil = Image.fromarray(slice_)\n",
    "        fname = f\"oasis_{i}.png\"\n",
    "        fpath = os.path.join(images_dir, fname)\n",
    "        img_pil.save(fpath)\n",
    "        image_paths.append(fpath)\n",
    "        captions.append(f\"OASIS Brain MRI {i}\")\n",
    "    print(\"Saved 4 OASIS Brain MRI images:\", image_paths)\n",
    "    return image_paths, captions\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Image Visualization Utility\n",
    "\n",
    "Displays a set of processed brain MRI images with their corresponding captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_images(image_paths, captions):\n",
    "    \"\"\"\n",
    "    Visualize the processed brain MRI images.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        captions (list): List of corresponding image captions\n",
    "    \"\"\"\n",
    "    n = len(image_paths)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "    # If only one image, axes is not a list\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, (img_path, title) in enumerate(zip(image_paths, captions)):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img, cmap=\"gray\")\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.suptitle(\"OASIS Brain MRI Images\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prediction Visualization Utility\n",
    "\n",
    "Displays the query image and the most similar retrieved image from the database side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_prediction(query_img_path, db_image_paths, best_idx, db_reports):\n",
    "    \"\"\"\n",
    "    Visualize the query image and the most similar retrieved image.\n",
    "\n",
    "    Args:\n",
    "        query_img_path (str): Path to the query image\n",
    "        db_image_paths (list): List of database image paths\n",
    "        best_idx (int): Index of the most similar database image\n",
    "        db_reports (list): List of database reports\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].imshow(Image.open(query_img_path), cmap=\"gray\")\n",
    "    axes[0].set_title(\"Query Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(Image.open(db_image_paths[best_idx]), cmap=\"gray\")\n",
    "    axes[1].set_title(\"Retrieved Context Image\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.suptitle(\"Query and Most Similar Database Image\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Image Feature Extraction\n",
    "\n",
    "Extracts a feature vector from an image using the small `vision (MobileNetV3)` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_image_features(img_path, vision_model):\n",
    "    \"\"\"\n",
    "    Extract features from an image using the vision model.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the input image\n",
    "        vision_model: Pre-trained vision model for feature extraction\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Extracted feature vector\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((384, 384))\n",
    "    x = np.array(img) / 255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    features = vision_model(x)\n",
    "    return features\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## DB Reports\n",
    "\n",
    "List of example `radiology reports` corresponding to each database image. Used as context for the RAG pipeline to generate new reports for `query images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "db_reports = [\n",
    "    \"MRI shows a 1.5cm lesion in the right frontal lobe, non-enhancing, no edema.\",\n",
    "    \"Normal MRI scan, no abnormal findings.\",\n",
    "    \"Diffuse atrophy noted, no focal lesions.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Output Cleaning Utility\n",
    "\n",
    "Cleans the `generated text` output by removing prompt echoes and unwanted headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_generated_output(generated_text, prompt):\n",
    "    \"\"\"\n",
    "    Remove prompt echo and header details from generated text.\n",
    "\n",
    "    Args:\n",
    "        generated_text (str): Raw generated text from the language model\n",
    "        prompt (str): Original prompt used for generation\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text without prompt echo and headers\n",
    "    \"\"\"\n",
    "    # Remove the prompt from the beginning of the generated text\n",
    "    if generated_text.startswith(prompt):\n",
    "        cleaned_text = generated_text[len(prompt) :].strip()\n",
    "    else:\n",
    "        cleaned_text = generated_text.replace(prompt, \"\").strip()\n",
    "\n",
    "    # Remove header details and unwanted formatting\n",
    "    lines = cleaned_text.split(\"\\n\")\n",
    "    filtered_lines = []\n",
    "    skip_next = False\n",
    "    subheading_pattern = re.compile(r\"^(\\s*[A-Za-z0-9 .\\-()]+:)(.*)\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.replace(\"<end_of_turn>\", \"\").strip()\n",
    "        line = line.replace(\"**\", \"\")\n",
    "        line = line.replace(\"*\", \"\")\n",
    "        # Remove empty lines after headers (existing logic)\n",
    "        if any(\n",
    "            header in line\n",
    "            for header in [\n",
    "                \"**Patient:**\",\n",
    "                \"**Date of Exam:**\",\n",
    "                \"**Exam:**\",\n",
    "                \"**Referring Physician:**\",\n",
    "                \"**Patient ID:**\",\n",
    "                \"Patient:\",\n",
    "                \"Date of Exam:\",\n",
    "                \"Exam:\",\n",
    "                \"Referring Physician:\",\n",
    "                \"Patient ID:\",\n",
    "            ]\n",
    "        ):\n",
    "            continue\n",
    "        elif line.strip() == \"\" and skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        else:\n",
    "            # Split subheadings onto their own line if content follows\n",
    "            match = subheading_pattern.match(line)\n",
    "            if match and match.group(2).strip():\n",
    "                filtered_lines.append(match.group(1).strip())\n",
    "                filtered_lines.append(match.group(2).strip())\n",
    "                filtered_lines.append(\"\")  # Add a blank line after subheading\n",
    "            else:\n",
    "                filtered_lines.append(line)\n",
    "                # Add a blank line after subheadings (lines ending with ':')\n",
    "                if line.endswith(\":\") and (\n",
    "                    len(filtered_lines) == 1 or filtered_lines[-2] != \"\"\n",
    "                ):\n",
    "                    filtered_lines.append(\"\")\n",
    "            skip_next = False\n",
    "\n",
    "    # Remove any empty lines and excessive whitespace\n",
    "    cleaned_text = \"\\n\".join(\n",
    "        [l for l in filtered_lines if l.strip() or l == \"\"]\n",
    "    ).strip()\n",
    "\n",
    "    return cleaned_text\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## RAG Pipeline\n",
    "\n",
    "Implements the Retrieval-Augmented Generation (RAG) pipeline:\n",
    "\n",
    "- Extracts features from the query image and database images.\n",
    "- Finds the most similar image in the database.\n",
    "- Uses the retrieved report and the query image as input to the Gemma3 VLM to generate a new report.\n",
    "\n",
    "Returns the index of the matched image, the retrieved report, and the generated report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rag_pipeline(query_img_path, db_image_paths, db_reports, vision_model, text_model):\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation pipeline using vision model for retrieval and a compact text model for report generation.\n",
    "    Args:\n",
    "        query_img_path (str): Path to the query image\n",
    "        db_image_paths (list): List of database image paths\n",
    "        db_reports (list): List of database reports\n",
    "        vision_model: Vision model for feature extraction\n",
    "        text_model: Compact text model for report generation\n",
    "    Returns:\n",
    "        tuple: (best_idx, retrieved_report, generated_report)\n",
    "    \"\"\"\n",
    "    # Extract features for the query image\n",
    "    query_features = extract_image_features(query_img_path, vision_model)\n",
    "    # Extract features for the database images\n",
    "    db_features = np.vstack(\n",
    "        [extract_image_features(p, vision_model) for p in db_image_paths]\n",
    "    )\n",
    "    # Ensure features are numpy arrays for similarity search\n",
    "    db_features_np = np.array(db_features)\n",
    "    query_features_np = np.array(query_features)\n",
    "    # Similarity search\n",
    "    similarity = np.dot(db_features_np, query_features_np.T).squeeze()\n",
    "    best_idx = np.argmax(similarity)\n",
    "    retrieved_report = db_reports[best_idx]\n",
    "    print(f\"[RAG] Matched image index: {best_idx}\")\n",
    "    print(f\"[RAG] Matched image path: {db_image_paths[best_idx]}\")\n",
    "    print(f\"[RAG] Retrieved context/report:\\n{retrieved_report}\\n\")\n",
    "    PROMPT_TEMPLATE = (\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Based on the above radiology report and the provided brain MRI image, please:\\n\"\n",
    "        \"1. Provide a diagnostic impression.\\n\"\n",
    "        \"2. Explain the diagnostic reasoning.\\n\"\n",
    "        \"3. Suggest possible treatment options.\\n\"\n",
    "        \"Format your answer as a structured radiology report.\\n\"\n",
    "    )\n",
    "    prompt = PROMPT_TEMPLATE.format(context=retrieved_report)\n",
    "    # Generate report using the text model (text only, no image input)\n",
    "    output = text_model.generate(\n",
    "        {\n",
    "            \"prompts\": prompt,\n",
    "        }\n",
    "    )\n",
    "    cleaned_output = clean_generated_output(output, prompt)\n",
    "    return best_idx, retrieved_report, cleaned_output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Vision-Language Model (Direct Approach)\n",
    "\n",
    "Generates a radiology report directly from the `query image` using the `Gemma3(4B)` VLM model without retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def vlm_generate_report(query_img_path, vlm_model, question=None):\n",
    "    \"\"\"\n",
    "    Generate a radiology report directly from the image using a vision-language model.\n",
    "    Args:\n",
    "        query_img_path (str): Path to the query image\n",
    "        vlm_model: Pre-trained vision-language model (Gemma3(4B) VLM)\n",
    "        question (str): Optional question or prompt to include\n",
    "    Returns:\n",
    "        str: Generated radiology repor\n",
    "    \"\"\"\n",
    "    PROMPT_TEMPLATE = (\n",
    "        \"Based on the provided brain MRI image, please:\\n\"\n",
    "        \"1. Provide a diagnostic impression.\\n\"\n",
    "        \"2. Explain the diagnostic reasoning.\\n\"\n",
    "        \"3. Suggest possible treatment options.\\n\"\n",
    "        \"Format your answer as a structured radiology report.\\n\"\n",
    "    )\n",
    "    if question is None:\n",
    "        question = \"\"\n",
    "    # Preprocess the image as required by the model\n",
    "    img = Image.open(query_img_path).convert(\"RGB\").resize((224, 224))\n",
    "    image = np.array(img) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # Generate report using the VLM\n",
    "    output = vlm_model.generate(\n",
    "        {\n",
    "            \"images\": image,\n",
    "            \"prompts\": PROMPT_TEMPLATE.format(question=question),\n",
    "        }\n",
    "    )\n",
    "    # Clean the generated outpu\n",
    "    cleaned_output = clean_generated_output(\n",
    "        output, PROMPT_TEMPLATE.format(question=question)\n",
    "    )\n",
    "    return cleaned_output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Main Execution Pipeline\n",
    "\n",
    "This section loads models, prepares data, runs the RAG pipeline, and compares RAG with direct VLM generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    vision_model, vlm_model, text_model = load_models()\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"Preparing OASIS dataset...\")\n",
    "    oasis = datasets.fetch_oasis_vbm(n_subjects=4)  # Use 4 images\n",
    "    print(\"Download dataset is completed.\")\n",
    "    image_paths, captions = prepare_images_and_captions(oasis)\n",
    "    visualize_images(image_paths, captions)\n",
    "\n",
    "    # Split data: first 3 as database, last as query\n",
    "    db_image_paths = image_paths[:-1]\n",
    "    query_img_path = image_paths[-1]\n",
    "\n",
    "    # Extract database features\n",
    "    print(\"Extracting database features...\")\n",
    "    db_features = np.vstack(\n",
    "        [extract_image_features(p, vision_model) for p in db_image_paths]\n",
    "    )\n",
    "\n",
    "    # Run RAG pipeline\n",
    "    print(\"Running RAG pipeline...\")\n",
    "    best_idx, retrieved_report, generated_report = rag_pipeline(\n",
    "        query_img_path, db_image_paths, db_reports, vision_model, text_model\n",
    "    )\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_prediction(query_img_path, db_image_paths, best_idx, db_reports)\n",
    "\n",
    "    # Print RAG results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"RAG PIPELINE RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nMatched DB Report Index: {best_idx}\")\n",
    "    print(f\"Matched DB Report: {retrieved_report}\")\n",
    "    print(\"\\n--- Generated Report ---\\n\", generated_report)\n",
    "\n",
    "    # Run VLM (direct approach)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"VLM RESULTS (Direct Approach)\")\n",
    "    print(\"=\" * 50)\n",
    "    vlm_report = vlm_generate_report(query_img_path, vlm_model)\n",
    "    print(\"\\n--- Vision-Language Model (No Retrieval) Report ---\\n\", vlm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Comparison: RAG Pipeline vs Direct VLM\n",
    "\n",
    "- **MobileNet + Gemma3(1B) text model**: ~1B total parameters\n",
    "- **Gemma3 VLM 4B model**: ~4B total parameters\n",
    "- **Results**: The RAG pipeline `(MobileNetV3 + Gemma3(1B) text model)` report is better due to its use of retrieval and context, providing more relevant and accurate reports with fewer parameters.\n",
    "\n",
    "**Detailed Comparison:**\n",
    "\n",
    "- **Accuracy & Relevance:**\n",
    "\n",
    "  - RAG pipeline leverages retrieval to provide contextually relevant and case-specific reports, often matching or exceeding the quality of much larger VLMs.\n",
    "  - Direct VLM `Gemma3(4B)` produces more generic outputs, lacking access to specific prior cases.\n",
    "\n",
    "- **Speed & Resource Usage:**\n",
    "\n",
    "  - `MobileNetV3 + Gemma3(1B)` is significantly faster and more memory-efficient, making it suitable for edge devices and real-time applications.\n",
    "  - `Gemma3(4B)` requires more computational resources and is slower, especially on limited hardware.\n",
    "\n",
    "- **Scalability & Flexibility:**\n",
    "\n",
    "  - The RAG approach allows easy swapping of retriever/generator models and can be adapted to different domains or datasets.\n",
    "  - Direct VLM is less flexible and requires retraining or fine-tuning for new domains.\n",
    "\n",
    "- **Interpretability:**\n",
    "\n",
    "  - RAG pipeline provides traceability by showing which database report was used for context, aiding in clinical interpretability and trust.\n",
    "  - Direct VLM does not provide this transparency.\n",
    "\n",
    "- **Practical Implications:**\n",
    "\n",
    "  - RAG is more practical for deployment in resource-constrained environments and can be incrementally improved by updating the database.\n",
    "  - Large VLMs are best suited for cloud or high-performance environments.\n",
    "\n",
    "In practice, the RAG approach leverages both image similarity and prior knowledge to generate more precise and clinically meaningful reports, while the direct VLM approach is limited to general knowledge and lacks case-specific context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This demonstration showcases the power of Retrieval-Augmented Generation (RAG) in combining vision and language models for intelligent analysis using KerasHub models.\n",
    "\n",
    "**Key Achievements:**\n",
    "\n",
    "- Model Integration:`[MobileNetV3 + Gemma3(1B) text model]` and `Gemma3(4B) VLM model` via `KerasHub`\n",
    "- Feature Extraction: Meaningful features from brain MRI images\n",
    "- Similarity Search: Efficient retrieval of relevant context\n",
    "- Report Generation: Comprehensive reports using retrieved context\n",
    "- Comparison Analysis: RAG vs direct VLM approaches\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "- Enhanced Accuracy: More contextually relevant outputs\n",
    "- Scalable Architecture: Easy to extend with different models\n",
    "- KerasHub Integration: State-of-the-art models efficiently\n",
    "- Real-world Applicability: Various vision-language tasks\n",
    "\n",
    "This guide demonstrates how KerasHub enables rapid prototyping and deployment of advanced AI systems for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Security Warning\n",
    "\n",
    "\u26a0\ufe0f **IMPORTANT SECURITY AND PRIVACY CONSIDERATIONS**\n",
    "\n",
    "This pipeline is for educational purposes only. For production use:\n",
    "\n",
    "- Anonymize medical data following HIPAA guidelines\n",
    "- Implement access controls and encryption\n",
    "- Validate inputs and secure APIs\n",
    "- Consult medical professionals for clinical decisions\n",
    "- This system should NOT be used for actual medical diagnosis without proper validation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rag_pipeline_with_keras_hub",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}