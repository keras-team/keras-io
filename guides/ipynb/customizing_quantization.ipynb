{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Customizing Quantization with QuantizationConfig\n",
    "\n",
    "**Author:** [Jyotinder Singh](https://x.com/Jyotinder_Singh)<br>\n",
    "**Date created:** 2025/12/18<br>\n",
    "**Last modified:** 2025/12/18<br>\n",
    "**Description:** Guide on using QuantizationConfig for weight-only quantization and custom quantizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This guide explores the flexible `QuantizationConfig` API in Keras, introduced to give you granular control over how your models are quantized.\n",
    "While `model.quantize(\"int8\")` provides a great default, you often need more control. For example, to perform **weight-only quantization** (common in LLMs) or to use **custom quantization schemes** (like percentile-based clipping).\n",
    "\n",
    "We will cover:\n",
    "\n",
    "1.  **Customizing INT8 Quantization**: Modifying the default parameters (e.g., custom value range).\n",
    "2.  **Weight-Only Quantization (INT4)**: Quantizing weights to 4-bit while keeping activations in float, using `Int4QuantizationConfig`.\n",
    "3.  **Custom Quantizers**: Implementing a completely custom quantizer (e.g., `PercentileQuantizer`) and using it with `QuantizationConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import ops\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Builds a simple Sequential model for demonstration.\"\"\"\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(10,)),\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 1. Customizing INT8 Quantization\n",
    "\n",
    "By default, `model.quantize(\"int8\")` uses `AbsMaxQuantizer` for both weights and activations which uses the default value range of [-127, 127].\n",
    "You might want to specify different parameters, such as a restricted value range (if you expect your activations to be within a certain range).\n",
    "You can do this by creating an `Int8QuantizationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.quantizers import Int8QuantizationConfig, AbsMaxQuantizer\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Create a custom config\n",
    "# Here we restrict the weight range to [-100, 100] instead of the default [-127, 127]\n",
    "custom_int8_config = Int8QuantizationConfig(\n",
    "    weight_quantizer=AbsMaxQuantizer(value_range=(-100, 100), axis=0),\n",
    "    activation_quantizer=AbsMaxQuantizer(value_range=(-100, 100), axis=-1),\n",
    ")\n",
    "\n",
    "# Apply quantization with the custom config\n",
    "model.quantize(config=custom_int8_config)\n",
    "\n",
    "print(\"Layer 0 kernel dtype:\", model.layers[0].kernel.dtype)\n",
    "# Ensure all kernel values are within the specified range\n",
    "assert ops.all(\n",
    "    ops.less_equal(model.layers[0].kernel, 100)\n",
    "), \"Kernel values are not <= 100\"\n",
    "assert ops.all(\n",
    "    ops.greater_equal(model.layers[0].kernel, -100)\n",
    "), \"Kernel values are not >= -100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 2. Weight-Only Quantization (INT4)\n",
    "\n",
    "By default, `model.quantize(\"int4\")` quantizes activations to INT8 while keeping weights in INT4.\n",
    "For large language models and memory-constrained environments, **weight-only quantization** is a popular technique.\n",
    "It reduces the model size significantly (keeping weights in 4-bit) while maintaining higher precision for activations.\n",
    "\n",
    "To achieve this, we set `activation_quantizer=None` in the `Int4QuantizationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.quantizers import Int4QuantizationConfig\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Define Int4 weight-only config\n",
    "# We enable Int4 for weights, but disable activation quantization by setting it to None.\n",
    "# Note that we use `\"int8\"` as the output dtype since TensorFlow and PyTorch don't support\n",
    "# `int4`. However, we still benefit from the lower memory usage of int4 weights because of\n",
    "# bitpacking implemented by Keras.\n",
    "custom_int4_config = Int4QuantizationConfig(\n",
    "    weight_quantizer=AbsMaxQuantizer(value_range=(-8, 7), output_dtype=\"int8\", axis=0),\n",
    "    activation_quantizer=None,\n",
    ")\n",
    "\n",
    "model.quantize(config=custom_int4_config)\n",
    "\n",
    "# Verify that weights are quantized (int8 backing int4) but no activation quantization logic is added\n",
    "print(\"Layer 0 kernel dtype:\", model.layers[0].kernel.dtype)\n",
    "print(\"Layer 0 has inputs_quantizer:\", model.layers[0].inputs_quantizer is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 3. Custom Quantizers: Implementing a Percentile Quantizer\n",
    "\n",
    "Sometimes, standard absolute-max quantization isn't enough. You might want to be robust to outliers by using **percentile-based quantization**.\n",
    "Keras allows you to define your own quantizer by subclassing `keras.quantizers.Quantizer`.\n",
    "\n",
    "Below is an implementation of a `PercentileQuantizer` that sets the scale based on a specified percentile of the absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.quantizers import Quantizer\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "class PercentileQuantizer(Quantizer):\n",
    "    \"\"\"Quantizes x using the percentile-based scale.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        percentile=99.9,\n",
    "        value_range=(-127, 127),  # Default range for int8\n",
    "        epsilon=backend.epsilon(),\n",
    "        output_dtype=\"int8\",  # Default dtype for int8\n",
    "    ):\n",
    "        super().__init__(output_dtype=output_dtype)\n",
    "        self.percentile = percentile\n",
    "        self.value_range = value_range\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, x, axis, to_numpy=False):\n",
    "        \"\"\"Quantizes x using the percentile-based scale.\n",
    "\n",
    "        `to_numpy` can be set to True to perform the computation on the host CPU,\n",
    "        which saves device memory.\n",
    "        \"\"\"\n",
    "        # 1. Compute the percentile value of absolute inputs\n",
    "        x_abs = ops.abs(x)\n",
    "\n",
    "        if to_numpy:\n",
    "            x_np = ops.convert_to_numpy(x_abs)\n",
    "            max_val = np.percentile(x_np, self.percentile, axis=axis, keepdims=True)\n",
    "        else:\n",
    "            max_val = ops.quantile(\n",
    "                x_abs, self.percentile / 100, axis=axis, keepdims=True\n",
    "            )\n",
    "\n",
    "        # 2. Compute scale\n",
    "        # scale = range_max / max_val\n",
    "        # We ensure max_val is at least epsilon\n",
    "        scale = ops.divide(self.value_range[1], ops.add(max_val, self.epsilon))\n",
    "        if not to_numpy:\n",
    "            scale = ops.cast(scale, backend.standardize_dtype(x.dtype))\n",
    "\n",
    "        # 3. Quantize\n",
    "        # q = x * scale\n",
    "        outputs = ops.multiply(x, scale)\n",
    "        outputs = ops.clip(ops.round(outputs), self.value_range[0], self.value_range[1])\n",
    "        outputs = ops.cast(outputs, self.output_dtype)\n",
    "\n",
    "        return outputs, scale\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the config of the quantizer for serialization support.\"\"\"\n",
    "        return {\n",
    "            \"percentile\": self.percentile,\n",
    "            \"value_range\": self.value_range,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"output_dtype\": self.output_dtype,\n",
    "        }\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now we can use this `PercentileQuantizer` in our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "# Use the custom quantizer for activations\n",
    "custom_int8_config = Int8QuantizationConfig(\n",
    "    weight_quantizer=AbsMaxQuantizer(axis=0),\n",
    "    activation_quantizer=PercentileQuantizer(percentile=99.9),\n",
    ")\n",
    "\n",
    "model.quantize(config=custom_int8_config)\n",
    "\n",
    "# Verify the integration\n",
    "print(\n",
    "    \"Layer 0 uses custom activation quantizer:\",\n",
    "    isinstance(model.layers[0].inputs_quantizer, PercentileQuantizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "With `QuantizationConfig`, you are no longer limited to stock quantization options.\n",
    "Whether you need weight-only quantization or custom quantizers for specialized hardware or research,\n",
    "Keras provides the modularity to build exactly what you need."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "customizing_quantization",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}