{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Define a Custom TPU/GPU Kernel\n",
    "\n",
    "**Author:** [jeffcarp](https://www.jeffcarp.com/)<br>\n",
    "**Date created:** 2025/12/18<br>\n",
    "**Last modified:** 2025/12/18<br>\n",
    "**Description:** Write high-performance custom Keras layers for TPUs and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# How to Write a Custom TPU or GPU Kernel in Keras\n",
    "\n",
    "Keras has [many pre-made layers to choose from](/api/layers/), and the\n",
    "ability to easily [create your\n",
    "own](/guides/making_new_layers_and_models_via_subclassing/) if you can't\n",
    "find the exact one you need. However, if you have a need for speed, or otherwise\n",
    "need to customize the exact behavior of your model at the hardware level, you\n",
    "may want to look into writing a custom kernel. A good way to know if you need a\n",
    "custom kernel is to look at the profile of your model and see if there are any\n",
    "idle gaps caused by computation or memory transfer bottlenecks (see the\n",
    "[TensorBoard callback](/api/callbacks/tensorboard/) for how to get a profile).\n",
    "\n",
    "This guide will explore how to write a custom kernel and add it to your\n",
    "Keras model. We will utilize **Pallas**, a library that lets you write\n",
    "kernels in Python that can run on both TPU or GPU, where they're lowered\n",
    "to Mosaic or Triton, respectively. You can learn more in the [Pallas\n",
    "docs](https://docs.jax.dev/en/latest/pallas/index.html).\n",
    "\n",
    "**Compatibility note:** Pallas is only available when using the JAX backend on\n",
    "certain hardware:\n",
    "\n",
    "- TPU v4 and above\n",
    "- NVIDIA Ampere GPUs (compute capability 8.0) and above\n",
    "\n",
    "If you're running in Colab, the v5e-1 in the free tier supports running this\n",
    "guide.\n",
    "\n",
    "First, make sure you're running the latest version of `libtpu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import jax\n",
    "from jax.experimental import pallas as pl\n",
    "import jax.numpy as jnp\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Simple Example\n",
    "\n",
    "Let's start with the example from the [Pallas\n",
    "quickstart](https://docs.jax.dev/en/latest/pallas/quickstart.html): a simple\n",
    "kernel to add two vectors together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_vectors_kernel(x_ref, y_ref, o_ref):\n",
    "    \"\"\"Pallas kernel for adding two vectors together.\"\"\"\n",
    "    x, y = x_ref[...], y_ref[...]\n",
    "    o_ref[...] = x + y\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now jit-compile the Pallas function into a function that can be used by JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return pl.pallas_call(\n",
    "        add_vectors_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n",
    "    )(x, y)\n",
    "\n",
    "\n",
    "add_vectors(jnp.arange(8), jnp.arange(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now we can embed the jitted `add_vectors` function containing the Pallas kernel into a\n",
    "Keras layer, just by calling it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PallasAddLayer(keras.Layer):\n",
    "    def call(self, x, y):\n",
    "        # Reuse the JIT-compiled Pallas function\n",
    "        return add_vectors(x, y)\n",
    "\n",
    "\n",
    "layer = PallasAddLayer()\n",
    "\n",
    "x_data = jnp.arange(8, dtype=jnp.int32)\n",
    "y_data = jnp.arange(8, dtype=jnp.int32)\n",
    "\n",
    "layer(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That's how to integrate a Pallas kernel into a Keras layer! Now for a more\n",
    "in-depth example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Writing a Fused Linear Activation Layer\n",
    "\n",
    "Some common reasons you might want to write a custom kernel is to take advantage of\n",
    "**fusion** and **tiling**.\n",
    "\n",
    "**Operator fusion** is the process of combining two or more ops into one \"fused\" op, for\n",
    "example instead of calling `keras.ops.matmul` then `keras.ops.relu` sequentially, we\n",
    "could write a custom op that combines both into one more efficient operator.\n",
    "XLA already [does operator fusion when possible](https://arxiv.org/abs/2301.13062) for\n",
    "certain use cases, but to squeeze even more performance out of the TPU or GPU, we need to\n",
    "write a custom op to specify the fusion exactly.\n",
    "\n",
    "**Tiling** is the ability to control how blocks of memory are loaded from the TPU or\n",
    "GPU's larger High Bandwidth Memory (HBM) to the smaller, extremely fast on-chip\n",
    "memory (called VMEM on TPU or SMEM on GPU) that the accelerator's computation\n",
    "units (e.g., TPU's Matrix Units or a GPU's Tensor Cores) use directly. This is\n",
    "critical for improving the performance of large matrix multiplications, for\n",
    "example those in the MLP layer at the end of Transformer blocks.\n",
    "\n",
    "In Pallas, tiling is controlled by the `BlockSpec`. Learn more in the\n",
    "[Pallas BlockSpec guide\n",
    "here](https://docs.jax.dev/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs).\n",
    "\n",
    "In this section, we'll take two operations that commonly appear together: a\n",
    "matrix multiplication (like in a `Dense` layer) and a ReLU activation. We will\n",
    "write a new op that fuses them together for better performance.\n",
    "\n",
    "## Original Unoptimized Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class StandardDenseReLU(keras.layers.Layer):\n",
    "    \"\"\"Standard Matmul and ReLU implementation using keras.ops.\"\"\"\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The standard implementation performs two separate operations.\n",
    "        # Each one involves expensive data transfer with the main device memory (HBM).\n",
    "        # 1. Matmul: inputs (HBM) -> compute -> intermediate (HBM)\n",
    "        y = keras.ops.matmul(inputs, self.w)\n",
    "        # 2. ReLU: intermediate (HBM) -> compute -> output (HBM)\n",
    "        return keras.ops.relu(y)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 1. Define the Fused Kernel\n",
    "\n",
    "First we create an inner kernel function that defines the fused computation that\n",
    "combines both matmul (`pl.dot`) and activation (`jnp.maximum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "\n",
    "\n",
    "def matmul_relu_kernel(a_ref, b_ref, c_ref):\n",
    "    \"\"\"Pallas kernel for fused matmul + ReLU.\"\"\"\n",
    "    # Perform the matrix multiplication on the local tile\n",
    "    # pl.dot leverages the hardware's Matrix Unit (MXU)\n",
    "    acc = pl.dot(a_ref[...], b_ref[...])\n",
    "\n",
    "    # Fusion happens here: apply activation while data is in VMEM\n",
    "    result = jnp.maximum(acc, 0)\n",
    "\n",
    "    # Write the final result to the output reference\n",
    "    c_ref[...] = result\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 2. Specify the Tiling (BlockSpec)\n",
    "\n",
    "Since the input matrices are usually too large to fit into VMEM, Pallas needs ot\n",
    "know how to \"slice\" them for loading from HBM to VMEM.\n",
    "\n",
    "We define this using `BlockSpec` - this tells the hardware: \"Take a 128-row\n",
    "chunk of Matrix A and a 128-column chunk of Matrix B to produce a 128x128 tile\n",
    "of Matrix C.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def fused_matmul(a, b):\n",
    "    m, k = a.shape\n",
    "    _, n = b.shape\n",
    "\n",
    "    # Define tile sizes\n",
    "    tile_m, tile_k, tile_n = 128, 128, 128\n",
    "    assert (\n",
    "        m % tile_m == 0 and k % tile_k == 0 and n % tile_n == 0\n",
    "    ), \"Inputs must be multiples of 128 for this demo\"\n",
    "\n",
    "    return pl.pallas_call(\n",
    "        matmul_relu_kernel,\n",
    "        # Map output indices to input blocks\n",
    "        out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),\n",
    "        in_specs=[\n",
    "            # For each output tile, we take a (tile_m, tile_k) slice of A\n",
    "            pl.BlockSpec(index_map=lambda i, j: (i, 0), block_shape=(tile_m, tile_k)),\n",
    "            # For each output tile, we take a (tile_k, tile_n) slice of B\n",
    "            pl.BlockSpec(index_map=lambda i, j: (0, j), block_shape=(tile_k, tile_n)),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec(\n",
    "            index_map=lambda i, j: (i, j), block_shape=(tile_m, tile_n)\n",
    "        ),\n",
    "        grid=(m // tile_m, n // tile_n),\n",
    "    )(a, b)\n",
    "\n",
    "\n",
    "fused_matmul(jnp.ones((256, 256)), jnp.ones((256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 3. Integrating into a Keras Layer\n",
    "\n",
    "Now for the final step, call the jit-compiled `fused_matmul` kernel from a\n",
    "`keras.Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FusedDense(keras.layers.Layer):\n",
    "    \"\"\"Custom Keras layer that applies the fused Dense and ReLU op.\"\"\"\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units), initializer=\"glorot_uniform\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Dispatch to our Pallas kernel\n",
    "        return fused_matmul(inputs, self.w.value)\n",
    "\n",
    "\n",
    "FusedDense(256)(jnp.ones((256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 4. Benchmarking the Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Data\n",
    "N = 8192  # Large enough to be memory bound\n",
    "input_data = jnp.ones((N, N), dtype=\"float32\")\n",
    "\n",
    "# Initialize layers\n",
    "standard_layer = StandardDenseReLU(units=N)\n",
    "pallas_layer = FusedDense(units=N)\n",
    "\n",
    "# Build layers by calling them once\n",
    "standard_layer(input_data)\n",
    "pallas_layer(input_data)\n",
    "\n",
    "\n",
    "def benchmark(layer, x, name, iterations=100):\n",
    "    # Warm up to ensure JIT compilation is finished\n",
    "    for _ in range(10):\n",
    "        layer(x).block_until_ready()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        layer(x).block_until_ready()\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    avg_time = (end_time - start_time) / iterations * 1000  # convert to ms\n",
    "    print(f\"{name} Average Latency: {avg_time:.3f} ms\")\n",
    "\n",
    "\n",
    "# 2. Run Comparison\n",
    "print(f\"Benchmarking Matrix Size: {N}x{N}\\n\" + \"-\" * 30)\n",
    "benchmark(standard_layer, input_data, \"Standard Keras (Matmul + ReLU)\")\n",
    "benchmark(pallas_layer, input_data, \"Pallas Fused (Matmul + ReLU)\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Why this Works\n",
    "\n",
    "**Memory Bandwidth Efficiency:** By fusing the matrix multiplication and\n",
    "activation, we perform the ReLU computation while data is still in the chip's\n",
    "fast VMEM. This drastically reduces expensive read/write roundtrips to HBM.\n",
    "\n",
    "**Automatic Parallelization:** Pallas handles the \"grid\" execution, meaning\n",
    "it automatically parallelizes your defined tiles across the available hardware\n",
    "cores (whether TPU MXUs or GPU Tensor Cores).\n",
    "\n",
    "**Drop-in Inference Speed:** This `FusedDense` kernel can be integrated into any\n",
    "Keras model, giving an example of improving serving/inference performance with\n",
    "minimal code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 5. Enabling Training\n",
    "\n",
    "In order for a Pallas kernel to be trainable, you must also supply\n",
    "a second kernel to define the custom backward pass, since JAX can't\n",
    "[AutoGrad](https://docs.jax.dev/en/latest/automatic-differentiation.html)\n",
    "through Pallas kernels. Without it, you might see an error like this:\n",
    "\n",
    "```\n",
    "model = keras.Sequential([FusedDense(256)])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(jnp.ones((256, 256)), jnp.ones((256, 256)))\n",
    ">>> Linearization failed to produce known values for all output primals. This is\n",
    "typically caused by attempting to differentiate a function uses an operation\n",
    "that does not support reverse-mode autodiff.\n",
    "```\n",
    "\n",
    "To extend our fused matmul example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Define the wrapper with `custom_vjp` using our original `fused_matmul`.\n",
    "@jax.custom_vjp\n",
    "def fused_matmul_trainable(x, w):\n",
    "    return fused_matmul(x, w)\n",
    "\n",
    "\n",
    "# 2. Define the Forward Pass\n",
    "# It must return the output AND \"residuals\" (data needed for the backward pass)\n",
    "def fused_matmul_fwd(x, w):\n",
    "    y = fused_matmul_trainable(x, w)\n",
    "    # We save inputs x, w and output y for the backward calculation\n",
    "    return y, (x, w, y)\n",
    "\n",
    "\n",
    "# 3. Define the Backward Pass\n",
    "# JAX gives us the residuals and the incoming gradient (g)\n",
    "def fused_matmul_bwd(residuals, g):\n",
    "    x, w, y = residuals\n",
    "\n",
    "    # Calculate the gradient of ReLU: 1 if y > 0, else 0\n",
    "    # g is the gradient flowing back from the next layer\n",
    "    grad_relu = g * (y > 0)\n",
    "\n",
    "    # Standard backprop math for matmul:\n",
    "    # grad_x = grad_relu @ w.T\n",
    "    grad_x = jnp.dot(grad_relu, w.T)\n",
    "\n",
    "    # grad_w = x.T @ grad_relu\n",
    "    grad_w = jnp.dot(x.T, grad_relu)\n",
    "\n",
    "    return grad_x, grad_w\n",
    "\n",
    "\n",
    "# 4. Register the forward and backward functions\n",
    "fused_matmul_trainable.defvjp(fused_matmul_fwd, fused_matmul_bwd)\n",
    "\n",
    "\n",
    "class FusedDenseTrainable(FusedDense):\n",
    "    \"\"\"Updated layer that contains Pallas forward and backward pass.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Dispatch to our trainable Pallas kernel\n",
    "        return fused_matmul_trainable(inputs, self.w.value)\n",
    "\n",
    "\n",
    "# Demonstrate trainability on dummy data\n",
    "model = keras.Sequential([FusedDenseTrainable(256)])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(jnp.ones((256, 256)), jnp.ones((256, 256)), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Followups\n",
    "\n",
    "In this guide we covered how to define a simple custom Pallas kernel performing vector\n",
    "addition to include in a Keras model. Then we followed up with a more in-depth\n",
    "example of a fused matmul + activation kernel that you might use in a real-world\n",
    "model to improve performance.\n",
    "\n",
    "Please refer to the [Pallas\n",
    "docs](https://docs.jax.dev/en/latest/pallas/index.html#) for further\n",
    "documentation on writing custom kernels. Additionally to explore more examples\n",
    "of Pallas kernels, including FlashAttention and MoE layers, check out the\n",
    "[Tokamax](https://github.com/openxla/tokamax) library."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "define_custom_kernel",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}