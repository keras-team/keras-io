{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tune hyperparameters in your custom training loop\n",
    "\n",
    "**Authors:** Tom O'Malley, Haifeng Jin<br>\n",
    "**Date created:** 2019/10/28<br>\n",
    "**Last modified:** 2022/01/12<br>\n",
    "**Description:** Use `HyperModel.fit()` to tune training hyperparameters (such as batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The `HyperModel` class in KerasTuner provides a convenient way to define your\n",
    "search space in a reusable object. You can override `HyperModel.build()` to\n",
    "define and hypertune the model itself. To hypertune the training process (e.g.\n",
    "by selecting the proper batch size, number of training epochs, or data\n",
    "augmentation setup), you can override `HyperModel.fit()`, where you can access:\n",
    "\n",
    "- The `hp` object, which is an instance of `keras_tuner.HyperParameters`\n",
    "- The model built by `HyperModel.build()`\n",
    "\n",
    "A basic example is shown in the \"tune model training\" section of\n",
    "[Getting Started with KerasTuner](https://keras.io/guides/keras_tuner/getting_started/#tune-model-training).\n",
    "\n",
    "## Tuning the custom training loop\n",
    "\n",
    "In this guide, we will subclass the `HyperModel` class and write a custom\n",
    "training loop by overriding `HyperModel.fit()`. For how to write a custom\n",
    "training loop with Keras, you can refer to the guide\n",
    "[Writing a training loop from scratch](https://keras.io/guides/writing_a_training_loop_from_scratch/).\n",
    "\n",
    "First, we import the libraries we need, and we create datasets for training and\n",
    "validation. Here, we just use some random data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.random.rand(1000, 28, 28, 1)\n",
    "y_train = np.random.randint(0, 10, (1000, 1))\n",
    "x_val = np.random.rand(1000, 28, 28, 1)\n",
    "y_val = np.random.randint(0, 10, (1000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then, we subclass the `HyperModel` class as `MyHyperModel`. In\n",
    "`MyHyperModel.build()`, we build a simple Keras model to do image\n",
    "classification for 10 different classes. `MyHyperModel.fit()` accepts several\n",
    "arguments. Its signature is shown below:\n",
    "\n",
    "```python\n",
    "def fit(self, hp, model, x, y, validation_data, callbacks=None, **kwargs):\n",
    "```\n",
    "\n",
    "* The `hp` argument is for defining the hyperparameters.\n",
    "* The `model` argument is the model returned by `MyHyperModel.build()`.\n",
    "* `x`, `y`, and `validation_data` are all custom-defined arguments. We will\n",
    "pass our data to them by calling `tuner.search(x=x, y=y,\n",
    "validation_data=(x_val, y_val))` later. You can define any number of them and\n",
    "give custom names.\n",
    "* The `callbacks` argument was intended to be used with `model.fit()`.\n",
    "KerasTuner put some helpful Keras callbacks in it, for example, the callback\n",
    "for checkpointing the model at its best epoch.\n",
    "\n",
    "We will manually call the callbacks in the custom training loop. Before we\n",
    "can call them, we need to assign our model to them with the following code so\n",
    "that they have access to the model for checkpointing.\n",
    "\n",
    "```py\n",
    "for callback in callbacks:\n",
    "    callback.model = model\n",
    "```\n",
    "\n",
    "In this example, we only called the `on_epoch_end()` method of the callbacks\n",
    "to help us checkpoint the model. You may also call other callback methods\n",
    "if needed. If you don't need to save the model, you don't need to use the\n",
    "callbacks.\n",
    "\n",
    "In the custom training loop, we tune the batch size of the dataset as we wrap\n",
    "the NumPy data into a `tf.data.Dataset`. Note that you can tune any\n",
    "preprocessing steps here as well. We also tune the learning rate of the\n",
    "optimizer.\n",
    "\n",
    "We will use the validation loss as the evaluation metric for the model. To\n",
    "compute the mean validation loss, we will use `keras.metrics.Mean()`, which\n",
    "averages the validation loss across the batches. We need to return the\n",
    "validation loss for the tuner to make a record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        \"\"\"Builds a convolutional model.\"\"\"\n",
    "        inputs = keras.Input(shape=(28, 28, 1))\n",
    "        x = keras.layers.Flatten()(inputs)\n",
    "        x = keras.layers.Dense(\n",
    "            units=hp.Choice(\"units\", [32, 64, 128]), activation=\"relu\"\n",
    "        )(x)\n",
    "        outputs = keras.layers.Dense(10)(x)\n",
    "        return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    def fit(self, hp, model, x, y, validation_data, callbacks=None, **kwargs):\n",
    "        # Convert the datasets to tf.data.Dataset.\n",
    "        batch_size = hp.Int(\"batch_size\", 32, 128, step=32, default=64)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(\n",
    "            batch_size\n",
    "        )\n",
    "        validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(\n",
    "            batch_size\n",
    "        )\n",
    "\n",
    "        # Define the optimizer.\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\", default=1e-3)\n",
    "        )\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # The metric to track validation loss.\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        # Function to run the train step.\n",
    "        @tf.function\n",
    "        def run_train_step(images, labels):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(labels, logits)\n",
    "                # Add any regularization losses.\n",
    "                if model.losses:\n",
    "                    loss += tf.math.add_n(model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Function to run the validation step.\n",
    "        @tf.function\n",
    "        def run_val_step(images, labels):\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(labels, logits)\n",
    "            # Update the metric.\n",
    "            epoch_loss_metric.update_state(loss)\n",
    "\n",
    "        # Assign the model to the callbacks.\n",
    "        for callback in callbacks:\n",
    "            callback.model = model\n",
    "\n",
    "        # Record the best validation loss value\n",
    "        best_epoch_loss = float(\"inf\")\n",
    "\n",
    "        # The custom training loop.\n",
    "        for epoch in range(2):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            # Iterate the training data to run the training step.\n",
    "            for images, labels in train_ds:\n",
    "                run_train_step(images, labels)\n",
    "\n",
    "            # Iterate the validation data to run the validation step.\n",
    "            for images, labels in validation_data:\n",
    "                run_val_step(images, labels)\n",
    "\n",
    "            # Calling the callbacks after epoch.\n",
    "            epoch_loss = float(epoch_loss_metric.result().numpy())\n",
    "            for callback in callbacks:\n",
    "                # The \"my_metric\" is the objective passed to the tuner.\n",
    "                callback.on_epoch_end(epoch, logs={\"my_metric\": epoch_loss})\n",
    "            epoch_loss_metric.reset_states()\n",
    "\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "            best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
    "\n",
    "        # Return the evaluation metric value.\n",
    "        return best_epoch_loss\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, we can initialize the tuner. Here, we use `Objective(\"my_metric\", \"min\")`\n",
    "as our metric to be minimized. The objective name should be consistent with the\n",
    "one you use as the key in the `logs` passed to the 'on_epoch_end()' method of\n",
    "the callbacks. The callbacks need to use this value in the `logs` to find the\n",
    "best epoch to checkpoint the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    objective=keras_tuner.Objective(\"my_metric\", \"min\"),\n",
    "    max_trials=2,\n",
    "    hypermodel=MyHyperModel(),\n",
    "    directory=\"results\",\n",
    "    project_name=\"custom_training\",\n",
    "    overwrite=True,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We start the search by passing the arguments we defined in the signature of\n",
    "`MyHyperModel.fit()` to `tuner.search()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.search(x=x_train, y=y_train, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, we can retrieve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hps.values)\n",
    "\n",
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In summary, to tune the hyperparameters in your custom training loop, you just\n",
    "override `HyperModel.fit()` to train the model and return the evaluation\n",
    "results. With the provided callbacks, you can easily save the trained models at\n",
    "their best epochs and load the best models later.\n",
    "\n",
    "To find out more about the basics of KerasTuner, please see\n",
    "[Getting Started with KerasTuner](https://keras.io/guides/keras_tuner/getting_started/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_tuner",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}