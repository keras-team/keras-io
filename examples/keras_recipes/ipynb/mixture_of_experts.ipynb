{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMy7CTVDH93L"
   },
   "source": [
    "# Standalone Mixture-of-Experts (MoE) Layers\n",
    "\n",
    "**Author:** [Abhinav Kumar Singh](https://github.com/Abhinavexists)<br>\n",
    "**Date created:** 2025/12/10<br>\n",
    "**Last modified:** 2025/12/10<br>\n",
    "**Description:** Implementing standalone MoE layers as drop-in replacements for Dense and Conv2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhBDTIApH93M"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates the implementation of Mixture-of-Experts (MoE) layers\n",
    "as drop-in replacements for standard Keras `Dense` and `Conv2D` layers.\n",
    "\n",
    "MoE is a technique for scaling neural networks that increases model capacity while\n",
    "keeping computational costs manageable. Instead of processing all inputs through\n",
    "the same parameters, MoE layers use a gating mechanism to route each input to a\n",
    "weighted combination of specialized \"expert\" networks. This allows the model to\n",
    "learn specialized sub-networks for different types of inputs.\n",
    "\n",
    "The implementation is based on the approach described in\n",
    "[Outrageously Large Neural Networks](https://arxiv.org/abs/1701.06538)\n",
    "by Shazeer et al., and draws inspiration from the reference implementation by\n",
    "[Emin Orhan](https://github.com/eminorhan/mixture-of-experts).\n",
    "\n",
    "In this example, we implement two MoE layer types:\n",
    "\n",
    "- **DenseMoE**: A Mixture-of-Experts version of the Dense layer\n",
    "- **Conv2DMoE**: A Mixture-of-Experts version of the Conv2D layer\n",
    "\n",
    "We demonstrate their usage on the CIFAR-10 image classification task and\n",
    "compare performance against a baseline model using standard layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45YdF_E6H93M"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JsyaL741H93N"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, ops, activations, optimizers\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcC-n9VrH93N"
   },
   "source": [
    "## Implement the DenseMoE layer\n",
    "\n",
    "The DenseMoE layer computes: `output = sum_i gate_i(x) * expert_i(x)`\n",
    "\n",
    "where `gate_i(x)` is the learned gating weight for expert `i` and `expert_i(x)`\n",
    "is the output of expert `i`. Each input is processed by all experts, and their\n",
    "outputs are combined using learned gating weights. This approach uses soft routing,\n",
    "which is more efficient than top-k routing for smaller numbers of experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAIBDmAjH93N"
   },
   "outputs": [],
   "source": "\nclass DenseMoE(layers.Layer):\n    \"\"\"\n    Mixture-of-Experts Dense layer.\n\n    A drop-in replacement for keras.layers.Dense that uses multiple expert\n    networks combined via a learned gating mechanism.\n\n    Args:\n        units: Positive Integer, dimensionality of the output space\n        n_experts: Positive Integer, number of expert Dense layers\n        expert_activation: Activation function for expert model (for example, \"relu\" or \"tanh\")\n        gating_activation: Activation function for gating network (for example, \"softmax\")\n        use_expert_bias: Boolean, whether to use bias in expert layers\n        use_gating_bias: Boolean, whether to use bias in gating layer\n    \"\"\"\n\n    def __init__(\n        self,\n        units,\n        n_experts,\n        expert_activation=None,\n        gating_activation=None,\n        use_expert_bias=True,\n        use_gating_bias=True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.units = units\n        self.n_experts = n_experts\n        self.expert_activation = activations.get(expert_activation)\n        self.gating_activation = activations.get(gating_activation)\n        self.use_expert_bias = use_expert_bias\n        self.use_gating_bias = use_gating_bias\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n\n        self.expert_kernel = self.add_weight(\n            name=\"expert_kernel\",\n            shape=(input_dim, self.units, self.n_experts),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n        )\n\n        if self.use_expert_bias:\n            self.expert_bias = self.add_weight(\n                name=\"expert_bias\",\n                shape=(self.units, self.n_experts),\n                initializer=\"zeros\",\n                trainable=True,\n            )\n\n        self.gating_kernel = self.add_weight(\n            name=\"gating_kernel\",\n            shape=(input_dim, self.n_experts),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n        )\n\n        if self.use_gating_bias:\n            self.gating_bias = self.add_weight(\n                name=\"gating_bias\",\n                shape=(self.n_experts,),\n                initializer=\"zeros\",\n                trainable=True,\n            )\n\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # Compute expert outputs\n        expert_outputs = ops.tensordot(inputs, self.expert_kernel, axes=1)\n        if self.use_expert_bias:\n            expert_outputs = expert_outputs + self.expert_bias\n        expert_outputs = self.expert_activation(expert_outputs)\n\n        # Compute gating weights\n        gating_outputs = ops.tensordot(inputs, self.gating_kernel, axes=1)\n        if self.use_gating_bias:\n            gating_outputs = gating_outputs + self.gating_bias\n        gating_outputs = self.gating_activation(gating_outputs)\n\n        # Load balancing loss: encourages uniform expert utilization\n        if self.trainable:\n            importance = ops.mean(gating_outputs, axis=0)\n            load_loss = self.n_experts * ops.sum(ops.square(importance))\n            self.add_loss(1e-2 * load_loss)\n\n        # Weighted combination of expert outputs\n        gating_outputs = ops.expand_dims(gating_outputs, axis=1)\n        output = ops.sum(expert_outputs * gating_outputs, axis=-1)\n\n        return output\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"units\": self.units,\n                \"n_experts\": self.n_experts,\n                \"expert_activation\": activations.serialize(self.expert_activation),\n                \"gating_activation\": activations.serialize(self.gating_activation),\n                \"use_expert_bias\": self.use_expert_bias,\n                \"use_gating_bias\": self.use_gating_bias,\n            }\n        )\n        return config\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohFyw2crH93O"
   },
   "source": [
    "## Implement the Conv2DMoE layer\n",
    "\n",
    "The Conv2DMoE layer applies the MoE concept to convolutional operations.\n",
    "Similar to DenseMoE, it uses multiple expert Conv2D layers combined via learned\n",
    "gating weights. The gating network uses 1x1 convolutions for efficient\n",
    "per-position gating across spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaWUTle5H93O"
   },
   "outputs": [],
   "source": "\nclass Conv2DMoE(layers.Layer):\n    \"\"\"\n    Mixture-of-Experts Conv2D layer.\n\n    A drop-in replacement for keras.layers.Conv2D that uses multiple expert\n    convolutional models combined via a learned gating mechanism.\n\n    Args:\n        filters: Positive Integer, number of output filters\n        kernel_size: Size of the convolutional kernel\n        n_experts: Positive Integer, number of expert Conv2D layers\n        strides: Positive Integer or Tuple of 2 Integers, stride of the convolution\n        padding: String, padding mode ('valid' or 'same')\n        expert_activation: Activation function for expert model (for example, \"relu\" or \"tanh\")\n        gating_activation: Activation function for gating network (for example, \"softmax\")\n        use_expert_bias: Boolean, whether to use bias in expert layers\n        use_gating_bias: Boolean, whether to use bias in gating layer\n    \"\"\"\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        n_experts,\n        strides=1,\n        padding=\"valid\",\n        expert_activation=None,\n        gating_activation=None,\n        use_expert_bias=True,\n        use_gating_bias=True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = (\n            kernel_size\n            if isinstance(kernel_size, tuple)\n            else (kernel_size, kernel_size)\n        )\n        self.n_experts = n_experts\n        self.strides = strides if isinstance(strides, tuple) else (strides, strides)\n        self.padding = padding\n        self.expert_activation = activations.get(expert_activation)\n        self.gating_activation = activations.get(gating_activation)\n        self.use_expert_bias = use_expert_bias\n        self.use_gating_bias = use_gating_bias\n\n    def build(self, input_shape):\n        input_channels = input_shape[-1]\n\n        self.expert_kernel = self.add_weight(\n            name=\"expert_kernel\",\n            shape=(\n                self.kernel_size[0],\n                self.kernel_size[1],\n                input_channels,\n                self.filters,\n                self.n_experts,\n            ),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n        )\n\n        if self.use_expert_bias:\n            self.expert_bias = self.add_weight(\n                name=\"expert_bias\",\n                shape=(self.filters, self.n_experts),\n                initializer=\"zeros\",\n                trainable=True,\n            )\n\n        # Gating uses 1x1 convolution for efficient per-position gating\n        self.gating_kernel = self.add_weight(\n            name=\"gating_kernel\",\n            shape=(1, 1, input_channels, self.n_experts),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n        )\n\n        if self.use_gating_bias:\n            self.gating_bias = self.add_weight(\n                name=\"gating_bias\",\n                shape=(self.n_experts,),\n                initializer=\"zeros\",\n                trainable=True,\n            )\n\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # Compute expert outputs\n        expert_kernel = ops.reshape(\n            self.expert_kernel,\n            (\n                self.kernel_size[0],\n                self.kernel_size[1],\n                ops.shape(inputs)[-1],\n                self.filters * self.n_experts,\n            ),\n        )\n        expert_outputs = ops.conv(\n            inputs,\n            expert_kernel,\n            strides=self.strides,\n            padding=self.padding,\n        )\n        output_shape = ops.shape(expert_outputs)\n        expert_outputs = ops.reshape(\n            expert_outputs,\n            (\n                output_shape[0],\n                output_shape[1],\n                output_shape[2],\n                self.filters,\n                self.n_experts,\n            ),\n        )\n        if self.use_expert_bias:\n            expert_outputs = expert_outputs + self.expert_bias\n        expert_outputs = self.expert_activation(expert_outputs)\n\n        # Compute gating weights using 1x1 convolution\n        gating_outputs = ops.conv(\n            inputs,\n            self.gating_kernel,\n            strides=self.strides,\n            padding=self.padding,\n        )\n        if self.use_gating_bias:\n            gating_outputs = gating_outputs + self.gating_bias\n        gating_outputs = self.gating_activation(gating_outputs)\n\n        # Load balancing loss: encourages uniform expert utilization\n        if self.trainable:\n            importance = ops.mean(gating_outputs, axis=[0, 1, 2])\n            load_loss = self.n_experts * ops.sum(ops.square(importance))\n            self.add_loss(1e-2 * load_loss)\n\n        # Weighted combination of expert outputs\n        gating_outputs = ops.expand_dims(gating_outputs, axis=-2)\n        output = ops.sum(expert_outputs * gating_outputs, axis=-1)\n\n        return output\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"filters\": self.filters,\n                \"kernel_size\": self.kernel_size,\n                \"n_experts\": self.n_experts,\n                \"strides\": self.strides,\n                \"padding\": self.padding,\n                \"expert_activation\": activations.serialize(self.expert_activation),\n                \"gating_activation\": activations.serialize(self.gating_activation),\n                \"use_expert_bias\": self.use_expert_bias,\n                \"use_gating_bias\": self.use_gating_bias,\n            }\n        )\n        return config\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPPEobYfH93O"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "For this example, we use the CIFAR-10 dataset. To keep training time\n",
    "reasonable for demonstration purposes, we use only a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRqoAKVOH93P",
    "outputId": "4bdc7548-6362-4cef-dc09-8768975a1769"
   },
   "outputs": [],
   "source": "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nx_train = x_train.astype(\"float32\") / 255.0\nx_test = x_test.astype(\"float32\") / 255.0\n\nprint(f\"Training samples: {len(x_train)}\")\nprint(f\"Test samples: {len(x_test)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny0GH9lwH93P"
   },
   "source": [
    "## Build a baseline model\n",
    "\n",
    "First, we create a baseline CNN model using standard Keras layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NEn88DtH93P"
   },
   "outputs": [],
   "source": "\ndef create_baseline_model():\n    inputs = keras.Input(shape=(32, 32, 3))\n\n    x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    x = layers.Flatten()(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    outputs = layers.Dense(10, activation=\"softmax\")(x)\n\n    model = keras.Model(inputs, outputs, name=\"baseline_model\")\n    return model\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv8oD4DxH93P"
   },
   "source": [
    "## Build a model with MoE layers\n",
    "\n",
    "Now we create an equivalent model using MoE layers. We replace the Conv2D\n",
    "and Dense layers with their MoE counterparts: Conv2DMoE and DenseMoE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVJ5ymndH93P"
   },
   "outputs": [],
   "source": "\ndef create_moe_model():\n    inputs = keras.Input(shape=(32, 32, 3))\n\n    x = Conv2DMoE(\n        32,\n        3,\n        n_experts=4,\n        padding=\"same\",\n        expert_activation=\"relu\",\n        gating_activation=\"softmax\",\n    )(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    x = Conv2DMoE(\n        64,\n        3,\n        n_experts=4,\n        padding=\"same\",\n        expert_activation=\"relu\",\n        gating_activation=\"softmax\",\n    )(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    x = layers.Flatten()(x)\n    x = DenseMoE(\n        128,\n        n_experts=8,\n        expert_activation=\"relu\",\n        gating_activation=\"softmax\",\n    )(x)\n    x = layers.Dropout(0.5)(x)\n    x = DenseMoE(\n        64,\n        n_experts=4,\n        expert_activation=\"relu\",\n        gating_activation=\"softmax\",\n    )(x)\n    outputs = layers.Dense(10, activation=\"softmax\")(x)\n\n    model = keras.Model(inputs, outputs, name=\"moe_model\")\n    return model\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StjKmCVaH93P"
   },
   "source": [
    "## Train and compare models\n",
    "\n",
    "We train both models and compare their performance. The MoE model has more\n",
    "total parameters due to the multiple expert networks, but can potentially\n",
    "learn more diverse representations through expert specialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N5X1QMICH93P",
    "outputId": "2f956427-2aef-4dc8-fc7a-899f7fcbcb1f"
   },
   "outputs": [],
   "source": "epochs = 10\nbatch_size = 128\n\nprint(\"BASELINE MODEL\")\nbaseline_model = create_baseline_model()\nbaseline_model.compile(\n    optimizer=optimizers.Adam(learning_rate=0.001),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nbaseline_model.summary()\n\nprint(\"\\nTraining baseline model...\")\nbaseline_history = baseline_model.fit(\n    x_train,\n    y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.2,\n    verbose=1,\n)\n\nbaseline_test_loss, baseline_test_acc = baseline_model.evaluate(\n    x_test, y_test, verbose=0\n)\nprint(f\"\\nBaseline Test Accuracy: {baseline_test_acc:.4f}\")\n\nprint(\"\\nMIXTURE-OF-EXPERTS MODEL\")\nmoe_model = create_moe_model()\nmoe_model.compile(\n    optimizer=optimizers.Adam(learning_rate=0.001),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmoe_model.summary()\n\nprint(\"\\nTraining MoE model...\")\nmoe_history = moe_model.fit(\n    x_train,\n    y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.2,\n    verbose=1,\n)\n\nmoe_test_loss, moe_test_acc = moe_model.evaluate(x_test, y_test, verbose=0)\nprint(f\"\\nMoE Test Accuracy: {moe_test_acc:.4f}\")\n\nprint(\"\\nMODEL COMPARISON\")\nprint(\n    f\"Baseline - Params: {baseline_model.count_params():,} | \"\n    f\"Test Accuracy: {baseline_test_acc:.4f}\"\n)\nprint(\n    f\"MoE Model - Params: {moe_model.count_params():,} | \"\n    f\"Test Accuracy: {moe_test_acc:.4f}\"\n)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize training history\n",
    "\n",
    "Let's compare the training curves of both models to understand their learning dynamics."
   ],
   "metadata": {
    "id": "vWAul95SM5Md"
   }
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(baseline_history.history[\"accuracy\"], label=\"Baseline Train\", linestyle=\"--\")\nplt.plot(\n    baseline_history.history[\"val_accuracy\"], label=\"Baseline Val\", linestyle=\"--\"\n)\nplt.plot(moe_history.history[\"accuracy\"], label=\"MoE Train\")\nplt.plot(moe_history.history[\"val_accuracy\"], label=\"MoE Val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Model Accuracy Comparison\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(baseline_history.history[\"loss\"], label=\"Baseline Train\", linestyle=\"--\")\nplt.plot(baseline_history.history[\"val_loss\"], label=\"Baseline Val\", linestyle=\"--\")\nplt.plot(moe_history.history[\"loss\"], label=\"MoE Train\")\nplt.plot(moe_history.history[\"val_loss\"], label=\"MoE Val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Comparison\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFHL229MH93Q"
   },
   "source": [
    "## Analyzing expert utilization\n",
    "\n",
    "One key aspect of MoE models is how the gating network learns to distribute\n",
    "inputs across experts. Let's examine the gating weights to understand expert\n",
    "utilization patterns."
   ]
  },
  {
   "cell_type": "code",
   "source": "dense_moe_layer = None\nfor layer in moe_model.layers:\n    if isinstance(layer, DenseMoE):\n        dense_moe_layer = layer\n        break\n\nif dense_moe_layer:\n    feature_extractor = keras.Model(\n        inputs=moe_model.input,\n        outputs=dense_moe_layer.input,\n    )\n\n    test_features = feature_extractor.predict(x_test[:100], verbose=0)\n\n    gating_logits = ops.tensordot(test_features, dense_moe_layer.gating_kernel, axes=1)\n    if dense_moe_layer.use_gating_bias:\n        gating_logits = gating_logits + dense_moe_layer.gating_bias\n    gating_weights = dense_moe_layer.gating_activation(gating_logits)\n\n    gating_weights_np = ops.convert_to_numpy(gating_weights)\n\n    print(\"\\nGating weights distribution across experts:\")\n    print(f\"Mean gating weights per expert: {gating_weights_np.mean(axis=0)}\")\n    print(f\"Std of gating weights per expert: {gating_weights_np.std(axis=0)}\")\n    print(\"\\nA more uniform distribution indicates all experts are being utilized.\")\n    print(\"Highly skewed distributions suggest some experts dominate.\")\n\n    mean_weights = gating_weights_np.mean(axis=0)\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.bar(range(len(mean_weights)), mean_weights)\n    plt.xlabel(\"Expert Index\")\n    plt.ylabel(\"Average Gating Weight\")\n    plt.title(\"Average Expert Utilization\")\n    plt.grid(True, alpha=0.3, axis=\"y\")\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(gating_weights_np[:50].T, aspect=\"auto\", cmap=\"viridis\")\n    plt.xlabel(\"Sample Index\")\n    plt.ylabel(\"Expert Index\")\n    plt.title(\"Gating Weights Heatmap (50 samples)\")\n    plt.colorbar(label=\"Gating Weight\")\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "Shqm0iZiNGNE",
    "outputId": "ef30eb15-5cb5-491d-c9ec-439586c64fe2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key takeaways\n\nThis example demonstrated how to implement Mixture-of-Experts layers as drop-in\nreplacements for standard Keras layers. Here are the main points:\n\n**MoE characteristics:**\n\n- **Increased capacity**: MoE models have more parameters due to multiple expert\n  networks, which can capture more diverse patterns in the data.\n- **Learned routing**: The gating network learns to dynamically route inputs to\n  appropriate experts based on the input features.\n- **Soft routing**: This implementation uses soft routing (weighted combination\n  of all experts) rather than hard top-k selection, providing smooth gradients\n  during training.\n\n**When to use MoE layers:**\n\n- You need high model capacity but want conditional computation\n- Your dataset has diverse patterns that could benefit from specialized sub-networks\n- You have sufficient training data to properly train multiple experts\n- You're working on large-scale tasks where model capacity is important\n\n**Practical tips:**\n\n- Start with 4-8 experts and increase if needed\n- Monitor gating weights to ensure all experts are being utilized\n- MoE layers work best in deeper parts of the network where features are more abstract\n- For very large numbers of experts (100+), consider top-k routing for efficiency\n  (see the [Switch Transformer example](https://keras.io/examples/nlp/text_classification_with_switch_transformer/))\n\n**Implementation notes:**\n\nThis implementation uses soft routing, which computes a weighted combination of\nall expert outputs. This approach provides smooth gradients and is well-suited\nfor moderate numbers of experts (4-8).\n\nWe also include a **load balancing loss** that encourages uniform expert utilization.\nWithout this loss, the model may learn to rely heavily on just one or two dominant\nexperts, wasting the capacity of the others. The load balancing loss penalizes\nimbalanced gating distributions, helping ensure all experts contribute to the model.\n\nFor very large expert counts (100+), consider top-k routing for efficiency\n(see the [Switch Transformer example](https://keras.io/examples/nlp/text_classification_with_switch_transformer/)),\nwhere load balancing becomes even more critical.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuIwUshyH93Q"
   },
   "source": "## Conclusion\n\nThis example showed how to implement Mixture-of-Experts layers as drop-in\nreplacements for standard Keras Dense and Conv2D layers. MoE enables scaling\nmodel capacity through expert specialization while maintaining computational\nefficiency through learned routing.\n\nYou can use `DenseMoE` and `Conv2DMoE` in your own models wherever you would\nnormally use `Dense` or `Conv2D` layers. Try experimenting with different numbers\nof experts and observe how the gating network learns to route inputs for your\nspecific task and dataset."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}