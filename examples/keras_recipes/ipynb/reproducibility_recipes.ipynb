{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Reproducibility in Keras Models\n",
    "\n",
    "**Author:** [Frightera](https://github.com/Frightera)<br>\n",
    "**Date created:** 2023/05/05<br>\n",
    "**Last modified:** 2023/05/05<br>\n",
    "**Description:** Demonstration of random weight initialization and reproducibility in Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates how to control randomness in Keras models. Sometimes\n",
    "you may want to reproduce the exact same results across runs, for experimentation\n",
    "purposes or to debug a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import initializers\n",
    "\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) backend random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "# If using TensorFlow, this will make GPU ops as deterministic as possible,\n",
    "# but it will affect the overall performance, so be mindful of that.\n",
    "tf.config.experimental.enable_op_determinism()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Weight initialization in Keras\n",
    "\n",
    "Most of the layers in Keras have `kernel_initializer` and `bias_initializer`\n",
    "parameters. These parameters allow you to specify the strategy used for\n",
    "initializing the weights of layer variables. The following built-in initializers\n",
    "are available as part of `keras.initializers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "initializers_list = [\n",
    "    initializers.RandomNormal,\n",
    "    initializers.RandomUniform,\n",
    "    initializers.TruncatedNormal,\n",
    "    initializers.VarianceScaling,\n",
    "    initializers.GlorotNormal,\n",
    "    initializers.GlorotUniform,\n",
    "    initializers.HeNormal,\n",
    "    initializers.HeUniform,\n",
    "    initializers.LecunNormal,\n",
    "    initializers.LecunUniform,\n",
    "    initializers.Orthogonal,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In a reproducible model, the weights of the model should be initialized with\n",
    "same values in subsequent runs. First, we'll check how initializers behave when\n",
    "they are called multiple times with same `seed` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for initializer in initializers_list:\n",
    "    print(f\"Running {initializer}\")\n",
    "\n",
    "    for iteration in range(2):\n",
    "        # In order to get same results across multiple runs from an initializer,\n",
    "        # you can specify a seed value.\n",
    "        result = float(initializer(seed=42)(shape=(1, 1)))\n",
    "        print(f\"\\tIteration --> {iteration} // Result --> {result}\")\n",
    "    print(\"\\n\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's inspect how two different initializer objects behave when they are\n",
    "have the same seed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Setting the seed value for an initializer will cause two different objects\n",
    "# to produce same results.\n",
    "glorot_normal_1 = keras.initializers.GlorotNormal(seed=42)\n",
    "glorot_normal_2 = keras.initializers.GlorotNormal(seed=42)\n",
    "\n",
    "input_dim, neurons = 3, 5\n",
    "\n",
    "# Call two different objects with same shape\n",
    "result_1 = glorot_normal_1(shape=(input_dim, neurons))\n",
    "result_2 = glorot_normal_2(shape=(input_dim, neurons))\n",
    "\n",
    "# Check if the results are equal.\n",
    "equal = np.allclose(result_1, result_2)\n",
    "print(f\"Are the results equal? {equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "If the seed value is not set (or different seed values are used), two different\n",
    "objects will produce different results. Since the random seed is set at the beginning\n",
    "of the notebook, the results will be same in the sequential runs. This is related\n",
    "to the `keras.utils.set_random_seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "glorot_normal_3 = keras.initializers.GlorotNormal()\n",
    "glorot_normal_4 = keras.initializers.GlorotNormal()\n",
    "\n",
    "# Let's call the initializer.\n",
    "result_3 = glorot_normal_3(shape=(input_dim, neurons))\n",
    "\n",
    "# Call the second initializer.\n",
    "result_4 = glorot_normal_4(shape=(input_dim, neurons))\n",
    "\n",
    "equal = np.allclose(result_3, result_4)\n",
    "print(f\"Are the results equal? {equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`result_3` and `result_4` will be different, but when you run the notebook\n",
    "again, `result_3` will have identical values to the ones in the previous run.\n",
    "Same goes for `result_4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Reproducibility in model training process\n",
    "If you want to reproduce the results of a model training process, you need to\n",
    "control the randomness sources during the training process. In order to show a\n",
    "realistic example, this section utilizes `tf.data` using parallel map and shuffle\n",
    "operations.\n",
    "\n",
    "In order to start, let's create a simple function which returns the history\n",
    "object of the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_data: tf.data.Dataset, test_data: tf.data.Dataset) -> dict:\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "        jit_compile=False,\n",
    "    )\n",
    "    # jit_compile's default value is \"auto\" which will cause some problems in some\n",
    "    # ops, therefore it's set to False.\n",
    "\n",
    "    # model.fit has a `shuffle` parameter which has a default value of `True`.\n",
    "    # If you are using array-like objects, this will shuffle the data before\n",
    "    # training. This argument is ignored when `x` is a generator or\n",
    "    # `tf.data.Dataset`.\n",
    "    history = model.fit(train_data, epochs=2, validation_data=test_data)\n",
    "\n",
    "    print(f\"Model accuracy on test data: {model.evaluate(test_data)[1] * 100:.2f}%\")\n",
    "\n",
    "    return history.history\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (\n",
    "    test_images,\n",
    "    test_labels,\n",
    ") = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Construct tf.data.Dataset objects\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Remember we called `tf.config.experimental.enable_op_determinism()` at the\n",
    "beginning of the function. This makes the `tf.data` operations deterministic.\n",
    "However, making `tf.data` operations deterministic comes with a performance\n",
    "cost. If you want to learn more about it, please check this\n",
    "[official guide](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism#determinism_and_tfdata).\n",
    "\n",
    "Small summary what's going on here. Models have `kernel_initializer` and\n",
    "`bias_initializer` parameters. Since we set random seeds using\n",
    "`keras.utils.set_random_seed` in the beginning of the notebook, the initializers\n",
    "will produce same results in the sequential runs. Additionally, TensorFlow\n",
    "operations have now become deterministic. Frequently, you will be utilizing GPUs\n",
    "that have thousands of hardware threads which causes non-deterministic behavior\n",
    "to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(image, label):\n",
    "    # Cast and normalize the image\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Expand the channel dimension\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, (32, 32))\n",
    "\n",
    "    return image, label\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`tf.data.Dataset` objects have a `shuffle` method which shuffles the data.\n",
    "This method has a `buffer_size` parameter which controls the size of the\n",
    "buffer. If you set this value to `len(train_images)`, the whole dataset will\n",
    "be shuffled. If the buffer size is equal to the length of the dataset,\n",
    "then the elements will be shuffled in a completely random order.\n",
    "\n",
    "Main drawback of setting the buffer size to the length of the dataset is that\n",
    "filling the buffer can take a while depending on the size of the dataset.\n",
    "\n",
    "Here is a small summary of what's going on here:\n",
    "1) The `shuffle()` method creates a buffer of the specified size.\n",
    "2) The elements of the dataset are randomly shuffled and placed into the buffer.\n",
    "3) The elements of the buffer are then returned in a random order.\n",
    "\n",
    "Since `tf.config.experimental.enable_op_determinism()` is enabled and we set\n",
    "random seeds using `keras.utils.set_random_seed` in the beginning of the\n",
    "notebook, the `shuffle()` method will produce same results in the sequential\n",
    "runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Prepare the datasets, batch-map --> vectorized operations\n",
    "train_data = (\n",
    "    train_ds.shuffle(buffer_size=len(train_images))\n",
    "    .batch(batch_size=64)\n",
    "    .map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    test_ds.batch(batch_size=64)\n",
    "    .map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Train the model for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = train_model(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's save our results into a JSON file, and restart the kernel. After\n",
    "restarting the kernel, we should see the same results as the previous run,\n",
    "this includes metrics and loss values both on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Save the history object into a json file\n",
    "with open(\"history.json\", \"w\") as fp:\n",
    "    json.dump(history, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Do not run the cell above in order not to overwrite the results. Execute the\n",
    "model training cell again and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "with open(\"history.json\", \"r\") as fp:\n",
    "    history_loaded = json.load(fp)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Compare the results one by one. You will see that they are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for key in history.keys():\n",
    "    for i in range(len(history[key])):\n",
    "        if not np.allclose(history[key][i], history_loaded[key][i]):\n",
    "            print(f\"{key} not equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you learned how to control the randomness sources in Keras and\n",
    "TensorFlow. You also learned how to reproduce the results of a model training\n",
    "process.\n",
    "\n",
    "If you want to initialize the model with the same weights everytime, you need to\n",
    "set `kernel_initializer` and `bias_initializer` parameters of the layers and provide\n",
    "a `seed` value to the initializer.\n",
    "\n",
    "There still may be some inconsistencies due to numerical error accumulation such\n",
    "as using `recurrent_dropout` in RNN layers.\n",
    "\n",
    "Reproducibility is subject to the environment. You'll get the same results if you\n",
    "run the notebook or the code on the same machine with the same environment."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "reproducibility_recipes",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}