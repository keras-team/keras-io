{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# DistributedEmbedding using TPU SparseCore and TensorFlow\n",
    "\n",
    "**Author:** [Fabien Hertschuh](https://github.com/hertschuh/), [Abheesht Sharma](https://github.com/abheesht17/)<br>\n",
    "**Date created:** 2025/09/02<br>\n",
    "**Last modified:** 2025/09/02<br>\n",
    "**Description:** Rank movies using a two tower model with embeddings on SparseCore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In the [basic ranking](/keras_rs/examples/basic_ranking/) tutorial, we showed\n",
    "how to build a ranking model for the MovieLens dataset to suggest movies to\n",
    "users.\n",
    "\n",
    "This tutorial implements the same model trained on the same dataset but with the\n",
    "use of `keras_rs.layers.DistributedEmbedding`, which makes use of SparseCore on\n",
    "TPU. This is the TensorFlow version of the tutorial. It needs to be run on TPU\n",
    "v5p or v6e.\n",
    "\n",
    "Let's begin by installing the necessary libraries. Note that we need\n",
    "`tensorflow-tpu` version 2.19. We'll also install `keras-rs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q tensorflow-tpu==2.19.1\n",
    "!pip install -q keras-rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We're using the PJRT version of the runtime for TensorFlow. We're also enabling\n",
    "the MLIR bridge. This requires setting a few flags before importing TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import libtpu\n",
    "\n",
    "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
    "os.environ[\"NEXT_PLUGGABLE_DEVICE_USE_C_API\"] = \"true\"\n",
    "os.environ[\"TF_PLUGGABLE_DEVICE_LIBRARY_PATH\"] = libtpu.get_library_path()\n",
    "os.environ[\"TF_XLA_FLAGS\"] = (\n",
    "    \"--tf_mlir_enable_mlir_bridge=true \"\n",
    "    \"--tf_mlir_enable_convert_control_to_data_outputs_pass=true \"\n",
    "    \"--tf_mlir_enable_merge_control_flow_pass=true\"\n",
    ")\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We now set the Keras backend to TensorFlow and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import keras_rs\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Creating a `TPUStrategy`\n",
    "\n",
    "To run TensorFlow on TPU, you need to use a\n",
    "[`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)\n",
    "to handle the distribution of the model.\n",
    "\n",
    "The core of the model is replicated across TPU instances, which is done by the\n",
    "`TPUStrategy`. Note that on GPU you would use\n",
    "[`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy)\n",
    "instead, but this strategy is not for TPU.\n",
    "\n",
    "Only the embedding tables handled by `DistributedEmbedding` are sharded across\n",
    "the SparseCore chips of all the available TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "topology = tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "tpu_metadata = resolver.get_tpu_system_metadata()\n",
    "\n",
    "device_assignment = tf.tpu.experimental.DeviceAssignment.build(\n",
    "    topology, num_replicas=tpu_metadata.num_cores\n",
    ")\n",
    "strategy = tf.distribute.TPUStrategy(\n",
    "    resolver, experimental_device_assignment=device_assignment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset distribution\n",
    "\n",
    "While the model is replicated and the embedding tables are sharded across\n",
    "SparseCores, the dataset is distributed by sharding each batch across the TPUs.\n",
    "We need to make sure the batch size is a multiple of the number of TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "PER_REPLICA_BATCH_SIZE = 256\n",
    "BATCH_SIZE = PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "We're going to use the same MovieLens data. The ratings are the objectives we\n",
    "are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We need to know the number of users as we're using the user ID directly as an\n",
    "index in the user embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "users_count = int(\n",
    "    ratings.map(lambda x: tf.strings.to_number(x[\"user_id\"], out_type=tf.int32))\n",
    "    .reduce(tf.constant(0, tf.int32), tf.maximum)\n",
    "    .numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We also need do know the number of movies as we're using the movie ID directly\n",
    "as an index in the movie embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "movies_count = int(movies.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The inputs to the model are the user IDs and movie IDs and the labels are the\n",
    "ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_rating(x):\n",
    "    return (\n",
    "        # Inputs are user IDs and movie IDs\n",
    "        {\n",
    "            \"user_id\": tf.strings.to_number(x[\"user_id\"], out_type=tf.int32),\n",
    "            \"movie_id\": tf.strings.to_number(x[\"movie_id\"], out_type=tf.int32),\n",
    "        },\n",
    "        # Labels are ratings between 0 and 1.\n",
    "        (x[\"user_rating\"] - 1.0) / 4.0,\n",
    "    )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We'll split the data by putting 80% of the ratings in the train set, and 20% in\n",
    "the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "shuffled_ratings = ratings.map(preprocess_rating).shuffle(\n",
    "    100_000, seed=42, reshuffle_each_iteration=False\n",
    ")\n",
    "train_ratings = (\n",
    "    shuffled_ratings.take(80_000).batch(BATCH_SIZE, drop_remainder=True).cache()\n",
    ")\n",
    "test_ratings = (\n",
    "    shuffled_ratings.skip(80_000)\n",
    "    .take(20_000)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configuring DistributedEmbedding\n",
    "\n",
    "The `keras_rs.layers.DistributedEmbedding` handles multiple features and\n",
    "multiple embedding tables. This is to enable the sharing of tables between\n",
    "features and allow some optimizations that come from combining multiple\n",
    "embedding lookups into a single invocation. In this section, we'll describe\n",
    "how to configure these.\n",
    "\n",
    "### Configuring tables\n",
    "\n",
    "Tables are configured using `keras_rs.layers.TableConfig`, which has:\n",
    "\n",
    "- A name.\n",
    "- A vocabulary size (input size).\n",
    "- an embedding dimension (output size).\n",
    "- A combiner to specify how to reduce multiple embeddings into a single one in\n",
    "  the case when we embed a sequence. Note that this doesn't apply to our example\n",
    "  because we're getting a single embedding for each user and each movie.\n",
    "- A placement to tell whether to put the table on the SparseCore chips or not.\n",
    "  In this case, we want the `\"sparsecore\"` placement.\n",
    "- An optimizer to specify how to apply gradients when training. Each table has\n",
    "  its own optimizer and the one passed to `model.compile()` is not used for the\n",
    "  embedding tables.\n",
    "\n",
    "### Configuring features\n",
    "\n",
    "Features are configured using `keras_rs.layers.FeatureConfig`, which has:\n",
    "\n",
    "- A name.\n",
    "- A table, the embedding table to use.\n",
    "- An input shape (batch size is for all TPUs).\n",
    "- An output shape (batch size is for all TPUs).\n",
    "\n",
    "We can organize features in any structure we want, which can be nested. A dict\n",
    "is often a good choice to have names for the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 32\n",
    "\n",
    "movie_table = keras_rs.layers.TableConfig(\n",
    "    name=\"movie_table\",\n",
    "    vocabulary_size=movies_count + 1,  # +1 for movie ID 0, which is not used\n",
    "    embedding_dim=EMBEDDING_DIMENSION,\n",
    "    optimizer=\"adam\",\n",
    "    placement=\"sparsecore\",\n",
    ")\n",
    "user_table = keras_rs.layers.TableConfig(\n",
    "    name=\"user_table\",\n",
    "    vocabulary_size=users_count + 1,  # +1 for user ID 0, which is not used\n",
    "    embedding_dim=EMBEDDING_DIMENSION,\n",
    "    optimizer=\"adam\",\n",
    "    placement=\"sparsecore\",\n",
    ")\n",
    "\n",
    "FEATURE_CONFIGS = {\n",
    "    \"movie_id\": keras_rs.layers.FeatureConfig(\n",
    "        name=\"movie\",\n",
    "        table=movie_table,\n",
    "        input_shape=(BATCH_SIZE,),\n",
    "        output_shape=(BATCH_SIZE, EMBEDDING_DIMENSION),\n",
    "    ),\n",
    "    \"user_id\": keras_rs.layers.FeatureConfig(\n",
    "        name=\"user\",\n",
    "        table=user_table,\n",
    "        input_shape=(BATCH_SIZE,),\n",
    "        output_shape=(BATCH_SIZE, EMBEDDING_DIMENSION),\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "We're now ready to create a `DistributedEmbedding` inside a model. Once we have\n",
    "the configuration, we simply pass it the constructor of `DistributedEmbedding`.\n",
    "Then, within the model `call` method, `DistributedEmbedding` is the first layer\n",
    "we call.\n",
    "\n",
    "The ouputs have the exact same structure as the inputs. In our example, we\n",
    "concatenate the embeddings we got as outputs and run them through a tower of\n",
    "dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingModel(keras.Model):\n",
    "    \"\"\"Create the model with the embedding configuration.\n",
    "\n",
    "    Args:\n",
    "        feature_configs: the configuration for `DistributedEmbedding`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_configs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = keras_rs.layers.DistributedEmbedding(\n",
    "            feature_configs=feature_configs\n",
    "        )\n",
    "        self.ratings = keras.Sequential(\n",
    "            [\n",
    "                # Learn multiple dense layers.\n",
    "                keras.layers.Dense(256, activation=\"relu\"),\n",
    "                keras.layers.Dense(64, activation=\"relu\"),\n",
    "                # Make rating predictions in the final layer.\n",
    "                keras.layers.Dense(1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        # Embedding lookup. Outputs have the same structure as the inputs.\n",
    "        embedding = self.embedding_layer(features)\n",
    "        return self.ratings(\n",
    "            keras.ops.concatenate(\n",
    "                [embedding[\"user_id\"], embedding[\"movie_id\"]],\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's now instantiate the model. We then use `model.compile()` to configure the\n",
    "loss, metrics and optimizer. Again, this Adagrad optimizer will only apply to\n",
    "the dense layers and not the embedding tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = EmbeddingModel(FEATURE_CONFIGS)\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "        optimizer=\"adagrad\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "We can use the standard Keras `model.fit()` to train the model. Keras will\n",
    "automatically use the `TPUStrategy` to distribute the model and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model.fit(train_ratings, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Same for `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model.evaluate(test_ratings, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That's it.\n",
    "\n",
    "This example shows that after setting up the `TPUStrategy` and configuring the\n",
    "`DistributedEmbedding`, you can use the standard Keras workflows."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "distributed_embedding_tf",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}