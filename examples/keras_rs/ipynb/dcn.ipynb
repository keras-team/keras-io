{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Ranking with Deep and Cross Networks\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Fabien Hertschuh](https://github.com/hertschuh/)<br>\n",
    "**Date created:** 2025/04/28<br>\n",
    "**Last modified:** 2025/04/28<br>\n",
    "**Description:** Rank movies using Deep and Cross Networks (DCN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use Deep & Cross Networks (DCN) to effectively\n",
    "learn feature crosses. Before diving into the example, let's briefly discuss\n",
    "feature crosses.\n",
    "\n",
    "Imagine that we are building a recommender system for blenders. Individual\n",
    "features might include a customer's past purchase history (e.g.,\n",
    "`purchased_bananas`, `purchased_cooking_books`) or geographic location. However,\n",
    "a customer who has purchased both bananas and cooking books is more likely to be\n",
    "interested in a blender than someone who purchased only one or the other. The\n",
    "combination of `purchased_bananas` and `purchased_cooking_books` is a feature\n",
    "cross. Feature crosses capture interaction information between individual\n",
    "features, providing richer context than the individual features alone.\n",
    "\n",
    "![Why are feature crosses important?](https://i.imgur.com/qDK6UZh.gif)\n",
    "\n",
    "Learning effective feature crosses presents several challenges. In web-scale\n",
    "applications, data is often categorical, resulting in high-dimensional and\n",
    "sparse feature spaces.  Identifying impactful feature crosses in such\n",
    "environments typically relies on manual feature engineering or computationally\n",
    "expensive exhaustive searches. While traditional feed-forward multilayer\n",
    "perceptrons (MLPs) are universal function approximators, they often struggle to\n",
    "efficiently learn even second- or third-order feature interactions.\n",
    "\n",
    "The Deep & Cross Network (DCN) architecture is designed for more effective\n",
    "learning of explicit and bounded-degree feature crosses. It comprises three main\n",
    "components: an input layer (typically an embedding layer), a cross network for\n",
    "modeling explicit feature interactions, and a deep network for capturing\n",
    "implicit interactions.\n",
    "\n",
    "The cross network is the core of the DCN. It explicitly performs feature\n",
    "crossing at each layer, with the highest polynomial degree of feature\n",
    "interaction increasing with depth. The following figure shows the `(i+1)`-th\n",
    "cross layer.\n",
    "\n",
    "![Feature Cross Layer](https://i.imgur.com/ip5uRsl.png)\n",
    "\n",
    "The deep network is a standard feedforward multilayer perceptron\n",
    "(MLP). These two networks are then combined to form the DCN.  Two common\n",
    "combination strategies exist: a stacked structure, where the deep network is\n",
    "placed on top of the cross network, and a parallel structure, where they\n",
    "operate in parallel.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <figure>\n",
    "        <img src=\"https://i.imgur.com/rNn0zxS.png\" alt=\"Parallel layers\" width=\"1000\" height=\"500\">\n",
    "        <figcaption>Parallel layers</figcaption>\n",
    "      </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "      <figure>\n",
    "        <img src=\"https://i.imgur.com/g32nzCl.png\" alt=\"Stacked layers\" width=\"1000\" height=\"500\">\n",
    "        <figcaption>Stacked layers</figcaption>\n",
    "      </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Now that we know a little bit about DCN, let's start writing some code. We will\n",
    "first train a DCN on a toy dataset, and demonstrate that the model has indeed\n",
    "learnt important feature crosses.\n",
    "\n",
    "Let's set the backend to JAX, and get our imports sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # `\"tensorflow\"`/`\"torch\"`\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import keras_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's also define variables which will be reused throughout the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "TOY_CONFIG = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_epochs\": 100,\n",
    "    \"batch_size\": 1024,\n",
    "}\n",
    "\n",
    "MOVIELENS_CONFIG = {\n",
    "    # features\n",
    "    \"int_features\": [\n",
    "        \"movie_id\",\n",
    "        \"user_id\",\n",
    "        \"user_gender\",\n",
    "        \"bucketized_user_age\",\n",
    "    ],\n",
    "    \"str_features\": [\n",
    "        \"user_zip_code\",\n",
    "        \"user_occupation_text\",\n",
    "    ],\n",
    "    # model\n",
    "    \"embedding_dim\": 32,\n",
    "    \"deep_net_num_units\": [192, 192, 192],\n",
    "    \"projection_dim\": 20,\n",
    "    \"dcn_num_units\": [192, 192],\n",
    "    # training\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 1024,\n",
    "}\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Here, we define a helper function for visualising weights of the cross layer in\n",
    "order to better understand its functioning. Also, we define a function for\n",
    "compiling, training and evaluating a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_layer(matrix, features):\n",
    "    plt.figure(figsize=(9, 9))\n",
    "\n",
    "    im = plt.matshow(np.abs(matrix), cmap=plt.cm.Blues)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    divider = make_axes_locatable(plt.gca())\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    cax.tick_params(labelsize=10)\n",
    "    ax.set_xticklabels([\"\"] + features, rotation=45, fontsize=10)\n",
    "    ax.set_yticklabels([\"\"] + features, fontsize=10)\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    learning_rate,\n",
    "    epochs,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    model,\n",
    "):\n",
    "    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    loss = keras.losses.MeanSquaredError()\n",
    "    rmse = keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[rmse],\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_data,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    results = model.evaluate(test_data, return_dict=True, verbose=0)\n",
    "    rmse_value = results[\"root_mean_squared_error\"]\n",
    "\n",
    "    return rmse_value, model.count_params()\n",
    "\n",
    "\n",
    "def print_stats(rmse_list, num_params, model_name):\n",
    "    # Report metrics.\n",
    "    num_trials = len(rmse_list)\n",
    "    avg_rmse = np.mean(rmse_list)\n",
    "    std_rmse = np.std(rmse_list)\n",
    "\n",
    "    if num_trials == 1:\n",
    "        print(f\"{model_name}: RMSE = {avg_rmse}; #params = {num_params}\")\n",
    "    else:\n",
    "        print(f\"{model_name}: RMSE = {avg_rmse} \u00b1 {std_rmse}; #params = {num_params}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Toy Example\n",
    "\n",
    "To illustrate the benefits of DCNs, let's consider a simple example. Suppose we\n",
    "have a dataset for modeling the likelihood of a customer clicking on a blender\n",
    "advertisement. The features and label are defined as follows:\n",
    "\n",
    "| **Features / Label** | **Description**                | **Range**|\n",
    "|:--------------------:|:------------------------------:|:--------:|\n",
    "| `x1` = country       | Customer's resident country    | [0, 199] |\n",
    "| `x2` = bananas       | # bananas purchased            | [0, 23]  |\n",
    "| `x3` = cookbooks     | # cooking books purchased      | [0, 5]   |\n",
    "| `y`                  | Blender ad click likelihood    | -        |\n",
    "\n",
    "Then, we let the data follow the following underlying distribution:\n",
    "`y = f(x1, x2, x3) = 0.1x1 + 0.4x2 + 0.7x3 + 0.1x1x2 +`\n",
    "`3.1x2x3 + 0.1x3^2`.\n",
    "\n",
    "This distribution shows that the click likelihood (`y`) depends linearly on\n",
    "individual features (`xi`) and on multiplicative interactions between them. In\n",
    "this scenario, the likelihood of purchasing a blender (`y`) is influenced not\n",
    "only by purchasing bananas (`x2`) or cookbooks (`x3`) individually, but also\n",
    "significantly by the interaction of purchasing both bananas and cookbooks\n",
    "(`x2x3`).\n",
    "\n",
    "### Preparing the dataset\n",
    "\n",
    "Let's create synthetic data based on the above equation, and form the train-test\n",
    "splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_mixer_data(data_size=100_000):\n",
    "    country = np.random.randint(200, size=[data_size, 1]) / 200.0\n",
    "    bananas = np.random.randint(24, size=[data_size, 1]) / 24.0\n",
    "    cookbooks = np.random.randint(6, size=[data_size, 1]) / 6.0\n",
    "\n",
    "    x = np.concatenate([country, bananas, cookbooks], axis=1)\n",
    "\n",
    "    # Create 1st-order terms.\n",
    "    y = 0.1 * country + 0.4 * bananas + 0.7 * cookbooks\n",
    "\n",
    "    # Create 2nd-order cross terms.\n",
    "    y += (\n",
    "        0.1 * country * bananas\n",
    "        + 3.1 * bananas * cookbooks\n",
    "        + (0.1 * cookbooks * cookbooks)\n",
    "    )\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = get_mixer_data(data_size=100_000)\n",
    "num_train = 90_000\n",
    "train_x = x[:num_train]\n",
    "train_y = y[:num_train]\n",
    "test_x = x[num_train:]\n",
    "test_y = y[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building the model\n",
    "\n",
    "To demonstrate the advantages of a cross network in recommender systems, we'll\n",
    "compare its performance with a deep network. Since our example data only\n",
    "contains second-order feature interactions, a single-layered cross network will\n",
    "suffice. For datasets with higher-order interactions, multiple cross layers can\n",
    "be stacked to form a multi-layered cross network. We will build two models:\n",
    "\n",
    "1. A cross network with a single cross layer.\n",
    "2. A deep network with wider and deeper feedforward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "cross_network = keras.Sequential(\n",
    "    [\n",
    "        keras_rs.layers.FeatureCross(),\n",
    "        keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "deep_network = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dense(1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Model training\n",
    "\n",
    "Before we train the model, we need to batch our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(\n",
    "    TOY_CONFIG[\"batch_size\"]\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(\n",
    "    TOY_CONFIG[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's train both models. Remember we have set `verbose=0` for brevity's\n",
    "sake, so do not be alarmed if you do not see any output for a while.\n",
    "\n",
    "After training, we evaluate the models on the unseen dataset. We will report\n",
    "the Root Mean Squared Error (RMSE) here.\n",
    "\n",
    "We observe that the cross network achieved significantly lower RMSE compared to\n",
    "a ReLU-based DNN, while also using fewer parameters. This points to the\n",
    "efficiency of the cross network in learning feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "cross_network_rmse, cross_network_num_params = train_and_evaluate(\n",
    "    learning_rate=TOY_CONFIG[\"learning_rate\"],\n",
    "    epochs=TOY_CONFIG[\"num_epochs\"],\n",
    "    train_data=train_ds,\n",
    "    test_data=test_ds,\n",
    "    model=cross_network,\n",
    ")\n",
    "print_stats(\n",
    "    rmse_list=[cross_network_rmse],\n",
    "    num_params=cross_network_num_params,\n",
    "    model_name=\"Cross Network\",\n",
    ")\n",
    "\n",
    "deep_network_rmse, deep_network_num_params = train_and_evaluate(\n",
    "    learning_rate=TOY_CONFIG[\"learning_rate\"],\n",
    "    epochs=TOY_CONFIG[\"num_epochs\"],\n",
    "    train_data=train_ds,\n",
    "    test_data=test_ds,\n",
    "    model=deep_network,\n",
    ")\n",
    "print_stats(\n",
    "    rmse_list=[deep_network_rmse],\n",
    "    num_params=deep_network_num_params,\n",
    "    model_name=\"Deep Network\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualizing feature interactions\n",
    "\n",
    "Since we already know which feature crosses are important in our data, it would\n",
    "be interesting to verify whether our model has indeed learned these key feature\n",
    "interactions. This can be done by visualizing the learned weight matrix in the\n",
    "cross network, where the weight `Wij` represents the learned importance of\n",
    "the interaction between features `xi` and `xj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "visualize_layer(\n",
    "    matrix=cross_network.weights[0].numpy(),\n",
    "    features=[\"country\", \"purchased_bananas\", \"purchased_cookbooks\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Real-world example\n",
    "\n",
    "Let's use the MovieLens 100K dataset. This dataset is used to train models to\n",
    "predict users' movie ratings, based on user-related features and movie-related\n",
    "features.\n",
    "\n",
    "### Preparing the dataset\n",
    "\n",
    "The dataset processing steps here are similar to what's given in the\n",
    "[basic ranking](/keras_rs/examples/basic_ranking/)\n",
    "tutorial. Let's load the dataset, and keep only the useful columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ratings_ds = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "ratings_ds = ratings_ds.map(\n",
    "    lambda x: (\n",
    "        {\n",
    "            \"movie_id\": int(x[\"movie_id\"]),\n",
    "            \"user_id\": int(x[\"user_id\"]),\n",
    "            \"user_gender\": int(x[\"user_gender\"]),\n",
    "            \"user_zip_code\": x[\"user_zip_code\"],\n",
    "            \"user_occupation_text\": x[\"user_occupation_text\"],\n",
    "            \"bucketized_user_age\": int(x[\"bucketized_user_age\"]),\n",
    "        },\n",
    "        x[\"user_rating\"],  # label\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For every feature, let's get the list of unique values, i.e., vocabulary, so\n",
    "that we can use that for the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "vocabularies = {}\n",
    "for feature_name in MOVIELENS_CONFIG[\"int_features\"] + MOVIELENS_CONFIG[\"str_features\"]:\n",
    "    vocabulary = ratings_ds.batch(10_000).map(lambda x, y: x[feature_name])\n",
    "    vocabularies[feature_name] = np.unique(np.concatenate(list(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "One thing we need to do is to use `keras.layers.StringLookup` and\n",
    "`keras.layers.IntegerLookup` to convert all features into indices, which can\n",
    "then be fed into embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "lookup_layers = {}\n",
    "lookup_layers.update(\n",
    "    {\n",
    "        feature: keras.layers.IntegerLookup(vocabulary=vocabularies[feature])\n",
    "        for feature in MOVIELENS_CONFIG[\"int_features\"]\n",
    "    }\n",
    ")\n",
    "lookup_layers.update(\n",
    "    {\n",
    "        feature: keras.layers.StringLookup(vocabulary=vocabularies[feature])\n",
    "        for feature in MOVIELENS_CONFIG[\"str_features\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "ratings_ds = ratings_ds.map(\n",
    "    lambda x, y: (\n",
    "        {\n",
    "            feature_name: lookup_layers[feature_name](x[feature_name])\n",
    "            for feature_name in vocabularies\n",
    "        },\n",
    "        y,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's split our data into train and test sets. We also use `cache()` and\n",
    "`prefetch()` for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ratings_ds = ratings_ds.shuffle(100_000)\n",
    "\n",
    "train_ds = (\n",
    "    ratings_ds.take(80_000)\n",
    "    .batch(MOVIELENS_CONFIG[\"batch_size\"])\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_ds = (\n",
    "    ratings_ds.skip(80_000)\n",
    "    .batch(MOVIELENS_CONFIG[\"batch_size\"])\n",
    "    .take(20_000)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building the model\n",
    "\n",
    "The model will have embedding layers, followed by cross and/or feedforward\n",
    "layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model(\n",
    "    dense_num_units_lst,\n",
    "    embedding_dim=MOVIELENS_CONFIG[\"embedding_dim\"],\n",
    "    use_cross_layer=False,\n",
    "    projection_dim=None,\n",
    "):\n",
    "    inputs = {}\n",
    "    embeddings = []\n",
    "    for feature_name, vocabulary in vocabularies.items():\n",
    "        inputs[feature_name] = keras.Input(shape=(), dtype=\"int32\", name=feature_name)\n",
    "        embedding_layer = keras.layers.Embedding(\n",
    "            input_dim=len(vocabulary) + 1,\n",
    "            output_dim=embedding_dim,\n",
    "        )\n",
    "        embedding = embedding_layer(inputs[feature_name])\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    x = keras.ops.concatenate(embeddings, axis=1)\n",
    "\n",
    "    # Cross layer.\n",
    "    if use_cross_layer:\n",
    "        x = keras_rs.layers.FeatureCross(projection_dim=projection_dim)(x)\n",
    "\n",
    "    # Dense layer.\n",
    "    for num_units in dense_num_units_lst:\n",
    "        x = keras.layers.Dense(num_units, activation=\"relu\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We have three models - a deep cross network, an optimised deep cross\n",
    "network with a low-rank matrix (to reduce training and serving costs) and a\n",
    "normal deep network without cross layers. The deep cross network is a stacked\n",
    "DCN model, i.e., the inputs are fed to cross layers, followed by feedforward\n",
    "layers.  Let's run each model 10 times, and report the average/standard\n",
    "deviation of the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "cross_network_rmse_list = []\n",
    "opt_cross_network_rmse_list = []\n",
    "deep_network_rmse_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "    cross_network = get_model(\n",
    "        dense_num_units_lst=MOVIELENS_CONFIG[\"dcn_num_units\"],\n",
    "        embedding_dim=MOVIELENS_CONFIG[\"embedding_dim\"],\n",
    "        use_cross_layer=True,\n",
    "    )\n",
    "    rmse, cross_network_num_params = train_and_evaluate(\n",
    "        learning_rate=MOVIELENS_CONFIG[\"learning_rate\"],\n",
    "        epochs=MOVIELENS_CONFIG[\"num_epochs\"],\n",
    "        train_data=train_ds,\n",
    "        test_data=test_ds,\n",
    "        model=cross_network,\n",
    "    )\n",
    "    cross_network_rmse_list.append(rmse)\n",
    "\n",
    "    opt_cross_network = get_model(\n",
    "        dense_num_units_lst=MOVIELENS_CONFIG[\"dcn_num_units\"],\n",
    "        embedding_dim=MOVIELENS_CONFIG[\"embedding_dim\"],\n",
    "        use_cross_layer=True,\n",
    "        projection_dim=MOVIELENS_CONFIG[\"projection_dim\"],\n",
    "    )\n",
    "    rmse, opt_cross_network_num_params = train_and_evaluate(\n",
    "        learning_rate=MOVIELENS_CONFIG[\"learning_rate\"],\n",
    "        epochs=MOVIELENS_CONFIG[\"num_epochs\"],\n",
    "        train_data=train_ds,\n",
    "        test_data=test_ds,\n",
    "        model=opt_cross_network,\n",
    "    )\n",
    "    opt_cross_network_rmse_list.append(rmse)\n",
    "\n",
    "    deep_network = get_model(dense_num_units_lst=MOVIELENS_CONFIG[\"deep_net_num_units\"])\n",
    "    rmse, deep_network_num_params = train_and_evaluate(\n",
    "        learning_rate=MOVIELENS_CONFIG[\"learning_rate\"],\n",
    "        epochs=MOVIELENS_CONFIG[\"num_epochs\"],\n",
    "        train_data=train_ds,\n",
    "        test_data=test_ds,\n",
    "        model=deep_network,\n",
    "    )\n",
    "    deep_network_rmse_list.append(rmse)\n",
    "\n",
    "print_stats(\n",
    "    rmse_list=cross_network_rmse_list,\n",
    "    num_params=cross_network_num_params,\n",
    "    model_name=\"Cross Network\",\n",
    ")\n",
    "print_stats(\n",
    "    rmse_list=opt_cross_network_rmse_list,\n",
    "    num_params=opt_cross_network_num_params,\n",
    "    model_name=\"Optimised Cross Network\",\n",
    ")\n",
    "print_stats(\n",
    "    rmse_list=deep_network_rmse_list,\n",
    "    num_params=deep_network_num_params,\n",
    "    model_name=\"Deep Network\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "DCN outperforms a similarly sized DNN with ReLU layers, demonstrating\n",
    "superior performance. Furthermore, the low-rank DCN effectively reduces the\n",
    "number of parameters without compromising accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualizing feature interactions\n",
    "\n",
    "Like we did for the toy example, we will plot the weight matrix of the cross\n",
    "layer to see which feature crosses are important. In the previous example,\n",
    "the importance of interactions between the `i`-th and `j-th` features is\n",
    "captured by the `(i, j)`-{th} element of the weight matrix.\n",
    "\n",
    "In this case, the feature embeddings are of size 32 rather than 1. Therefore,\n",
    "the importance of feature interactions is represented by the `(i, j)`-th\n",
    "block of the weight matrix, which has dimensions `32 x 32`. To quantify the\n",
    "significance of these interactions, we use the Frobenius norm of each block. A\n",
    "larger value implies higher importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "features = list(vocabularies.keys())\n",
    "mat = cross_network.weights[len(features)].numpy()\n",
    "embedding_dim = MOVIELENS_CONFIG[\"embedding_dim\"]\n",
    "\n",
    "block_norm = np.zeros([len(features), len(features)])\n",
    "\n",
    "# Compute the norms of the blocks.\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features)):\n",
    "        block = mat[\n",
    "            i * embedding_dim : (i + 1) * embedding_dim,\n",
    "            j * embedding_dim : (j + 1) * embedding_dim,\n",
    "        ]\n",
    "        block_norm[i, j] = np.linalg.norm(block, ord=\"fro\")\n",
    "\n",
    "visualize_layer(\n",
    "    matrix=block_norm,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And we are all done!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcn",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}