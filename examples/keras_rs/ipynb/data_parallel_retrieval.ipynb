{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Retrieval with data parallel training\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Fabien Hertschuh](https://github.com/hertschuh/)<br>\n",
    "**Date created:** 2025/04/28<br>\n",
    "**Last modified:** 2025/04/28<br>\n",
    "**Description:** Retrieve movies using a two tower model (data parallel training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we are going to train the exact same retrieval model as we\n",
    "did in our\n",
    "[basic retrieval](/keras_rs/examples/basic_retrieval/)\n",
    "tutorial, but in a distributed way.\n",
    "\n",
    "Distributed training is used to train models on multiple devices or machines\n",
    "simultaneously, thereby reducing training time. Here, we focus on synchronous\n",
    "data parallel training. Each accelerator (GPU/TPU) holds a complete replica\n",
    "of the model, and sees a different mini-batch of the input data. Local gradients\n",
    "are computed on each device, aggregated and used to compute a global gradient\n",
    "update.\n",
    "\n",
    "Before we begin, let's note down a few things:\n",
    "\n",
    "1. The number of accelerators should be greater than 1.\n",
    "2. The `keras.distribution` API works only with JAX. So, make sure you select\n",
    "   JAX as your backend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import random\n",
    "\n",
    "import jax\n",
    "import keras\n",
    "import tensorflow as tf  # Needed only for the dataset\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import keras_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Data Parallel\n",
    "\n",
    "For the synchronous data parallelism strategy in distributed training,\n",
    "we will use the `DataParallel` class present in the `keras.distribution`\n",
    "API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "devices = jax.devices()  # Assume it has >1 local devices.\n",
    "data_parallel = keras.distribution.DataParallel(devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Alternatively, you can choose to create the `DataParallel` object\n",
    "using a 1D `DeviceMesh` object, like so:\n",
    "\n",
    "```\n",
    "mesh_1d = keras.distribution.DeviceMesh(\n",
    "    shape=(len(devices),), axis_names=[\"data\"], devices=devices\n",
    ")\n",
    "data_parallel = keras.distribution.DataParallel(device_mesh=mesh_1d)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Set the global distribution strategy.\n",
    "keras.distribution.set_distribution(data_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Now that we are done defining the global distribution\n",
    "strategy, the rest of the guide looks exactly the same\n",
    "as the previous basic retrieval guide.\n",
    "\n",
    "Let's load and prepare the dataset. Here too, we use the\n",
    "MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Ratings data with user and movie data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "# User, movie counts for defining vocabularies.\n",
    "users_count = (\n",
    "    ratings.map(lambda x: tf.strings.to_number(x[\"user_id\"], out_type=tf.int32))\n",
    "    .reduce(tf.constant(0, tf.int32), tf.maximum)\n",
    "    .numpy()\n",
    ")\n",
    "movies_count = movies.cardinality().numpy()\n",
    "\n",
    "\n",
    "# Preprocess dataset, and split it into train-test datasets.\n",
    "def preprocess_rating(x):\n",
    "    return (\n",
    "        # Input is the user IDs\n",
    "        tf.strings.to_number(x[\"user_id\"], out_type=tf.int32),\n",
    "        # Labels are movie IDs + ratings between 0 and 1.\n",
    "        {\n",
    "            \"movie_id\": tf.strings.to_number(x[\"movie_id\"], out_type=tf.int32),\n",
    "            \"rating\": (x[\"user_rating\"] - 1.0) / 4.0,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "shuffled_ratings = ratings.map(preprocess_rating).shuffle(\n",
    "    100_000, seed=42, reshuffle_each_iteration=False\n",
    ")\n",
    "train_ratings = shuffled_ratings.take(80_000).batch(1000).cache()\n",
    "test_ratings = shuffled_ratings.skip(80_000).take(20_000).batch(1000).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implementing the Model\n",
    "\n",
    "We build a two-tower retrieval model. Therefore, we need to combine a\n",
    "query tower for users and a candidate tower for movies. Note that we don't\n",
    "have to change anything here from the previous basic retrieval tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RetrievalModel(keras.Model):\n",
    "    \"\"\"Create the retrieval model with the provided parameters.\n",
    "\n",
    "    Args:\n",
    "      num_users: Number of entries in the user embedding table.\n",
    "      num_candidates: Number of entries in the candidate embedding table.\n",
    "      embedding_dimension: Output dimension for user and movie embedding tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_candidates,\n",
    "        embedding_dimension=32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Our query tower, simply an embedding table.\n",
    "        self.user_embedding = keras.layers.Embedding(num_users, embedding_dimension)\n",
    "        # Our candidate tower, simply an embedding table.\n",
    "        self.candidate_embedding = keras.layers.Embedding(\n",
    "            num_candidates, embedding_dimension\n",
    "        )\n",
    "        # The layer that performs the retrieval.\n",
    "        self.retrieval = keras_rs.layers.BruteForceRetrieval(k=10, return_scores=False)\n",
    "        self.loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.user_embedding.build(input_shape)\n",
    "        self.candidate_embedding.build(input_shape)\n",
    "        # In this case, the candidates are directly the movie embeddings.\n",
    "        # We take a shortcut and directly reuse the variable.\n",
    "        self.retrieval.candidate_embeddings = self.candidate_embedding.embeddings\n",
    "        self.retrieval.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_embeddings = self.user_embedding(inputs)\n",
    "        result = {\n",
    "            \"user_embeddings\": user_embeddings,\n",
    "        }\n",
    "        if not training:\n",
    "            # Skip the retrieval of top movies during training as the\n",
    "            # predictions are not used.\n",
    "            result[\"predictions\"] = self.retrieval(user_embeddings)\n",
    "        return result\n",
    "\n",
    "    def compute_loss(self, x, y, y_pred, sample_weight, training=True):\n",
    "        candidate_id, rating = y[\"movie_id\"], y[\"rating\"]\n",
    "        user_embeddings = y_pred[\"user_embeddings\"]\n",
    "        candidate_embeddings = self.candidate_embedding(candidate_id)\n",
    "\n",
    "        labels = keras.ops.expand_dims(rating, -1)\n",
    "        # Compute the affinity score by multiplying the two embeddings.\n",
    "        scores = keras.ops.sum(\n",
    "            keras.ops.multiply(user_embeddings, candidate_embeddings),\n",
    "            axis=1,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        return self.loss_fn(labels, scores, sample_weight)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we can use the standard Keras `model.fit()` to train\n",
    "and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = RetrievalModel(users_count + 1, movies_count + 1)\n",
    "model.compile(optimizer=keras.optimizers.Adagrad(learning_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's train the model. Evaluation takes a bit of time, so we only evaluate the\n",
    "model every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ratings, validation_data=test_ratings, validation_freq=5, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, let's run inference and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "movie_id_to_movie_title = {\n",
    "    int(x[\"movie_id\"]): x[\"movie_title\"] for x in movies.as_numpy_iterator()\n",
    "}\n",
    "movie_id_to_movie_title[0] = \"\"  # Because id 0 is not in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We then simply use the Keras `model.predict()` method. Under the hood, it calls\n",
    "the `BruteForceRetrieval` layer to perform the actual retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "user_ids = random.sample(range(1, 1001), len(devices))\n",
    "predictions = model.predict(keras.ops.convert_to_tensor(user_ids))\n",
    "predictions = keras.ops.convert_to_numpy(predictions[\"predictions\"])\n",
    "\n",
    "for i, user_id in enumerate(user_ids):\n",
    "    print(f\"\\n==Recommended movies for user {user_id}==\")\n",
    "    for movie_id in predictions[i]:\n",
    "        print(movie_id_to_movie_title[movie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And we're done! For data parallel training, all we had to do was add ~3-5 LoC.\n",
    "The rest is exactly the same."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "data_parallel_retrieval",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}