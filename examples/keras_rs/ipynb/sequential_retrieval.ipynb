{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Sequential retrieval [GRU4Rec]\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Fabien Hertschuh](https://github.com/hertschuh/)<br>\n",
    "**Date created:** 2025/04/28<br>\n",
    "**Last modified:** 2025/04/28<br>\n",
    "**Description:** Recommend movies using a GRU-based sequential retrieval model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we are going to build a sequential retrieval model. Sequential\n",
    "recommendation is a popular model that looks at a sequence of items that users\n",
    "have interacted with previously and then predicts the next item. Here, the order\n",
    "of the items within each sequence matters. So, we are going to use a recurrent\n",
    "neural network to model the sequential relationship. For more details,\n",
    "please refer to the [GRU4Rec](https://arxiv.org/abs/1511.06939) paper.\n",
    "\n",
    "Let's begin by choosing JAX as the backend we want to run on, and import all\n",
    "the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # `\"tensorflow\"`/`\"torch\"`\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf  # Needed only for the dataset\n",
    "\n",
    "import keras_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's also define all important variables/hyperparameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./raw/data/\"\n",
    "\n",
    "# MovieLens-specific variables\n",
    "MOVIELENS_1M_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "MOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\n",
    "\n",
    "RATINGS_FILE_NAME = \"ratings.dat\"\n",
    "MOVIES_FILE_NAME = \"movies.dat\"\n",
    "\n",
    "# Data processing args\n",
    "MAX_CONTEXT_LENGTH = 10\n",
    "MIN_SEQUENCE_LENGTH = 3\n",
    "TRAIN_DATA_FRACTION = 0.9\n",
    "\n",
    "RATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\n",
    "MOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "MIN_RATING = 2\n",
    "\n",
    "# Training/model args\n",
    "BATCH_SIZE = 4096\n",
    "TEST_BATCH_SIZE = 2048\n",
    "EMBEDDING_DIM = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Next, we need to prepare our dataset. Like we did in the\n",
    "[basic retrieval](/keras_rs/examples/basic_retrieval/)\n",
    "example, we are going to use the MovieLens dataset.\n",
    "\n",
    "The dataset preparation step is fairly involved. The original ratings dataset\n",
    "contains `(user, movie ID, rating, timestamp)` tuples (among other columns,\n",
    "which are not important for this example). Since we are dealing with sequential\n",
    "retrieval, we need to create movie sequences for every user, where the sequences\n",
    "are ordered by timestamp.\n",
    "\n",
    "Let's start by downloading and reading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Download the MovieLens dataset.\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "path_to_zip = keras.utils.get_file(\n",
    "    fname=\"ml-1m.zip\",\n",
    "    origin=MOVIELENS_1M_URL,\n",
    "    file_hash=MOVIELENS_ZIP_HASH,\n",
    "    hash_algorithm=\"sha256\",\n",
    "    extract=True,\n",
    "    cache_dir=DATA_DIR,\n",
    ")\n",
    "movielens_extracted_dir = os.path.join(\n",
    "    os.path.dirname(path_to_zip),\n",
    "    \"ml-1m_extracted\",\n",
    "    \"ml-1m\",\n",
    ")\n",
    "\n",
    "\n",
    "# Read the dataset.\n",
    "def read_data(data_directory, min_rating=None):\n",
    "    \"\"\"Read movielens ratings.dat and movies.dat file\n",
    "    into dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ratings_df = pd.read_csv(\n",
    "        os.path.join(data_directory, RATINGS_FILE_NAME),\n",
    "        sep=\"::\",\n",
    "        names=RATINGS_DATA_COLUMNS,\n",
    "        encoding=\"unicode_escape\",\n",
    "    )\n",
    "    ratings_df[\"Timestamp\"] = ratings_df[\"Timestamp\"].apply(int)\n",
    "\n",
    "    # Remove movies with `rating < min_rating`.\n",
    "    if min_rating is not None:\n",
    "        ratings_df = ratings_df[ratings_df[\"Rating\"] >= min_rating]\n",
    "\n",
    "    movies_df = pd.read_csv(\n",
    "        os.path.join(data_directory, MOVIES_FILE_NAME),\n",
    "        sep=\"::\",\n",
    "        names=MOVIES_DATA_COLUMNS,\n",
    "        encoding=\"unicode_escape\",\n",
    "    )\n",
    "    return ratings_df, movies_df\n",
    "\n",
    "\n",
    "ratings_df, movies_df = read_data(\n",
    "    data_directory=movielens_extracted_dir, min_rating=MIN_RATING\n",
    ")\n",
    "\n",
    "# Need to know #movies so as to define embedding layers.\n",
    "movies_count = movies_df[\"MovieID\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we have read the dataset, let's create sequences of movies\n",
    "for every user. Here is the function for doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_movie_sequence_per_user(ratings_df):\n",
    "    \"\"\"Get movieID sequences for every user.\"\"\"\n",
    "    sequences = collections.defaultdict(list)\n",
    "\n",
    "    for user_id, movie_id, rating, timestamp in ratings_df.values:\n",
    "        sequences[user_id].append(\n",
    "            {\n",
    "                \"movie_id\": movie_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"rating\": rating,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort movie sequences by timestamp for every user.\n",
    "    for user_id, context in sequences.items():\n",
    "        context.sort(key=lambda x: x[\"timestamp\"])\n",
    "        sequences[user_id] = context\n",
    "\n",
    "    return sequences\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We need to do some filtering and processing before we proceed\n",
    "with training the model:\n",
    "\n",
    "1. Form sequences of all lengths up to\n",
    "   `min(user_sequence_length, MAX_CONTEXT_LENGTH)`. So, every user\n",
    "   will have multiple sequences corresponding to it.\n",
    "2. Get labels, i.e., Given a sequence of length `n`, the first\n",
    "   `n-1` tokens will be fed to the model as input, and the label\n",
    "   will be the last token.\n",
    "3. Remove all user sequences with less than `MIN_SEQUENCE_LENGTH`\n",
    "   movies.\n",
    "4. Pad all sequences to `MAX_CONTEXT_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_examples_from_user_sequences(sequences):\n",
    "    \"\"\"Generates sequences for all users, with padding, truncation, etc.\"\"\"\n",
    "\n",
    "    def generate_examples_from_user_sequence(sequence):\n",
    "        \"\"\"Generates examples for a single user sequence.\"\"\"\n",
    "\n",
    "        examples = []\n",
    "        for label_idx in range(1, len(sequence)):\n",
    "            start_idx = max(0, label_idx - MAX_CONTEXT_LENGTH)\n",
    "            context = sequence[start_idx:label_idx]\n",
    "\n",
    "            # Padding\n",
    "            while len(context) < MAX_CONTEXT_LENGTH:\n",
    "                context.append(\n",
    "                    {\n",
    "                        \"movie_id\": 0,\n",
    "                        \"timestamp\": 0,\n",
    "                        \"rating\": 0.0,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            label_movie_id = int(sequence[label_idx][\"movie_id\"])\n",
    "            context_movie_id = [int(movie[\"movie_id\"]) for movie in context]\n",
    "\n",
    "            examples.append(\n",
    "                {\n",
    "                    \"context_movie_id\": context_movie_id,\n",
    "                    \"label_movie_id\": label_movie_id,\n",
    "                },\n",
    "            )\n",
    "        return examples\n",
    "\n",
    "    all_examples = []\n",
    "    for sequence in sequences.values():\n",
    "        if len(sequence) < MIN_SEQUENCE_LENGTH:\n",
    "            continue\n",
    "\n",
    "        user_examples = generate_examples_from_user_sequence(sequence)\n",
    "\n",
    "        all_examples.extend(user_examples)\n",
    "\n",
    "    return all_examples\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's split the dataset into train and test sets. Also, we need to\n",
    "change the format of the dataset dictionary so as to enable conversion\n",
    "to a `tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sequences = get_movie_sequence_per_user(ratings_df)\n",
    "examples = generate_examples_from_user_sequences(sequences)\n",
    "\n",
    "# Train-test split.\n",
    "random.shuffle(examples)\n",
    "split_index = int(TRAIN_DATA_FRACTION * len(examples))\n",
    "train_examples = examples[:split_index]\n",
    "test_examples = examples[split_index:]\n",
    "\n",
    "\n",
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    \"\"\"Convert list of dictionaries to dictionary of lists for\n",
    "    `tf.data` conversion.\n",
    "    \"\"\"\n",
    "    dict_of_lists = collections.defaultdict(list)\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key, value in dictionary.items():\n",
    "            dict_of_lists[key].append(value)\n",
    "    return dict_of_lists\n",
    "\n",
    "\n",
    "train_examples = list_of_dicts_to_dict_of_lists(train_examples)\n",
    "test_examples = list_of_dicts_to_dict_of_lists(test_examples)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_examples).map(\n",
    "    lambda x: (x[\"context_movie_id\"], x[\"label_movie_id\"])\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_examples).map(\n",
    "    lambda x: (x[\"context_movie_id\"], x[\"label_movie_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We need to batch our datasets. We also user `cache()` and `prefetch()`\n",
    "for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(TEST_BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's print out one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for sample in train_ds.take(1):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model and Training\n",
    "\n",
    "In the basic retrieval example, we used one query tower for the\n",
    "user, and the candidate tower for the candidate movie. We are\n",
    "going to use a two-tower architecture here as well. However,\n",
    "we use the query tower with a Gated Recurrent Unit (GRU) layer\n",
    "to encode the sequence of historical movies, and keep the same\n",
    "candidate tower for the candidate movie.\n",
    "\n",
    "Note: Take a look at how the labels are defined. The label tensor\n",
    "(of shape `(batch_size, batch_size)`) contains one-hot vectors. The idea\n",
    "is: for every sample, consider movie IDs corresponding to other samples in\n",
    "the batch as negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SequentialRetrievalModel(keras.Model):\n",
    "    \"\"\"Create the sequential retrieval model.\n",
    "\n",
    "    Args:\n",
    "      movies_count: Total number of unique movies in the dataset.\n",
    "      embedding_dimension: Output dimension for movie embedding tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        movies_count,\n",
    "        embedding_dimension=128,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Our query tower, simply an embedding table followed by\n",
    "        # a GRU unit. This encodes sequence of historical movies.\n",
    "        self.query_model = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Embedding(movies_count + 1, embedding_dimension),\n",
    "                keras.layers.GRU(embedding_dimension),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Our candidate tower, simply an embedding table.\n",
    "        self.candidate_model = keras.layers.Embedding(\n",
    "            movies_count + 1, embedding_dimension\n",
    "        )\n",
    "\n",
    "        # The layer that performs the retrieval.\n",
    "        self.retrieval = keras_rs.layers.BruteForceRetrieval(k=10, return_scores=False)\n",
    "        self.loss_fn = keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_model.build(input_shape)\n",
    "        self.candidate_model.build(input_shape)\n",
    "\n",
    "        # In this case, the candidates are directly the movie embeddings.\n",
    "        # We take a shortcut and directly reuse the variable.\n",
    "        self.retrieval.candidate_embeddings = self.candidate_model.embeddings\n",
    "        self.retrieval.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        query_embeddings = self.query_model(inputs)\n",
    "        result = {\n",
    "            \"query_embeddings\": query_embeddings,\n",
    "        }\n",
    "\n",
    "        if not training:\n",
    "            # Skip the retrieval of top movies during training as the\n",
    "            # predictions are not used.\n",
    "            result[\"predictions\"] = self.retrieval(query_embeddings)\n",
    "        return result\n",
    "\n",
    "    def compute_loss(self, x, y, y_pred, sample_weight, training=True):\n",
    "        candidate_id = y\n",
    "        query_embeddings = y_pred[\"query_embeddings\"]\n",
    "        candidate_embeddings = self.candidate_model(candidate_id)\n",
    "\n",
    "        num_queries = keras.ops.shape(query_embeddings)[0]\n",
    "        num_candidates = keras.ops.shape(candidate_embeddings)[0]\n",
    "\n",
    "        # One-hot vectors for labels.\n",
    "        labels = keras.ops.eye(num_queries, num_candidates)\n",
    "\n",
    "        # Compute the affinity score by multiplying the two embeddings.\n",
    "        scores = keras.ops.matmul(\n",
    "            query_embeddings, keras.ops.transpose(candidate_embeddings)\n",
    "        )\n",
    "\n",
    "        return self.loss_fn(labels, scores, sample_weight)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's instantiate, compile and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = SequentialRetrievalModel(\n",
    "    movies_count=movies_count + 1, embedding_dimension=EMBEDDING_DIM\n",
    ")\n",
    "\n",
    "# Compile.\n",
    "model.compile(optimizer=keras.optimizers.AdamW(learning_rate=LEARNING_RATE))\n",
    "\n",
    "# Train.\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, we would like to be able to make predictions.\n",
    "\n",
    "So far, we have only handled movies by id. Now is the time to create a mapping\n",
    "keyed by movie IDs to be able to surface the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "movie_id_to_movie_title = dict(zip(movies_df[\"MovieID\"], movies_df[\"Title\"]))\n",
    "movie_id_to_movie_title[0] = \"\"  # Because id 0 is not in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We then simply use the Keras `model.predict()` method. Under the hood, it calls\n",
    "the `BruteForceRetrieval` layer to perform the actual retrieval.\n",
    "\n",
    "Note that this model can retrieve movies already watched by the user. We could\n",
    "easily add logic to remove them if that is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"\\n==> Movies the user has watched:\")\n",
    "movie_sequence = test_ds.unbatch().take(1)\n",
    "for element in movie_sequence:\n",
    "    for movie_id in element[0][:-1]:\n",
    "        print(movie_id_to_movie_title[movie_id.numpy()], end=\", \")\n",
    "    print(movie_id_to_movie_title[element[0][-1].numpy()])\n",
    "\n",
    "predictions = model.predict(movie_sequence.batch(1))\n",
    "predictions = keras.ops.convert_to_numpy(predictions[\"predictions\"])\n",
    "\n",
    "print(\"\\n==> Recommended movies for the above sequence:\")\n",
    "for movie_id in predictions[0]:\n",
    "    print(movie_id_to_movie_title[movie_id])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sequential_retrieval",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}