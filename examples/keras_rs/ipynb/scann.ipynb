{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Faster retrieval with Scalable Nearest Neighbours (ScANN)\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Fabien Hertschuh](https://github.com/hertschuh/)<br>\n",
    "**Date created:** 2025/04/28<br>\n",
    "**Last modified:** 2025/04/28<br>\n",
    "**Description:** Using ScANN for faster retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Retrieval models are designed to quickly identify a small set of highly relevant\n",
    "candidates from vast pools of data, often comprising millions or even hundreds\n",
    "of millions of items. To effectively respond to the user's context and behavior\n",
    "in real time, these models must perform this task in just milliseconds.\n",
    "\n",
    "Approximate nearest neighbor (ANN) search is the key technology that enables\n",
    "this level of efficiency. In this tutorial, we'll demonstrate how to leverage\n",
    "ScANN\u2014a cutting-edge nearest neighbor retrieval library\u2014to effortlessly scale\n",
    "retrieval for millions of items.\n",
    "\n",
    "[ScANN](https://research.google/blog/announcing-scann-efficient-vector-similarity-search/),\n",
    "developed by Google Research, is a high-performance library designed for\n",
    "dense vector similarity search at scale. It efficiently indexes a database of\n",
    "candidate embeddings, enabling rapid search during inference. By leveraging\n",
    "advanced vector compression techniques and finely tuned algorithms, ScaNN\n",
    "strikes an optimal balance between speed and accuracy. As a result, it can\n",
    "significantly outperform brute-force search methods, delivering fast retrieval\n",
    "with minimal loss in accuracy.\n",
    "\n",
    "We will start with the same code as the\n",
    "[basic retrieval example](/keras_rs/examples/basic_retrieval/).\n",
    "Data processing, model building, and training remain exactly the same. Feel free\n",
    "to skip this part if you have gone over the basic retrieval example before.\n",
    "\n",
    "Note: ScANN does not have its own separate layer in KerasRS because the ScANN\n",
    "library is TensorFlow-only. Here, in this example, we directly use the ScANN\n",
    "library and demonstrate its usage with KerasRS.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Let's install the `scann` library and import all necessary packages. We will\n",
    "also set the backend to JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # `\"tensorflow\"`/`\"torch\"`\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf  # Needed for the dataset\n",
    "import tensorflow_datasets as tfds\n",
    "from scann import scann_ops\n",
    "\n",
    "import keras_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Ratings data with user and movie data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "# Get user and movie counts so that we can define embedding layers for both.\n",
    "users_count = (\n",
    "    ratings.map(lambda x: tf.strings.to_number(x[\"user_id\"], out_type=tf.int32))\n",
    "    .reduce(tf.constant(0, tf.int32), tf.maximum)\n",
    "    .numpy()\n",
    ")\n",
    "\n",
    "movies_count = movies.cardinality().numpy()\n",
    "\n",
    "\n",
    "# Preprocess the dataset, by selecting only the relevant columns.\n",
    "def preprocess_rating(x):\n",
    "    return (\n",
    "        # Input is the user IDs\n",
    "        tf.strings.to_number(x[\"user_id\"], out_type=tf.int32),\n",
    "        # Labels are movie IDs + ratings between 0 and 1.\n",
    "        {\n",
    "            \"movie_id\": tf.strings.to_number(x[\"movie_id\"], out_type=tf.int32),\n",
    "            \"rating\": (x[\"user_rating\"] - 1.0) / 4.0,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "shuffled_ratings = ratings.map(preprocess_rating).shuffle(\n",
    "    100_000, seed=42, reshuffle_each_iteration=False\n",
    ")\n",
    "# Train-test split.\n",
    "train_ratings = shuffled_ratings.take(80_000).batch(1000).cache()\n",
    "test_ratings = shuffled_ratings.skip(80_000).take(20_000).batch(1000).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implementing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RetrievalModel(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_candidates,\n",
    "        embedding_dimension=32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Our query tower, simply an embedding table.\n",
    "        self.user_embedding = keras.layers.Embedding(num_users, embedding_dimension)\n",
    "        # Our candidate tower, simply an embedding table.\n",
    "        self.candidate_embedding = keras.layers.Embedding(\n",
    "            num_candidates, embedding_dimension\n",
    "        )\n",
    "\n",
    "        self.loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.user_embedding.build(input_shape)\n",
    "        self.candidate_embedding.build(input_shape)\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_embeddings = self.user_embedding(inputs)\n",
    "        result = {\n",
    "            \"user_embeddings\": user_embeddings,\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def compute_loss(self, x, y, y_pred, sample_weight, training=True):\n",
    "        candidate_id, rating = y[\"movie_id\"], y[\"rating\"]\n",
    "        user_embeddings = y_pred[\"user_embeddings\"]\n",
    "        candidate_embeddings = self.candidate_embedding(candidate_id)\n",
    "\n",
    "        labels = keras.ops.expand_dims(rating, -1)\n",
    "        # Compute the affinity score by multiplying the two embeddings.\n",
    "        scores = keras.ops.sum(\n",
    "            keras.ops.multiply(user_embeddings, candidate_embeddings),\n",
    "            axis=1,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        return self.loss_fn(labels, scores, sample_weight)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = RetrievalModel(users_count + 1000, movies_count + 1000)\n",
    "model.compile(optimizer=keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "history = model.fit(\n",
    "    train_ratings, validation_data=test_ratings, validation_freq=5, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Before we try out ScANN, let's go with the brute force method, i.e., for a given\n",
    "user, scores are computed for all movies, sorted and then the top-k\n",
    "movies are picked. This is, of course, not very scalable when we have a huge\n",
    "number of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "candidate_embeddings = keras.ops.array(model.candidate_embedding.embeddings.numpy())\n",
    "# Artificially duplicate candidate embeddings to simulate a large number of\n",
    "# movies.\n",
    "candidate_embeddings = keras.ops.concatenate(\n",
    "    [candidate_embeddings]\n",
    "    + [\n",
    "        candidate_embeddings\n",
    "        * keras.random.uniform(keras.ops.shape(candidate_embeddings))\n",
    "        for _ in range(100)\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "user_embedding = model.user_embedding(keras.ops.array([10, 5, 42, 345]))\n",
    "\n",
    "# Define the brute force retrieval layer.\n",
    "brute_force_layer = keras_rs.layers.BruteForceRetrieval(\n",
    "    candidate_embeddings=candidate_embeddings,\n",
    "    k=10,\n",
    "    return_scores=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's do a forward pass on the layer. Note that in previous tutorials, we\n",
    "have the above layer as an attribute of the model class, and we then call\n",
    "`.predict()`. This will obviously be faster (since it's compiled XLA code), but\n",
    "since we cannot do the same for ScANN, we just do a normal forward pass here\n",
    "without compilation to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "pred_movie_ids = brute_force_layer(user_embedding)\n",
    "print(\"Time taken by brute force layer (sec):\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's retrieve movies using ScANN. We will use the ScANN library from\n",
    "Google Research to build the layer and then call it. To fully understand all the\n",
    "arguments, please refer to the\n",
    "[ScANN README file](https://github.com/google-research/google-research/tree/master/scann#readme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_scann(\n",
    "    candidates,\n",
    "    k=10,\n",
    "    distance_measure=\"dot_product\",\n",
    "    dimensions_per_block=2,\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves=100,\n",
    "    num_leaves_to_search=30,\n",
    "    training_iterations=12,\n",
    "):\n",
    "    builder = scann_ops.builder(\n",
    "        db=candidates,\n",
    "        num_neighbors=k,\n",
    "        distance_measure=distance_measure,\n",
    "    )\n",
    "\n",
    "    builder = builder.tree(\n",
    "        num_leaves=num_leaves,\n",
    "        num_leaves_to_search=num_leaves_to_search,\n",
    "        training_iterations=training_iterations,\n",
    "    )\n",
    "    builder = builder.score_ah(dimensions_per_block=dimensions_per_block)\n",
    "\n",
    "    if num_reordering_candidates is not None:\n",
    "        builder = builder.reorder(num_reordering_candidates)\n",
    "\n",
    "    # Set a unique name to prevent unintentional sharing between\n",
    "    # ScaNN instances.\n",
    "    searcher = builder.build(shared_name=str(uuid.uuid4()))\n",
    "    return searcher\n",
    "\n",
    "\n",
    "def run_scann(searcher):\n",
    "    pred_movie_ids = searcher.search_batched_parallel(\n",
    "        user_embedding,\n",
    "        final_num_neighbors=10,\n",
    "    ).indices\n",
    "    return pred_movie_ids\n",
    "\n",
    "\n",
    "searcher = build_scann(candidates=candidate_embeddings)\n",
    "\n",
    "t0 = time.time()\n",
    "pred_movie_ids = run_scann(searcher)\n",
    "print(\"Time taken by ScANN (sec):\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You can clearly see the performance improvement in terms of latency. ScANN\n",
    "(0.003 seconds) takes one-fiftieth the time it takes for the brute force layer\n",
    "(0.15 seconds) to run!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "scann",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}