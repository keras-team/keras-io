{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# List-wise ranking\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Fabien Hertschuh](https://github.com/hertschuh/)<br>\n",
    "**Date created:** 2025/04/28<br>\n",
    "**Last modified:** 2025/04/28<br>\n",
    "**Description:** Rank movies using pairwise losses instead of pointwise losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In our\n",
    "[basic ranking tutorial](/keras_rs/examples/basic_ranking/), we explored a model\n",
    "that learned to predict ratings for specific user-movie combinations. This model\n",
    "took (user, movie) pairs as input and was trained using mean-squared error to\n",
    "precisely predict the rating a user might give to a movie.\n",
    "\n",
    "However, solely optimizing a model's accuracy in predicting individual movie\n",
    "scores isn't always the most effective strategy for developing ranking systems.\n",
    "For ranking models, pinpoint accuracy in predicting scores is less critical than\n",
    "the model's capability to generate an ordered list of items that aligns with a\n",
    "user's preferences. In essence, the relative order of items matters more than\n",
    "the exact predicted values.\n",
    "\n",
    "Instead of focusing on the model's predictions for individual query-item pairs\n",
    "(a pointwise approach), we can optimize the model based on its ability to\n",
    "correctly order items. One common method for this is pairwise ranking. In this\n",
    "approach, the model learns by comparing pairs of items (e.g., item A and item B)\n",
    "and determining which one should be ranked higher for a given user or query. The\n",
    "goal is to minimize the number of incorrectly ordered pairs.\n",
    "\n",
    "Let's begin by importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # `\"tensorflow\"`/`\"torch\"`\n",
    "\n",
    "import collections\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf  # Needed only for the dataset\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import ops\n",
    "\n",
    "import keras_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's define some hyperparameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Data args\n",
    "TRAIN_NUM_LIST_PER_USER = 50\n",
    "TEST_NUM_LIST_PER_USER = 1\n",
    "NUM_EXAMPLES_PER_LIST = 5\n",
    "\n",
    "# Model args\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "# Train args\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "We use the MovieLens dataset. The data loading and processing steps are similar\n",
    "to previous tutorials, so, we will only discuss the differences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "users_count = (\n",
    "    ratings.map(lambda x: tf.strings.to_number(x[\"user_id\"], out_type=tf.int32))\n",
    "    .reduce(tf.constant(0, tf.int32), tf.maximum)\n",
    "    .numpy()\n",
    ")\n",
    "movies_count = movies.cardinality().numpy()\n",
    "\n",
    "\n",
    "def preprocess_rating(x):\n",
    "    return {\n",
    "        \"user_id\": tf.strings.to_number(x[\"user_id\"], out_type=tf.int32),\n",
    "        \"movie_id\": tf.strings.to_number(x[\"movie_id\"], out_type=tf.int32),\n",
    "        # Normalise ratings between 0 and 1.\n",
    "        \"user_rating\": (x[\"user_rating\"] - 1.0) / 4.0,\n",
    "    }\n",
    "\n",
    "\n",
    "shuffled_ratings = ratings.map(preprocess_rating).shuffle(\n",
    "    100_000, seed=42, reshuffle_each_iteration=False\n",
    ")\n",
    "train_ratings = shuffled_ratings.take(70_000)\n",
    "val_ratings = shuffled_ratings.skip(70_000).take(15_000)\n",
    "test_ratings = shuffled_ratings.skip(85_000).take(15_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "So far, we've replicated what we have in the basic ranking tutorial.\n",
    "\n",
    "However, this existing dataset is not directly applicable to list-wise\n",
    "optimization. List-wise optimization requires, for each user, a list of movies\n",
    "they have rated, allowing the model to learn from the relative orderings within\n",
    "that list. The MovieLens 100K dataset, in its original form, provides individual\n",
    "rating instances (one user, one movie, one rating per example), rather than\n",
    "these aggregated user-specific lists.\n",
    "\n",
    "To enable listwise optimization, we need to restructure the dataset. This\n",
    "involves transforming it so that each data point or example represents a single\n",
    "user ID accompanied by a list of movies that user has rated. Within these lists,\n",
    "some movies will naturally be ranked higher by the user (as evidenced by their\n",
    "ratings) than others. The primary objective for our model will then be to learn\n",
    "to predict item orderings that correspond to these observed user preferences.\n",
    "\n",
    "Let's start by getting the entire list of movies and corresponding ratings for\n",
    "every user. We remove `user_ids` corresponding to users who have rated less than\n",
    "`NUM_EXAMPLES_PER_LIST` number of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_movie_sequence_per_user(ratings, min_examples_per_list):\n",
    "    \"\"\"Gets movieID sequences and ratings for every user.\"\"\"\n",
    "    sequences = collections.defaultdict(list)\n",
    "\n",
    "    for sample in ratings:\n",
    "        user_id = sample[\"user_id\"]\n",
    "        movie_id = sample[\"movie_id\"]\n",
    "        user_rating = sample[\"user_rating\"]\n",
    "\n",
    "        sequences[int(user_id.numpy())].append(\n",
    "            {\n",
    "                \"movie_id\": int(movie_id.numpy()),\n",
    "                \"user_rating\": float(user_rating.numpy()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Remove lists with < `min_examples_per_list` number of elements.\n",
    "    sequences = {\n",
    "        user_id: sequence\n",
    "        for user_id, sequence in sequences.items()\n",
    "        if len(sequence) >= min_examples_per_list\n",
    "    }\n",
    "\n",
    "    return sequences\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We now sample 50 lists for each user for the training data. For each list, we\n",
    "randomly sample 5 movies from the movies the user rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_sublist_from_list(\n",
    "    lst,\n",
    "    num_examples_per_list,\n",
    "):\n",
    "    \"\"\"Random selects `num_examples_per_list` number of elements from list.\"\"\"\n",
    "\n",
    "    indices = np.random.choice(\n",
    "        range(len(lst)),\n",
    "        size=num_examples_per_list,\n",
    "        replace=False,\n",
    "    )\n",
    "\n",
    "    samples = [lst[i] for i in indices]\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_examples(\n",
    "    sequences,\n",
    "    num_list_per_user,\n",
    "    num_examples_per_list,\n",
    "):\n",
    "    inputs = {\n",
    "        \"user_id\": [],\n",
    "        \"movie_id\": [],\n",
    "    }\n",
    "    labels = []\n",
    "    for user_id, user_list in sequences.items():\n",
    "        sampled_list = sample_sublist_from_list(\n",
    "            user_list,\n",
    "            num_examples_per_list,\n",
    "        )\n",
    "\n",
    "        inputs[\"user_id\"].append(user_id)\n",
    "        inputs[\"movie_id\"].append(\n",
    "            tf.convert_to_tensor([f[\"movie_id\"] for f in sampled_list])\n",
    "        )\n",
    "        labels.append(tf.convert_to_tensor([f[\"user_rating\"] for f in sampled_list]))\n",
    "\n",
    "    return (\n",
    "        {\"user_id\": inputs[\"user_id\"], \"movie_id\": inputs[\"movie_id\"]},\n",
    "        labels,\n",
    "    )\n",
    "\n",
    "\n",
    "train_sequences = get_movie_sequence_per_user(\n",
    "    ratings=train_ratings, min_examples_per_list=NUM_EXAMPLES_PER_LIST\n",
    ")\n",
    "train_examples = get_examples(\n",
    "    train_sequences,\n",
    "    num_list_per_user=TRAIN_NUM_LIST_PER_USER,\n",
    "    num_examples_per_list=NUM_EXAMPLES_PER_LIST,\n",
    ")\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_examples)\n",
    "\n",
    "val_sequences = get_movie_sequence_per_user(\n",
    "    ratings=val_ratings, min_examples_per_list=5\n",
    ")\n",
    "val_examples = get_examples(\n",
    "    val_sequences,\n",
    "    num_list_per_user=TEST_NUM_LIST_PER_USER,\n",
    "    num_examples_per_list=NUM_EXAMPLES_PER_LIST,\n",
    ")\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(val_examples)\n",
    "\n",
    "test_sequences = get_movie_sequence_per_user(\n",
    "    ratings=test_ratings, min_examples_per_list=5\n",
    ")\n",
    "test_examples = get_examples(\n",
    "    test_sequences,\n",
    "    num_list_per_user=TEST_NUM_LIST_PER_USER,\n",
    "    num_examples_per_list=NUM_EXAMPLES_PER_LIST,\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Batch up the dataset, and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE).cache()\n",
    "val_ds = val_ds.batch(BATCH_SIZE).cache()\n",
    "test_ds = test_ds.batch(BATCH_SIZE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Building the model\n",
    "\n",
    "We build a typical two-tower ranking model, similar to the\n",
    "[basic ranking tutorial](/keras_rs/examples/basic_ranking/).\n",
    "We have separate embedding layers for user ID and movie IDs. After obtaining\n",
    "these embeddings, we concatenate them and pass them through a network of dense\n",
    "layers.\n",
    "\n",
    "The only point of difference is that for movie IDs, we take a list of IDs\n",
    "rather than just one movie ID. So, when we concatenate user ID embedding and\n",
    "movie IDs' embeddings, we \"repeat\" the user ID 'NUM_EXAMPLES_PER_LIST' times so\n",
    "as to get the same shape as the movie IDs' embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RankingModel(keras.Model):\n",
    "    \"\"\"Create the ranking model with the provided parameters.\n",
    "\n",
    "    Args:\n",
    "      num_users: Number of entries in the user embedding table.\n",
    "      num_candidates: Number of entries in the candidate embedding table.\n",
    "      embedding_dimension: Output dimension for user and movie embedding tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_candidates,\n",
    "        embedding_dimension=32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # Embedding table for users.\n",
    "        self.user_embedding = keras.layers.Embedding(num_users, embedding_dimension)\n",
    "        # Embedding table for candidates.\n",
    "        self.candidate_embedding = keras.layers.Embedding(\n",
    "            num_candidates, embedding_dimension\n",
    "        )\n",
    "        # Predictions.\n",
    "        self.ratings = keras.Sequential(\n",
    "            [\n",
    "                # Learn multiple dense layers.\n",
    "                keras.layers.Dense(256, activation=\"relu\"),\n",
    "                keras.layers.Dense(64, activation=\"relu\"),\n",
    "                # Make rating predictions in the final layer.\n",
    "                keras.layers.Dense(1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.user_embedding.build(input_shape[\"user_id\"])\n",
    "        self.candidate_embedding.build(input_shape[\"movie_id\"])\n",
    "\n",
    "        output_shape = self.candidate_embedding.compute_output_shape(\n",
    "            input_shape[\"movie_id\"]\n",
    "        )\n",
    "\n",
    "        self.ratings.build(list(output_shape[:-1]) + [2 * output_shape[-1]])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, movie_id = inputs[\"user_id\"], inputs[\"movie_id\"]\n",
    "        user_embeddings = self.user_embedding(user_id)\n",
    "        candidate_embeddings = self.candidate_embedding(movie_id)\n",
    "\n",
    "        list_length = ops.shape(movie_id)[-1]\n",
    "        user_embeddings_repeated = ops.repeat(\n",
    "            ops.expand_dims(user_embeddings, axis=1),\n",
    "            repeats=list_length,\n",
    "            axis=1,\n",
    "        )\n",
    "        concatenated_embeddings = ops.concatenate(\n",
    "            [user_embeddings_repeated, candidate_embeddings], axis=-1\n",
    "        )\n",
    "\n",
    "        scores = self.ratings(concatenated_embeddings)\n",
    "        scores = ops.squeeze(scores, axis=-1)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's instantiate, compile and train our model. We will train two models:\n",
    "one with vanilla mean-squared error, and the other with pairwise hinge loss.\n",
    "For the latter, we will use `keras_rs.losses.PairwiseHingeLoss`.\n",
    "\n",
    "Pairwise losses compare pairs of items within each list, penalizing cases where\n",
    "an item with a higher true label has a lower predicted score than an item with a\n",
    "lower true label. This is why they are more suited for ranking tasks than\n",
    "pointwise losses.\n",
    "\n",
    "To quantify these results, we compute nDCG. nDCG is a measure of ranking quality\n",
    "that evaluates how well a system orders items based on relevance, giving more\n",
    "importance to highly relevant items appearing at the top of the list and\n",
    "normalizing the score against an ideal ranking.\n",
    "To compute it, we just need to pass `keras_rs.metrics.NDCG()` as a metric to\n",
    "`model.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model_mse = RankingModel(\n",
    "    num_users=users_count + 1,\n",
    "    num_candidates=movies_count + 1,\n",
    "    embedding_dimension=EMBEDDING_DIM,\n",
    ")\n",
    "model_mse.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras_rs.metrics.NDCG(k=NUM_EXAMPLES_PER_LIST, name=\"ndcg\")],\n",
    "    optimizer=keras.optimizers.Adagrad(learning_rate=LEARNING_RATE),\n",
    ")\n",
    "model_mse.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And now, the model with pairwise hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model_hinge = RankingModel(\n",
    "    num_users=users_count + 1,\n",
    "    num_candidates=movies_count + 1,\n",
    "    embedding_dimension=EMBEDDING_DIM,\n",
    ")\n",
    "model_hinge.compile(\n",
    "    loss=keras_rs.losses.PairwiseHingeLoss(),\n",
    "    metrics=[keras_rs.metrics.NDCG(k=NUM_EXAMPLES_PER_LIST, name=\"ndcg\")],\n",
    "    optimizer=keras.optimizers.Adagrad(learning_rate=LEARNING_RATE),\n",
    ")\n",
    "model_hinge.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Comparing the validation nDCG values, it is clear that the model trained with\n",
    "the pairwise hinge loss outperforms the other one. Let's make this observation\n",
    "more concrete by comparing results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ndcg_mse = model_mse.evaluate(test_ds, return_dict=True)[\"ndcg\"]\n",
    "ndcg_hinge = model_hinge.evaluate(test_ds, return_dict=True)[\"ndcg\"]\n",
    "print(ndcg_mse, ndcg_hinge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Now, let's rank some lists!\n",
    "\n",
    "Let's create a mapping from movie ID to title so that we can surface the titles\n",
    "for the ranked list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "movie_id_to_movie_title = {\n",
    "    int(x[\"movie_id\"]): x[\"movie_title\"] for x in movies.as_numpy_iterator()\n",
    "}\n",
    "movie_id_to_movie_title[0] = \"\"  # Because id 0 is not in the dataset.\n",
    "\n",
    "user_id = 42\n",
    "movie_ids = [409, 237, 131, 941, 543]\n",
    "predictions = model_hinge.predict(\n",
    "    {\n",
    "        \"user_id\": keras.ops.array([user_id]),\n",
    "        \"movie_id\": keras.ops.array([movie_ids]),\n",
    "    }\n",
    ")\n",
    "predictions = keras.ops.convert_to_numpy(keras.ops.squeeze(predictions, axis=0))\n",
    "sorted_indices = np.argsort(predictions)\n",
    "sorted_movies = [movie_ids[i] for i in sorted_indices]\n",
    "\n",
    "for i, movie_id in enumerate(sorted_movies):\n",
    "    print(f\"{i + 1}. \", movie_id_to_movie_title[movie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And we're all done!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "listwise_ranking",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}