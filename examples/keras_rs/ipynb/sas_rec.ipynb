{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Sequential retrieval using SASRec\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Fabien Hertschuh](https://github.com/hertschuh/)<br>\n",
    "**Date created:** 2025/04/28<br>\n",
    "**Last modified:** 2025/04/28<br>\n",
    "**Description:** Recommend movies using a Transformer-based retrieval model (SASRec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Sequential recommendation is a popular model that looks at a sequence of items\n",
    "that users have interacted with previously and then predicts the next item.\n",
    "Here, the order of the items within each sequence matters. Previously, in the\n",
    "[Recommending movies: retrieval using a sequential model](/keras_rs/examples/sequential_retrieval/)\n",
    "example, we built a GRU-based sequential retrieval model. In this example, we\n",
    "will build a popular Transformer decoder-based model named\n",
    "[Self-Attentive Sequential Recommendation (SASRec)](https://arxiv.org/abs/1808.09781)\n",
    "for the same sequential recommendation task.\n",
    "\n",
    "Let's begin by importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # `\"tensorflow\"`/`\"torch\"`\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf  # Needed only for the dataset\n",
    "from keras import ops\n",
    "\n",
    "import keras_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's also define all important variables/hyperparameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./raw/data/\"\n",
    "\n",
    "# MovieLens-specific variables\n",
    "MOVIELENS_1M_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "MOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\n",
    "\n",
    "RATINGS_FILE_NAME = \"ratings.dat\"\n",
    "MOVIES_FILE_NAME = \"movies.dat\"\n",
    "\n",
    "# Data processing args\n",
    "MAX_CONTEXT_LENGTH = 200\n",
    "MIN_SEQUENCE_LENGTH = 3\n",
    "PAD_ITEM_ID = 0\n",
    "\n",
    "RATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\n",
    "MOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "MIN_RATING = 2\n",
    "\n",
    "# Training/model args picked from SASRec paper\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 1\n",
    "HIDDEN_DIM = 50\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Next, we need to prepare our dataset. Like we did in the\n",
    "[sequential retrieval](/keras_rs/examples/sequential_retrieval/)\n",
    "example, we are going to use the MovieLens dataset.\n",
    "\n",
    "The dataset preparation step is fairly involved. The original ratings dataset\n",
    "contains `(user, movie ID, rating, timestamp)` tuples (among other columns,\n",
    "which are not important for this example). Since we are dealing with sequential\n",
    "retrieval, we need to create movie sequences for every user, where the sequences\n",
    "are ordered by timestamp.\n",
    "\n",
    "Let's start by downloading and reading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Download the MovieLens dataset.\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "path_to_zip = keras.utils.get_file(\n",
    "    fname=\"ml-1m.zip\",\n",
    "    origin=MOVIELENS_1M_URL,\n",
    "    file_hash=MOVIELENS_ZIP_HASH,\n",
    "    hash_algorithm=\"sha256\",\n",
    "    extract=True,\n",
    "    cache_dir=DATA_DIR,\n",
    ")\n",
    "movielens_extracted_dir = os.path.join(\n",
    "    os.path.dirname(path_to_zip),\n",
    "    \"ml-1m_extracted\",\n",
    "    \"ml-1m\",\n",
    ")\n",
    "\n",
    "\n",
    "# Read the dataset.\n",
    "def read_data(data_directory, min_rating=None):\n",
    "    \"\"\"Read movielens ratings.dat and movies.dat file\n",
    "    into dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ratings_df = pd.read_csv(\n",
    "        os.path.join(data_directory, RATINGS_FILE_NAME),\n",
    "        sep=\"::\",\n",
    "        names=RATINGS_DATA_COLUMNS,\n",
    "        encoding=\"unicode_escape\",\n",
    "    )\n",
    "    ratings_df[\"Timestamp\"] = ratings_df[\"Timestamp\"].apply(int)\n",
    "\n",
    "    # Remove movies with `rating < min_rating`.\n",
    "    if min_rating is not None:\n",
    "        ratings_df = ratings_df[ratings_df[\"Rating\"] >= min_rating]\n",
    "\n",
    "    movies_df = pd.read_csv(\n",
    "        os.path.join(data_directory, MOVIES_FILE_NAME),\n",
    "        sep=\"::\",\n",
    "        names=MOVIES_DATA_COLUMNS,\n",
    "        encoding=\"unicode_escape\",\n",
    "    )\n",
    "    return ratings_df, movies_df\n",
    "\n",
    "\n",
    "ratings_df, movies_df = read_data(\n",
    "    data_directory=movielens_extracted_dir, min_rating=MIN_RATING\n",
    ")\n",
    "\n",
    "# Need to know #movies so as to define embedding layers.\n",
    "movies_count = movies_df[\"MovieID\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we have read the dataset, let's create sequences of movies\n",
    "for every user. Here is the function for doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_movie_sequence_per_user(ratings_df):\n",
    "    \"\"\"Get movieID sequences for every user.\"\"\"\n",
    "    sequences = collections.defaultdict(list)\n",
    "\n",
    "    for user_id, movie_id, rating, timestamp in ratings_df.values:\n",
    "        sequences[user_id].append(\n",
    "            {\n",
    "                \"movie_id\": movie_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"rating\": rating,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort movie sequences by timestamp for every user.\n",
    "    for user_id, context in sequences.items():\n",
    "        context.sort(key=lambda x: x[\"timestamp\"])\n",
    "        sequences[user_id] = context\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "sequences = get_movie_sequence_per_user(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "So far, we have essentially replicated what we did in the sequential retrieval\n",
    "example. We have a sequence of movies for every user.\n",
    "\n",
    "SASRec is trained contrastively, which means the model learns to distinguish\n",
    "between sequences of movies a user has actually interacted with (positive\n",
    "examples) and sequences they have not interacted with (negative examples).\n",
    "\n",
    "The following function, `format_data`, prepares the data in this specific\n",
    "format. For each user's movie sequence, it generates a corresponding\n",
    "\"negative sequence\". This negative sequence consists of randomly\n",
    "selected movies that the user has *not* interacted with, but are of the same\n",
    "length as the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def format_data(sequences):\n",
    "    examples = {\n",
    "        \"sequence\": [],\n",
    "        \"negative_sequence\": [],\n",
    "    }\n",
    "\n",
    "    for user_id in sequences:\n",
    "        sequence = [int(d[\"movie_id\"]) for d in sequences[user_id]]\n",
    "\n",
    "        # Get negative sequence.\n",
    "        def random_negative_item_id(low, high, positive_lst):\n",
    "            sampled = np.random.randint(low=low, high=high)\n",
    "            while sampled in positive_lst:\n",
    "                sampled = np.random.randint(low=low, high=high)\n",
    "            return sampled\n",
    "\n",
    "        negative_sequence = [\n",
    "            random_negative_item_id(1, movies_count + 1, sequence)\n",
    "            for _ in range(len(sequence))\n",
    "        ]\n",
    "\n",
    "        examples[\"sequence\"].append(np.array(sequence))\n",
    "        examples[\"negative_sequence\"].append(np.array(negative_sequence))\n",
    "\n",
    "    examples[\"sequence\"] = tf.ragged.constant(examples[\"sequence\"])\n",
    "    examples[\"negative_sequence\"] = tf.ragged.constant(examples[\"negative_sequence\"])\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "examples = format_data(sequences)\n",
    "ds = tf.data.Dataset.from_tensor_slices(examples).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we have the original movie interaction sequences for each user (from\n",
    "`format_data`, stored in `examples[\"sequence\"]`) and their corresponding\n",
    "random negative sequences (in `examples[\"negative_sequence\"]`), the next step is\n",
    "to prepare this data for input to the model. The primary goals of this\n",
    "preprocessing are:\n",
    "\n",
    "1.  Creating Input Features and Target Labels: For sequential\n",
    "    recommendation, the model learns to predict the next item in a sequence\n",
    "    given the preceding items. This is achieved by:\n",
    "    - taking the original `example[\"sequence\"]` and creating the model's\n",
    "      input features (`item_ids`) from all items *except the last one*\n",
    "      (`example[\"sequence\"][..., :-1]`);\n",
    "    - creating the target \"positive sequence\" (what the model tries to predict\n",
    "      as the actual next items) by taking the original `example[\"sequence\"]`\n",
    "      and shifting it, using all items *except the first one*\n",
    "      (`example[\"sequence\"][..., 1:]`);\n",
    "    - shifting `example[\"negative_sequence\"]` (from `format_data`) is\n",
    "      to create the target \"negative sequence\" for the contrastive loss\n",
    "      (`example[\"negative_sequence\"][..., 1:]`).\n",
    "\n",
    "2.  Handling Variable Length Sequences: Neural networks typically require\n",
    "    fixed-size inputs. Therefore, both the input feature sequences and the\n",
    "    target sequences are padded (with a special `PAD_ITEM_ID`) or truncated\n",
    "    to a predefined `MAX_CONTEXT_LENGTH`. A `padding_mask` is also generated\n",
    "    from the input features to ensure the model ignores these padded tokens\n",
    "    during attention calculations, i.e, these tokens will be masked.\n",
    "\n",
    "3.  Differentiating Training and Validation/Testing:\n",
    "    - During training:\n",
    "      - Input features (`item_ids`) and context for negative sequences\n",
    "        are prepared as described above (all but the last item of the\n",
    "        original sequences).\n",
    "      - Target positive and negative sequences are the shifted versions of\n",
    "        the original sequences.\n",
    "        - `sample_weight` is created based on the input features to ensure\n",
    "          that loss is calculated only on actual items, not on padding tokens\n",
    "          in the targets.\n",
    "    - During validation/testing:\n",
    "      - Input features are prepared similarly.\n",
    "      - The model's performance is typically evaluated on its ability to\n",
    "        predict the actual last item of the original sequence. Thus,\n",
    "        `sample_weight` is configured to focus the loss calculation\n",
    "        only on this final prediction in the target sequences.\n",
    "\n",
    "Note: SASRec does the same thing we've done above, except that they take the\n",
    "`item_ids[:-2]` for the validation set and `item_ids[:-1]` for the test set.\n",
    "We skip that here for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _preprocess(example, train=False):\n",
    "    sequence = example[\"sequence\"]\n",
    "    negative_sequence = example[\"negative_sequence\"]\n",
    "\n",
    "    if train:\n",
    "        sequence = example[\"sequence\"][..., :-1]\n",
    "        negative_sequence = example[\"negative_sequence\"][..., :-1]\n",
    "\n",
    "    batch_size = tf.shape(sequence)[0]\n",
    "\n",
    "    if not train:\n",
    "        # Loss computed only on last token.\n",
    "        sample_weight = tf.zeros_like(sequence, dtype=\"float32\")[..., :-1]\n",
    "        sample_weight = tf.concat(\n",
    "            [sample_weight, tf.ones((batch_size, 1), dtype=\"float32\")], axis=1\n",
    "        )\n",
    "\n",
    "    # Truncate/pad sequence. +1 to account for truncation later.\n",
    "    sequence = sequence.to_tensor(\n",
    "        shape=[batch_size, MAX_CONTEXT_LENGTH + 1], default_value=PAD_ITEM_ID\n",
    "    )\n",
    "    negative_sequence = negative_sequence.to_tensor(\n",
    "        shape=[batch_size, MAX_CONTEXT_LENGTH + 1], default_value=PAD_ITEM_ID\n",
    "    )\n",
    "    if train:\n",
    "        sample_weight = tf.cast(sequence != PAD_ITEM_ID, dtype=\"float32\")\n",
    "    else:\n",
    "        sample_weight = sample_weight.to_tensor(\n",
    "            shape=[batch_size, MAX_CONTEXT_LENGTH + 1], default_value=0\n",
    "        )\n",
    "\n",
    "    example = (\n",
    "        {\n",
    "            # last token does not have a next token\n",
    "            \"item_ids\": sequence[..., :-1],\n",
    "            # padding mask for controlling attention mask\n",
    "            \"padding_mask\": (sequence != PAD_ITEM_ID)[..., :-1],\n",
    "        },\n",
    "        {\n",
    "            \"positive_sequence\": sequence[\n",
    "                ..., 1:\n",
    "            ],  # 0th token's label will be 1st token, and so on\n",
    "            \"negative_sequence\": negative_sequence[..., 1:],\n",
    "        },\n",
    "        sample_weight[..., 1:],  # loss will not be computed on pad tokens\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    return _preprocess(examples, train=True)\n",
    "\n",
    "\n",
    "def preprocess_val(examples):\n",
    "    return _preprocess(examples, train=False)\n",
    "\n",
    "\n",
    "train_ds = ds.map(preprocess_train)\n",
    "val_ds = ds.map(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can see a batch for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    print(batch)\n",
    "\n",
    "for batch in val_ds.take(1):\n",
    "    print(batch)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model\n",
    "\n",
    "To encode the input sequence, we use a Transformer decoder-based model. This\n",
    "part of the model is very similar to the GPT-2 architecture. Refer to the\n",
    "[GPT text generation from scratch with KerasHub](/examples/generative/text_generation_gpt/#build-the-model)\n",
    "guide for more details on this part.\n",
    "\n",
    "One part to note is that when we are \"predicting\", i.e., `training` is `False`,\n",
    "we get the embedding corresponding to the last movie in the sequence. This makes\n",
    "sense, because at inference time, we want to predict the movie the user will\n",
    "likely watch after watching the last movie.\n",
    "\n",
    "Also, it's worth discussing the `compute_loss` method. We embed the positive\n",
    "and negative sequences using the input embedding matrix. We compute the\n",
    "similarity of (positive sequence, input sequence) and (negative sequence,\n",
    "input sequence) pair embeddings by computing the dot product. The goal now is\n",
    "to maximize the similarity of the former and minimize the similarity of\n",
    "the latter. Let's see this mathematically. Binary Cross Entropy is written\n",
    "as follows:\n",
    "\n",
    "```\n",
    " loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "```\n",
    "\n",
    "Here, we assign the positive pairs a label of 1 and the negative pairs a label\n",
    "of 0. So, for a positive pair, the loss reduces to:\n",
    "\n",
    "```\n",
    "loss = -np.log(positive_logits)\n",
    "```\n",
    "\n",
    "Minimising the loss means we want to maximize the log term, which in turn,\n",
    "implies maximising `positive_logits`. Similarly, we want to minimize\n",
    "`negative_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SasRec(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        hidden_dim,\n",
    "        dropout=0.0,\n",
    "        max_sequence_length=100,\n",
    "        dtype=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "\n",
    "        # ======== Layers ========\n",
    "\n",
    "        # === Embeddings ===\n",
    "        self.item_embedding = keras_hub.layers.ReversibleEmbedding(\n",
    "            input_dim=vocabulary_size,\n",
    "            output_dim=hidden_dim,\n",
    "            embeddings_initializer=\"glorot_uniform\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(0.001),\n",
    "            dtype=dtype,\n",
    "            name=\"item_embedding\",\n",
    "        )\n",
    "        self.position_embedding = keras_hub.layers.PositionEmbedding(\n",
    "            initializer=\"glorot_uniform\",\n",
    "            sequence_length=max_sequence_length,\n",
    "            dtype=dtype,\n",
    "            name=\"position_embedding\",\n",
    "        )\n",
    "        self.embeddings_add = keras.layers.Add(\n",
    "            dtype=dtype,\n",
    "            name=\"embeddings_add\",\n",
    "        )\n",
    "        self.embeddings_dropout = keras.layers.Dropout(\n",
    "            dropout,\n",
    "            dtype=dtype,\n",
    "            name=\"embeddings_dropout\",\n",
    "        )\n",
    "\n",
    "        # === Decoder layers ===\n",
    "        self.transformer_layers = []\n",
    "        for i in range(num_layers):\n",
    "            self.transformer_layers.append(\n",
    "                keras_hub.layers.TransformerDecoder(\n",
    "                    intermediate_dim=hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    layer_norm_epsilon=1e-05,\n",
    "                    # SASRec uses ReLU, although GeLU might be a better option\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"glorot_uniform\",\n",
    "                    normalize_first=True,\n",
    "                    dtype=dtype,\n",
    "                    name=f\"transformer_layer_{i}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # === Final layer norm ===\n",
    "        self.layer_norm = keras.layers.LayerNormalization(\n",
    "            axis=-1,\n",
    "            epsilon=1e-8,\n",
    "            dtype=dtype,\n",
    "            name=\"layer_norm\",\n",
    "        )\n",
    "\n",
    "        # === Retrieval ===\n",
    "        # The layer that performs the retrieval.\n",
    "        self.retrieval = keras_rs.layers.BruteForceRetrieval(k=10, return_scores=False)\n",
    "\n",
    "        # === Loss ===\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True, reduction=None)\n",
    "\n",
    "        # === Attributes ===\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def _get_last_non_padding_token(self, tensor, padding_mask):\n",
    "        valid_token_mask = ops.logical_not(padding_mask)\n",
    "        seq_lengths = ops.sum(ops.cast(valid_token_mask, \"int32\"), axis=1)\n",
    "        last_token_indices = ops.maximum(seq_lengths - 1, 0)\n",
    "\n",
    "        indices = ops.expand_dims(last_token_indices, axis=(-2, -1))\n",
    "        gathered_tokens = ops.take_along_axis(tensor, indices, axis=1)\n",
    "        last_token_embedding = ops.squeeze(gathered_tokens, axis=1)\n",
    "\n",
    "        return last_token_embedding\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_shape = list(input_shape) + [self.hidden_dim]\n",
    "\n",
    "        # Model\n",
    "        self.item_embedding.build(input_shape)\n",
    "        self.position_embedding.build(embedding_shape)\n",
    "\n",
    "        self.embeddings_add.build((embedding_shape, embedding_shape))\n",
    "        self.embeddings_dropout.build(embedding_shape)\n",
    "\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            transformer_layer.build(decoder_sequence_shape=embedding_shape)\n",
    "\n",
    "        self.layer_norm.build(embedding_shape)\n",
    "\n",
    "        # Retrieval\n",
    "        self.retrieval.candidate_embeddings = self.item_embedding.embeddings\n",
    "        self.retrieval.build(input_shape)\n",
    "\n",
    "        # Chain to super\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        item_ids, padding_mask = inputs[\"item_ids\"], inputs[\"padding_mask\"]\n",
    "\n",
    "        x = self.item_embedding(item_ids)\n",
    "        position_embedding = self.position_embedding(x)\n",
    "        x = self.embeddings_add((x, position_embedding))\n",
    "        x = self.embeddings_dropout(x)\n",
    "\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x, decoder_padding_mask=padding_mask)\n",
    "\n",
    "        item_sequence_embedding = self.layer_norm(x)\n",
    "        result = {\"item_sequence_embedding\": item_sequence_embedding}\n",
    "\n",
    "        # At inference, perform top-k retrieval.\n",
    "        if not training:\n",
    "            # need to extract last non-padding token.\n",
    "            last_item_embedding = self._get_last_non_padding_token(\n",
    "                item_sequence_embedding, padding_mask\n",
    "            )\n",
    "            result[\"predictions\"] = self.retrieval(last_item_embedding)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_loss(self, x, y, y_pred, sample_weight, training=False):\n",
    "        item_sequence_embedding = y_pred[\"item_sequence_embedding\"]\n",
    "        y_positive_sequence = y[\"positive_sequence\"]\n",
    "        y_negative_sequence = y[\"negative_sequence\"]\n",
    "\n",
    "        # Embed positive, negative sequences.\n",
    "        positive_sequence_embedding = self.item_embedding(y_positive_sequence)\n",
    "        negative_sequence_embedding = self.item_embedding(y_negative_sequence)\n",
    "\n",
    "        # Logits\n",
    "        positive_logits = ops.sum(\n",
    "            ops.multiply(positive_sequence_embedding, item_sequence_embedding),\n",
    "            axis=-1,\n",
    "        )\n",
    "        negative_logits = ops.sum(\n",
    "            ops.multiply(negative_sequence_embedding, item_sequence_embedding),\n",
    "            axis=-1,\n",
    "        )\n",
    "        logits = ops.concatenate([positive_logits, negative_logits], axis=1)\n",
    "\n",
    "        # Labels\n",
    "        labels = ops.concatenate(\n",
    "            [\n",
    "                ops.ones_like(positive_logits),\n",
    "                ops.zeros_like(negative_logits),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # sample weights\n",
    "        sample_weight = ops.concatenate(\n",
    "            [sample_weight, sample_weight],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        loss = self.loss_fn(\n",
    "            y_true=ops.expand_dims(labels, axis=-1),\n",
    "            y_pred=ops.expand_dims(logits, axis=-1),\n",
    "            sample_weight=sample_weight,\n",
    "        )\n",
    "        loss = ops.divide_no_nan(ops.sum(loss), ops.sum(sample_weight))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_output_shape(self, inputs_shape):\n",
    "        return list(inputs_shape) + [self.hidden_dim]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's instantiate our model and do some sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = SasRec(\n",
    "    vocabulary_size=movies_count + 1,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    max_sequence_length=MAX_CONTEXT_LENGTH,\n",
    ")\n",
    "\n",
    "# Training\n",
    "output = model(\n",
    "    inputs={\n",
    "        \"item_ids\": ops.ones((2, MAX_CONTEXT_LENGTH), dtype=\"int32\"),\n",
    "        \"padding_mask\": ops.ones((2, MAX_CONTEXT_LENGTH), dtype=\"bool\"),\n",
    "    },\n",
    "    training=True,\n",
    ")\n",
    "print(output[\"item_sequence_embedding\"].shape)\n",
    "\n",
    "# Inference\n",
    "output = model(\n",
    "    inputs={\n",
    "        \"item_ids\": ops.ones((2, MAX_CONTEXT_LENGTH), dtype=\"int32\"),\n",
    "        \"padding_mask\": ops.ones((2, MAX_CONTEXT_LENGTH), dtype=\"bool\"),\n",
    "    },\n",
    "    training=False,\n",
    ")\n",
    "print(output[\"predictions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's compile and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_2=0.98),\n",
    ")\n",
    "model.fit(\n",
    "    x=train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, we would like to be able to make predictions.\n",
    "\n",
    "So far, we have only handled movies by id. Now is the time to create a mapping\n",
    "keyed by movie IDs to be able to surface the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "movie_id_to_movie_title = dict(zip(movies_df[\"MovieID\"], movies_df[\"Title\"]))\n",
    "movie_id_to_movie_title[0] = \"\"  # Because id 0 is not in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We then simply use the Keras `model.predict()` method. Under the hood, it calls\n",
    "the `BruteForceRetrieval` layer to perform the actual retrieval.\n",
    "\n",
    "Note that this model can retrieve movies already watched by the user. We could\n",
    "easily add logic to remove them if that is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for ele in val_ds.unbatch().take(1):\n",
    "    test_sample = ele[0]\n",
    "    test_sample[\"item_ids\"] = tf.expand_dims(test_sample[\"item_ids\"], axis=0)\n",
    "    test_sample[\"padding_mask\"] = tf.expand_dims(test_sample[\"padding_mask\"], axis=0)\n",
    "\n",
    "movie_sequence = np.array(test_sample[\"item_ids\"])[0]\n",
    "for movie_id in movie_sequence:\n",
    "    if movie_id == 0:\n",
    "        continue\n",
    "    print(movie_id_to_movie_title[movie_id], end=\"; \")\n",
    "print()\n",
    "\n",
    "predictions = model.predict(test_sample)[\"predictions\"]\n",
    "predictions = keras.ops.convert_to_numpy(predictions)\n",
    "\n",
    "for movie_id in predictions[0]:\n",
    "    print(movie_id_to_movie_title[movie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And that's all!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sas_rec",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}