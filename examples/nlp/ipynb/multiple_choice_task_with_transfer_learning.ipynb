{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# MultipleChoice Task with Transfer Learning\n",
    "\n",
    "**Author:** Md Awsafur Rahman<br>\n",
    "**Date created:** 2023/09/14<br>\n",
    "**Last modified:** 2023/09/14<br>\n",
    "**Description:** Use pre-trained nlp models for multiplechoice task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will demonstrate how to perform the **MultipleChoice** task by\n",
    "finetuning pre-trained DebertaV3 model. In this task, several candidate answers are\n",
    "provided along with a context and the model is trained to select the correct answer\n",
    "unlike question answering. We will use SWAG dataset to demonstrate this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "import keras\n",
    "import tensorflow as tf  # For tf.data only.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset\n",
    "In this example we'll use **SWAG** dataset for multiplechoice task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/rowanz/swagaf/archive/refs/heads/master.zip\" -O swag.zip\n",
    "!unzip -q swag.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!ls swagaf-master/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    preset = \"deberta_v3_extra_small_en\"  # Name of pretrained models\n",
    "    sequence_length = 200  # Input sequence length\n",
    "    seed = 42  # Random seed\n",
    "    epochs = 5  # Training epochs\n",
    "    batch_size = 8  # Batch size\n",
    "    augment = True  # Augmentation (Shuffle Options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Reproducibility\n",
    "Sets value for random seed to produce similar result in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Meta Data\n",
    "* **train.csv** - will be used for training.\n",
    "* `sent1` and `sent2`: these fields show how a sentence starts, and if you put the two\n",
    "together, you get the `startphrase` field.\n",
    "* `ending_<i>`: suggests a possible ending for how a sentence can end, but only one of\n",
    "them is correct.\n",
    "    * `label`: identifies the correct sentence ending.\n",
    "\n",
    "* **val.csv** - similar to `train.csv` but will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_df = pd.read_csv(\n",
    "    \"swagaf-master/data/train.csv\", index_col=0\n",
    ")  # Read CSV file into a DataFrame\n",
    "train_df = train_df.sample(frac=0.02)\n",
    "print(\"# Train Data: {:,}\".format(len(train_df)))\n",
    "\n",
    "# Valid data\n",
    "valid_df = pd.read_csv(\n",
    "    \"swagaf-master/data/val.csv\", index_col=0\n",
    ")  # Read CSV file into a DataFrame\n",
    "valid_df = valid_df.sample(frac=0.02)\n",
    "print(\"# Valid Data: {:,}\".format(len(valid_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Contextualize Options\n",
    "\n",
    "Our approach entails furnishing the model with question and answer pairs, as opposed to\n",
    "employing a single question for all five options. In practice, this signifies that for\n",
    "the five options, we will supply the model with the same set of five questions combined\n",
    "with each respective answer choice (e.g., `(Q + A)`, `(Q + B)`, and so on). This analogy\n",
    "draws parallels to the practice of revisiting a question multiple times during an exam to\n",
    "promote a deeper understanding of the problem at hand.\n",
    "\n",
    "> Notably, in the context of SWAG dataset, question is the start of a sentence and\n",
    "options are possible ending of that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to create options based on the prompt and choices\n",
    "def make_options(row):\n",
    "    row[\"options\"] = [\n",
    "        f\"{row.startphrase}\\n{row.ending0}\",  # Option 0\n",
    "        f\"{row.startphrase}\\n{row.ending1}\",  # Option 1\n",
    "        f\"{row.startphrase}\\n{row.ending2}\",  # Option 2\n",
    "        f\"{row.startphrase}\\n{row.ending3}\",\n",
    "    ]  # Option 3\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Apply the `make_options` function to each row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.apply(make_options, axis=1)\n",
    "valid_df = valid_df.apply(make_options, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "**What it does:** The preprocessor takes input strings and transforms them into a\n",
    "dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process\n",
    "starts with tokenization, where input strings are converted into sequences of token IDs.\n",
    "\n",
    "**Why it's important:** Initially, raw text data is complex and challenging for modeling\n",
    "due to its high dimensionality. By converting text into a compact set of tokens, such as\n",
    "transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`,\n",
    "we simplify the data. Many models rely on special tokens and additional tensors to\n",
    "understand input. These tokens help divide input and identify padding, among other tasks.\n",
    "Making all sequences the same length through padding boosts computational efficiency,\n",
    "making subsequent steps smoother.\n",
    "\n",
    "Explore the following pages to access the available preprocessing and tokenizer layers in\n",
    "**KerasHub**:\n",
    "- [Preprocessing](https://keras.io/api/keras_hub/preprocessing_layers/)\n",
    "- [Tokenizers](https://keras.io/api/keras_hub/tokenizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "preprocessor = keras_hub.models.DebertaV3Preprocessor.from_preset(\n",
    "    preset=CFG.preset,  # Name of the model\n",
    "    sequence_length=CFG.sequence_length,  # Max sequence length, will be padded if shorter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's examine what the output shape of the preprocessing layer looks like. The\n",
    "output shape of the layer can be represented as $(num\\_choices, sequence\\_length)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "outs = preprocessor(train_df.options.iloc[0])  # Process options for the first row\n",
    "\n",
    "# Display the shape of each processed output\n",
    "for k, v in outs.items():\n",
    "    print(k, \":\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We'll use the `preprocessing_fn` function to transform each text option using the\n",
    "`dataset.map(preprocessing_fn)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_fn(text, label=None):\n",
    "    text = preprocessor(text)  # Preprocess text\n",
    "    return (\n",
    "        (text, label) if label is not None else text\n",
    "    )  # Return processed text and label if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Augmentation\n",
    "\n",
    "In this notebook, we'll experiment with an interesting augmentation technique,\n",
    "`option_shuffle`. Since we're providing the model with one option at a time, we can\n",
    "introduce a shuffle to the order of options. For instance, options `[A, C, E, D, B]`\n",
    "would be rearranged as `[D, B, A, E, C]`. This practice will help the model focus on the\n",
    "content of the options themselves, rather than being influenced by their positions.\n",
    "\n",
    "**Note:** Even though `option_shuffle` function is written in pure\n",
    "tensorflow, it can be used with any backend (e.g. JAX, PyTorch) as it is only used\n",
    "in `tf.data.Dataset` pipeline which is compatible with Keras 3 routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def option_shuffle(options, labels, prob=0.50, seed=None):\n",
    "    if tf.random.uniform([]) > prob:  # Shuffle probability check\n",
    "        return options, labels\n",
    "    # Shuffle indices of options and labels in the same order\n",
    "    indices = tf.random.shuffle(tf.range(tf.shape(options)[0]), seed=seed)\n",
    "    # Shuffle options and labels\n",
    "    options = tf.gather(options, indices)\n",
    "    labels = tf.gather(labels, indices)\n",
    "    return options, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the following function, we'll merge all augmentation functions to apply to the text.\n",
    "These augmentations will be applied to the data using the `dataset.map(augment_fn)`\n",
    "approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def augment_fn(text, label=None):\n",
    "    text, label = option_shuffle(text, label, prob=0.5)  # Shuffle the options\n",
    "    return (text, label) if label is not None else text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## DataLoader\n",
    "\n",
    "The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data\n",
    "processing. Notable aspects of `tf.data` include its ability to simplify pipeline\n",
    "construction and represent components in sequences.\n",
    "\n",
    "To learn more about `tf.data`, refer to this\n",
    "[documentation](https://www.tensorflow.org/guide/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(\n",
    "    texts,\n",
    "    labels=None,\n",
    "    batch_size=32,\n",
    "    cache=False,\n",
    "    augment=False,\n",
    "    repeat=False,\n",
    "    shuffle=1024,\n",
    "):\n",
    "    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n",
    "    slices = (\n",
    "        (texts,)\n",
    "        if labels is None\n",
    "        else (texts, keras.utils.to_categorical(labels, num_classes=4))\n",
    "    )  # Create slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n",
    "    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n",
    "    if augment:  # Apply augmentation if enabled\n",
    "        ds = ds.map(augment_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n",
    "    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n",
    "    opt = tf.data.Options()  # Create dataset options\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n",
    "        opt.experimental_deterministic = False\n",
    "    ds = ds.with_options(opt)  # Set dataset options\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)  # Batch dataset\n",
    "    ds = ds.prefetch(AUTO)  # Prefetch next batch\n",
    "    return ds  # Return the built dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now let's create train and valid dataloader using above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Build train dataloader\n",
    "train_texts = train_df.options.tolist()  # Extract training texts\n",
    "train_labels = train_df.label.tolist()  # Extract training labels\n",
    "train_ds = build_dataset(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    batch_size=CFG.batch_size,\n",
    "    cache=True,\n",
    "    shuffle=True,\n",
    "    repeat=True,\n",
    "    augment=CFG.augment,\n",
    ")\n",
    "\n",
    "# Build valid dataloader\n",
    "valid_texts = valid_df.options.tolist()  # Extract validation texts\n",
    "valid_labels = valid_df.label.tolist()  # Extract validation labels\n",
    "valid_ds = build_dataset(\n",
    "    valid_texts,\n",
    "    valid_labels,\n",
    "    batch_size=CFG.batch_size,\n",
    "    cache=True,\n",
    "    shuffle=False,\n",
    "    repeat=False,\n",
    "    augment=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## LR Schedule\n",
    "\n",
    "Implementing a learning rate scheduler is crucial for transfer learning. The learning\n",
    "rate initiates at `lr_start` and gradually tapers down to `lr_min` using **cosine**\n",
    "curve.\n",
    "\n",
    "**Importance:** A well-structured learning rate schedule is essential for efficient model\n",
    "training, ensuring optimal convergence and avoiding issues such as overshooting or\n",
    "stagnation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_lr_callback(batch_size=8, mode=\"cos\", epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n",
    "    lr_ramp_ep, lr_sus_ep = 2, 0\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            decay_total_epochs, decay_epoch_index = (\n",
    "                epochs - lr_ramp_ep - lr_sus_ep + 3,\n",
    "                epoch - lr_ramp_ep - lr_sus_ep,\n",
    "            )\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(\n",
    "            np.arange(epochs),\n",
    "            [lrfn(epoch) for epoch in np.arange(epochs)],\n",
    "            marker=\"o\",\n",
    "        )\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"lr\")\n",
    "        plt.title(\"LR Scheduler\")\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(\n",
    "        lrfn, verbose=False\n",
    "    )  # Create lr callback\n",
    "\n",
    "\n",
    "_ = get_lr_callback(CFG.batch_size, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Callbacks\n",
    "\n",
    "The function below will gather all the training callbacks, such as `lr_scheduler`,\n",
    "`model_checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_callbacks():\n",
    "    callbacks = []\n",
    "    lr_cb = get_lr_callback(CFG.batch_size)  # Get lr callback\n",
    "    ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "        f\"best.keras\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"max\",\n",
    "    )  # Get Model checkpoint callback\n",
    "    callbacks.extend([lr_cb, ckpt_cb])  # Add lr and checkpoint callbacks\n",
    "    return callbacks  # Return the list of callbacks\n",
    "\n",
    "\n",
    "callbacks = get_callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## MultipleChoice Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Pre-trained Models\n",
    "\n",
    "The `KerasHub` library provides comprehensive, ready-to-use implementations of popular\n",
    "NLP model architectures. It features a variety of pre-trained models including `Bert`,\n",
    "`Roberta`, `DebertaV3`, and more. In this notebook, we'll showcase the usage of\n",
    "`DistillBert`. However, feel free to explore all available models in the [KerasHub\n",
    "documentation](https://keras.io/api/keras_hub/models/). Also for a deeper understanding\n",
    "of `KerasHub`, refer to the informative [getting started\n",
    "guide](https://keras.io/guides/keras_hub/getting_started/).\n",
    "\n",
    "Our approach involves using `keras_hub.models.XXClassifier` to process each question and\n",
    "option pari (e.g. (Q+A), (Q+B), etc.), generating logits. These logits are then combined\n",
    "and passed through a softmax function to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Classifier for Multiple-Choice Tasks\n",
    "\n",
    "When dealing with multiple-choice questions, instead of giving the model the question and\n",
    "all options together `(Q + A + B + C ...)`, we provide the model with one option at a\n",
    "time along with the question. For instance, `(Q + A)`, `(Q + B)`, and so on. Once we have\n",
    "the prediction scores (logits) for all options, we combine them using the `Softmax`\n",
    "function to get the ultimate result. If we had given all options at once to the model,\n",
    "the text's length would increase, making it harder for the model to handle. The picture\n",
    "below illustrates this idea:\n",
    "\n",
    "![Model Diagram](https://pbs.twimg.com/media/F3NUju_a8AAS8Fq?format=png&name=large)\n",
    "\n",
    "<div align=\"center\"><b> Picture Credict: </b> <a\n",
    "href=\"https://twitter.com/johnowhitaker\"> @johnowhitaker </a> </div></div><br>\n",
    "\n",
    "From a coding perspective, remember that we use the same model for all five options, with\n",
    "shared weights. Despite the figure suggesting five separate models, they are, in fact,\n",
    "one model with shared weights. Another point to consider is the the input shapes of\n",
    "Classifier and MultipleChoice.\n",
    "\n",
    "* Input shape for **Multiple Choice**: $(batch\\_size, num\\_choices, seq\\_length)$\n",
    "* Input shape for **Classifier**: $(batch\\_size, seq\\_length)$\n",
    "\n",
    "Certainly, it's clear that we can't directly give the data for the multiple-choice task\n",
    "to the model because the input shapes don't match. To handle this, we'll use **slicing**.\n",
    "This means we'll separate the features of each option, like $feature_{(Q + A)}$ and\n",
    "$feature_{(Q + B)}$, and give them one by one to the NLP classifier. After we get the\n",
    "prediction scores $logits_{(Q + A)}$ and $logits_{(Q + B)}$ for all the options, we'll\n",
    "use the Softmax function, like $\\operatorname{Softmax}([logits_{(Q + A)}, logits_{(Q +\n",
    "B)}])$, to combine them. This final step helps us make the ultimate decision or choice.\n",
    "\n",
    "> Note that in the classifier, we set `num_classes=1` instead of `5`. This is because the\n",
    "classifier produces a single output for each option. When dealing with five options,\n",
    "these individual outputs are joined together and then processed through a softmax\n",
    "function to generate the final result, which has a dimension of `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Selects one option from five\n",
    "class SelectOption(keras.layers.Layer):\n",
    "    def __init__(self, index, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.index = index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Selects a specific slice from the inputs tensor\n",
    "        return inputs[:, self.index, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        # For serialize the model\n",
    "        base_config = super().get_config()\n",
    "        config = {\n",
    "            \"index\": self.index,\n",
    "        }\n",
    "        return {**base_config, **config}\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Define input layers\n",
    "    inputs = {\n",
    "        \"token_ids\": keras.Input(shape=(4, None), dtype=\"int32\", name=\"token_ids\"),\n",
    "        \"padding_mask\": keras.Input(\n",
    "            shape=(4, None), dtype=\"int32\", name=\"padding_mask\"\n",
    "        ),\n",
    "    }\n",
    "    # Create a DebertaV3Classifier model\n",
    "    classifier = keras_hub.models.DebertaV3Classifier.from_preset(\n",
    "        CFG.preset,\n",
    "        preprocessor=None,\n",
    "        num_classes=1,  # one output per one option, for five options total 5 outputs\n",
    "    )\n",
    "    logits = []\n",
    "    # Loop through each option (Q+A), (Q+B) etc and compute associated logits\n",
    "    for option_idx in range(4):\n",
    "        option = {\n",
    "            k: SelectOption(option_idx, name=f\"{k}_{option_idx}\")(v)\n",
    "            for k, v in inputs.items()\n",
    "        }\n",
    "        logit = classifier(option)\n",
    "        logits.append(logit)\n",
    "\n",
    "    # Compute final output\n",
    "    logits = keras.layers.Concatenate(axis=-1)(logits)\n",
    "    outputs = keras.layers.Softmax(axis=-1)(logits)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with optimizer, loss, and metrics\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(5e-6),\n",
    "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        ],\n",
    "        jit_compile=True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the Build\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's checkout the model summary to have a better insight on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, let's check the model structure visually if everything is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=CFG.epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=int(len(train_df) / CFG.batch_size),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Make predictions using the trained model on last validation data\n",
    "predictions = model.predict(\n",
    "    valid_ds,\n",
    "    batch_size=CFG.batch_size,  # max batch size = valid size\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Format predictions and true answers\n",
    "pred_answers = np.arange(4)[np.argsort(-predictions)][:, 0]\n",
    "true_answers = valid_df.label.values\n",
    "\n",
    "# Check 5 Predictions\n",
    "print(\"# Predictions\\n\")\n",
    "for i in range(0, 50, 10):\n",
    "    row = valid_df.iloc[i]\n",
    "    question = row.startphrase\n",
    "    pred_answer = f\"ending{pred_answers[i]}\"\n",
    "    true_answer = f\"ending{true_answers[i]}\"\n",
    "    print(f\"❓ Sentence {i+1}:\\n{question}\\n\")\n",
    "    print(f\"✅ True Ending: {true_answer}\\n   >> {row[true_answer]}\\n\")\n",
    "    print(f\"🤖 Predicted Ending: {pred_answer}\\n   >> {row[pred_answer]}\\n\")\n",
    "    print(\"-\" * 90, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Reference\n",
    "* [Multiple Choice with\n",
    "HF](https://twitter.com/johnowhitaker/status/1689790373454041089?s=20)\n",
    "* [Keras NLP](https://keras.io/api/keras_hub/)\n",
    "* [BirdCLEF23: Pretraining is All you Need\n",
    "[Train]](https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train)\n",
    "[Train]](https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train)\n",
    "* [Triple Stratified KFold with\n",
    "TFRecords](https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multiple_choice_task_with_transfer_learning",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
