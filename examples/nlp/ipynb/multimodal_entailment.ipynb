{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Multimodal entailment\n",
    "\n",
    "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
    "**Date created:** 2021/08/08<br>\n",
    "**Last modified:** 2025/01/03<br>\n",
    "**Description:** Training a multimodal model for predicting entailment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will build and train a model for predicting multimodal entailment. We will be\n",
    "using the\n",
    "[multimodal entailment dataset](https://github.com/google-research-datasets/recognizing-multimodal-entailment)\n",
    "recently introduced by Google Research.\n",
    "\n",
    "### What is multimodal entailment?\n",
    "\n",
    "On social media platforms, to audit and moderate content\n",
    "we may want to find answers to the\n",
    "following questions in near real-time:\n",
    "\n",
    "* Does a given piece of information contradict the other?\n",
    "* Does a given piece of information imply the other?\n",
    "\n",
    "In NLP, this task is called analyzing _textual entailment_. However, that's only\n",
    "when the information comes from text content.\n",
    "In practice, it's often the case the information available comes not just\n",
    "from text content, but from a multimodal combination of text, images, audio, video, etc.\n",
    "_Multimodal entailment_ is simply the extension of textual entailment to a variety\n",
    "of new input modalities.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "This example requires TensorFlow 2.5 or higher. In addition, TensorFlow Hub and\n",
    "TensorFlow Text are required for the BERT model\n",
    "([Devlin et al.](https://arxiv.org/abs/1810.04805)). These libraries can be installed\n",
    "using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or tensorflow, or torch\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "from keras.utils import PyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define a label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "label_map = {\"Contradictory\": 0, \"Implies\": 1, \"NoEntailment\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Collect the dataset\n",
    "\n",
    "The original dataset is available\n",
    "[here](https://github.com/google-research-datasets/recognizing-multimodal-entailment).\n",
    "It comes with URLs of images which are hosted on Twitter's photo storage system called\n",
    "the\n",
    "[Photo Blob Storage (PBS for short)](https://blog.twitter.com/engineering/en_us/a/2012/blobstore-twitter-s-in-house-photo-storage-system).\n",
    "We will be working with the downloaded images along with additional data that comes with\n",
    "the original dataset. Thanks to\n",
    "[Nilabhra Roy Chowdhury](https://de.linkedin.com/in/nilabhraroychowdhury) who worked on\n",
    "preparing the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "image_base_path = keras.utils.get_file(\n",
    "    \"tweet_images\",\n",
    "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Read the dataset and apply basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/tweets.csv\"\n",
    ").iloc[\n",
    "    0:1000\n",
    "]  # Resources conservation since these are examples and not SOTA\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The columns we are interested in are the following:\n",
    "\n",
    "* `text_1`\n",
    "* `image_1`\n",
    "* `text_2`\n",
    "* `image_2`\n",
    "* `label`\n",
    "\n",
    "The entailment task is formulated as the following:\n",
    "\n",
    "***Given the pairs of (`text_1`, `image_1`) and (`text_2`, `image_2`) do they entail (or\n",
    "not entail or contradict) each other?***\n",
    "\n",
    "We have the images already downloaded. `image_1` is downloaded as `id1` as its filename\n",
    "and `image2` is downloaded as `id2` as its filename. In the next step, we will add two\n",
    "more columns to `df` - filepaths of `image_1`s and `image_2`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "images_one_paths = []\n",
    "images_two_paths = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    current_row = df.iloc[idx]\n",
    "    id_1 = current_row[\"id_1\"]\n",
    "    id_2 = current_row[\"id_2\"]\n",
    "    extentsion_one = current_row[\"image_1\"].split(\".\")[-1]\n",
    "    extentsion_two = current_row[\"image_2\"].split(\".\")[-1]\n",
    "\n",
    "    image_one_path = os.path.join(image_base_path, str(id_1) + f\".{extentsion_one}\")\n",
    "    image_two_path = os.path.join(image_base_path, str(id_2) + f\".{extentsion_two}\")\n",
    "\n",
    "    images_one_paths.append(image_one_path)\n",
    "    images_two_paths.append(image_two_path)\n",
    "\n",
    "df[\"image_1_path\"] = images_one_paths\n",
    "df[\"image_2_path\"] = images_two_paths\n",
    "\n",
    "# Create another column containing the integer ids of\n",
    "# the string labels.\n",
    "df[\"label_idx\"] = df[\"label\"].apply(lambda x: label_map[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize(idx):\n",
    "    current_row = df.iloc[idx]\n",
    "    image_1 = plt.imread(current_row[\"image_1_path\"])\n",
    "    image_2 = plt.imread(current_row[\"image_2_path\"])\n",
    "    text_1 = current_row[\"text_1\"]\n",
    "    text_2 = current_row[\"text_2\"]\n",
    "    label = current_row[\"label\"]\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Image One\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Image Two\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Text one: {text_1}\")\n",
    "    print(f\"Text two: {text_2}\")\n",
    "    print(f\"Label: {label}\")\n",
    "\n",
    "\n",
    "random_idx = random.choice(range(len(df)))\n",
    "visualize(random_idx)\n",
    "\n",
    "random_idx = random.choice(range(len(df)))\n",
    "visualize(random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train/test split\n",
    "\n",
    "The dataset suffers from\n",
    "[class imbalance problem](https://developers.google.com/machine-learning/glossary#class-imbalanced-dataset).\n",
    "We can confirm that in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "To account for that we will go for a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# 10% for test\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.1, stratify=df[\"label\"].values, random_state=42\n",
    ")\n",
    "# 5% for validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.05, stratify=train_df[\"label\"].values, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total training examples: {len(train_df)}\")\n",
    "print(f\"Total validation examples: {len(val_df)}\")\n",
    "print(f\"Total test examples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Data input pipeline\n",
    "\n",
    "Keras Hub provides\n",
    "[variety of BERT family of models](https://keras.io/keras_hub/presets/).\n",
    "Each of those models comes with a\n",
    "corresponding preprocessing layer. You can learn more about these models and their\n",
    "preprocessing layers from\n",
    "[this resource](https://www.kaggle.com/models/keras/bert/keras/bert_base_en_uncased/2).\n",
    "\n",
    "To keep the runtime of this example relatively short, we will use a base_unacased variant of\n",
    "the original BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "text preprocessing using KerasHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_preprocessor = keras_hub.models.BertTextClassifierPreprocessor.from_preset(\n",
    "    \"bert_base_en_uncased\",\n",
    "    sequence_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Run the preprocessor on a sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "idx = random.choice(range(len(train_df)))\n",
    "row = train_df.iloc[idx]\n",
    "sample_text_1, sample_text_2 = row[\"text_1\"], row[\"text_2\"]\n",
    "print(f\"Text 1: {sample_text_1}\")\n",
    "print(f\"Text 2: {sample_text_2}\")\n",
    "\n",
    "test_text = [sample_text_1, sample_text_2]\n",
    "text_preprocessed = text_preprocessor(test_text)\n",
    "\n",
    "print(\"Keys           : \", list(text_preprocessed.keys()))\n",
    "print(\"Shape Token Ids : \", text_preprocessed[\"token_ids\"].shape)\n",
    "print(\"Token Ids       : \", text_preprocessed[\"token_ids\"][0, :16])\n",
    "print(\" Shape Padding Mask     : \", text_preprocessed[\"padding_mask\"].shape)\n",
    "print(\"Padding Mask     : \", text_preprocessed[\"padding_mask\"][0, :16])\n",
    "print(\"Shape Segment Ids : \", text_preprocessed[\"segment_ids\"].shape)\n",
    "print(\"Segment Ids       : \", text_preprocessed[\"segment_ids\"][0, :16])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will now create `tf.data.Dataset` objects from the dataframes.\n",
    "\n",
    "Note that the text inputs will be preprocessed as a part of the data input pipeline. But\n",
    "the preprocessing modules can also be a part of their corresponding BERT models. This\n",
    "helps reduce the training/serving skew and lets our models operate with raw text inputs.\n",
    "Follow [this tutorial](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)\n",
    "to learn more about how to incorporate the preprocessing modules directly inside the\n",
    "models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\", \"label_idx\"]\n",
    "    ds = UnifiedPyDataset(\n",
    "        dataframe,\n",
    "        batch_size=32,\n",
    "        workers=4,\n",
    "    )\n",
    "    return ds\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preprocessing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
    "\n",
    "\n",
    "def preprocess_text(text_1, text_2):\n",
    "    output = text_preprocessor([text_1, text_2])\n",
    "    output = {\n",
    "        feature: keras.ops.reshape(output[feature], [-1])\n",
    "        for feature in bert_input_features\n",
    "    }\n",
    "    return output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Create the final datasets, method adapted from PyDataset doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class UnifiedPyDataset(PyDataset):\n",
    "    \"\"\"A Keras-compatible dataset that processes a DataFrame for TensorFlow, JAX, and PyTorch.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        batch_size=32,\n",
    "        workers=4,\n",
    "        use_multiprocessing=False,\n",
    "        max_queue_size=10,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: pandas DataFrame with data\n",
    "            batch_size: Batch size for dataset\n",
    "            workers: Number of workers to use for parallel loading (Keras)\n",
    "            use_multiprocessing: Whether to use multiprocessing\n",
    "            max_queue_size: Maximum size of the data queue for parallel loading\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataframe = df\n",
    "        columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\"]\n",
    "\n",
    "        # image files\n",
    "        self.image_x_1 = self.dataframe[\"image_1_path\"]\n",
    "        self.image_x_2 = self.dataframe[\"image_1_path\"]\n",
    "        self.image_y = self.dataframe[\"label_idx\"]\n",
    "\n",
    "        # text files\n",
    "        self.text_x_1 = self.dataframe[\"text_1\"]\n",
    "        self.text_x_2 = self.dataframe[\"text_2\"]\n",
    "        self.text_y = self.dataframe[\"label_idx\"]\n",
    "\n",
    "        # general\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "        self.use_multiprocessing = use_multiprocessing\n",
    "        self.max_queue_size = max_queue_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Fetches a batch of data from the dataset at the given index.\n",
    "        \"\"\"\n",
    "\n",
    "        # Return x, y for batch idx.\n",
    "        low = index * self.batch_size\n",
    "        # Cap upper bound at array length; the last batch may be smaller\n",
    "        # if the total number of items is not a multiple of batch size.\n",
    "\n",
    "        high_image_1 = min(low + self.batch_size, len(self.image_x_1))\n",
    "        high_image_2 = min(low + self.batch_size, len(self.image_x_2))\n",
    "\n",
    "        high_text_1 = min(low + self.batch_size, len(self.text_x_1))\n",
    "        high_text_2 = min(low + self.batch_size, len(self.text_x_1))\n",
    "\n",
    "        # images files\n",
    "        batch_image_x_1 = self.image_x_1[low:high_image_1]\n",
    "        batch_image_y_1 = self.image_y[low:high_image_1]\n",
    "\n",
    "        batch_image_x_2 = self.image_x_2[low:high_image_2]\n",
    "        batch_image_y_2 = self.image_y[low:high_image_2]\n",
    "\n",
    "        # text files\n",
    "        batch_text_x_1 = self.text_x_1[low:high_text_1]\n",
    "        batch_text_y_1 = self.text_y[low:high_text_1]\n",
    "\n",
    "        batch_text_x_2 = self.text_x_2[low:high_text_2]\n",
    "        batch_text_y_2 = self.text_y[low:high_text_2]\n",
    "\n",
    "        # image number 1 inputs\n",
    "        image_1 = [\n",
    "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_1\n",
    "        ]\n",
    "        image_1 = [\n",
    "            (  # exeperienced some shapes which were different from others.\n",
    "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
    "                if img.shape[2] == 4\n",
    "                else img\n",
    "            )\n",
    "            for img in image_1\n",
    "        ]\n",
    "        image_1 = np.array(image_1)\n",
    "\n",
    "        # Both text inputs to the model, return a dict for inputs to BertBackbone\n",
    "        text = {\n",
    "            key: np.array(\n",
    "                [\n",
    "                    d[key]\n",
    "                    for d in [\n",
    "                        preprocess_text(file_path1, file_path2)\n",
    "                        for file_path1, file_path2 in zip(\n",
    "                            batch_text_x_1, batch_text_x_2\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "            for key in [\"padding_mask\", \"token_ids\", \"segment_ids\"]\n",
    "        }\n",
    "\n",
    "        # Image number 2 model inputs\n",
    "        image_2 = [\n",
    "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_2\n",
    "        ]\n",
    "        image_2 = [\n",
    "            (  # exeperienced some shapes which were different from others\n",
    "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
    "                if img.shape[2] == 4\n",
    "                else img\n",
    "            )\n",
    "            for img in image_2\n",
    "        ]\n",
    "        # Stack the list comprehension to an nd.array\n",
    "        image_2 = np.array(image_2)\n",
    "\n",
    "        return (\n",
    "            {\n",
    "                \"image_1\": image_1,\n",
    "                \"image_2\": image_2,\n",
    "                \"padding_mask\": text[\"padding_mask\"],\n",
    "                \"segment_ids\": text[\"segment_ids\"],\n",
    "                \"token_ids\": text[\"token_ids\"],\n",
    "            },\n",
    "            # Target lables\n",
    "            np.array(batch_image_y_1),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches in the dataset.\n",
    "        \"\"\"\n",
    "        return math.ceil(len(self.dataframe) / self.batch_size)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Create train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(dataframe):\n",
    "    ds = dataframe_to_dataset(dataframe)\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = prepare_dataset(train_df)\n",
    "validation_ds = prepare_dataset(val_df)\n",
    "test_ds = prepare_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model building utilities\n",
    "\n",
    "Our final model will accept two images along with their text counterparts. While the\n",
    "images will be directly fed to the model the text inputs will first be preprocessed and\n",
    "then will make it into the model. Below is a visual illustration of this approach:\n",
    "\n",
    "![](https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/figures/brief_architecture.png)\n",
    "\n",
    "The model consists of the following elements:\n",
    "\n",
    "* A standalone encoder for the images. We will use a\n",
    "[ResNet50V2](https://arxiv.org/abs/1603.05027) pre-trained on the ImageNet-1k dataset for\n",
    "this.\n",
    "* A standalone encoder for the images. A pre-trained BERT will be used for this.\n",
    "\n",
    "After extracting the individual embeddings, they will be projected in an identical space.\n",
    "Finally, their projections will be concatenated and be fed to the final classification\n",
    "layer.\n",
    "\n",
    "This is a multi-class classification problem involving the following classes:\n",
    "\n",
    "* NoEntailment\n",
    "* Implies\n",
    "* Contradictory\n",
    "\n",
    "`project_embeddings()`, `create_vision_encoder()`, and `create_text_encoder()` utilities\n",
    "are referred from [this example](https://keras.io/examples/nlp/nl_image_search/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Projection utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = keras.ops.nn.gelu(projected_embeddings)\n",
    "        x = keras.layers.Dense(projection_dims)(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "        x = keras.layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
    "    return projected_embeddings\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Vision encoder utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
    "    resnet_v2 = keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in resnet_v2.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    # Receive the images as inputs.\n",
    "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
    "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
    "\n",
    "    # Preprocess the input image.\n",
    "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
    "    preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)\n",
    "\n",
    "    # Generate the embeddings for the images using the resnet_v2 model\n",
    "    # concatenate them.\n",
    "    embeddings_1 = resnet_v2(preprocessed_1)\n",
    "    embeddings_2 = resnet_v2(preprocessed_2)\n",
    "    embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
    "\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model([image_1, image_2], outputs, name=\"vision_encoder\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Text encoder utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained BERT BackBone using KerasHub.\n",
    "    bert = keras_hub.models.BertBackbone.from_preset(\n",
    "        \"bert_base_en_uncased\", num_classes=3\n",
    "    )\n",
    "\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "\n",
    "    # Receive the text as inputs.\n",
    "    bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
    "    inputs = {\n",
    "        feature: keras.Input(shape=(256,), dtype=\"int32\", name=feature)\n",
    "        for feature in bert_input_features\n",
    "    }\n",
    "\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(inputs)[\"pooled_output\"]\n",
    "\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Multimodal model utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_multimodal_model(\n",
    "    num_projection_layers=1,\n",
    "    projection_dims=256,\n",
    "    dropout_rate=0.1,\n",
    "    vision_trainable=False,\n",
    "    text_trainable=False,\n",
    "):\n",
    "    # Receive the images as inputs.\n",
    "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
    "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
    "\n",
    "    # Receive the text as inputs.\n",
    "    bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
    "    text_inputs = {\n",
    "        feature: keras.Input(shape=(256,), dtype=\"int32\", name=feature)\n",
    "        for feature in bert_input_features\n",
    "    }\n",
    "    text_inputs = list(text_inputs.values())\n",
    "    # Create the encoders.\n",
    "    vision_encoder = create_vision_encoder(\n",
    "        num_projection_layers, projection_dims, dropout_rate, vision_trainable\n",
    "    )\n",
    "    text_encoder = create_text_encoder(\n",
    "        num_projection_layers, projection_dims, dropout_rate, text_trainable\n",
    "    )\n",
    "\n",
    "    # Fetch the embedding projections.\n",
    "    vision_projections = vision_encoder([image_1, image_2])\n",
    "    text_projections = text_encoder(text_inputs)\n",
    "\n",
    "    # Concatenate the projections and pass through the classification layer.\n",
    "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
    "    outputs = keras.layers.Dense(3, activation=\"softmax\")(concatenated)\n",
    "    return keras.Model([image_1, image_2, *text_inputs], outputs)\n",
    "\n",
    "\n",
    "multimodal_model = create_multimodal_model()\n",
    "keras.utils.plot_model(multimodal_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You can inspect the structure of the individual encoders as well by setting the\n",
    "`expand_nested` argument of `plot_model()` to `True`. You are encouraged\n",
    "to play with the different hyperparameters involved in building this model and\n",
    "observe how the final performance is affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "multimodal_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = multimodal_model.fit(train_ds, validation_data=validation_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "_, acc = multimodal_model.evaluate(test_ds)\n",
    "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Additional notes regarding training\n",
    "\n",
    "**Incorporating regularization**:\n",
    "\n",
    "The training logs suggest that the model is starting to overfit and may have benefitted\n",
    "from regularization. Dropout ([Srivastava et al.](https://jmlr.org/papers/v15/srivastava14a.html))\n",
    "is a simple yet powerful regularization technique that we can use in our model.\n",
    "But how should we apply it here?\n",
    "\n",
    "We could always introduce Dropout (`keras.layers.Dropout`) in between different layers of the model.\n",
    "But here is another recipe. Our model expects inputs from two different data modalities.\n",
    "What if either of the modalities is not present during inference? To account for this,\n",
    "we can introduce Dropout to the individual projections just before they get concatenated:\n",
    "\n",
    "```python\n",
    "vision_projections = keras.layers.Dropout(rate)(vision_projections)\n",
    "text_projections = keras.layers.Dropout(rate)(text_projections)\n",
    "concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
    "```\n",
    "\n",
    "**Attending to what matters**:\n",
    "\n",
    "Do all parts of the images correspond equally to their textual counterparts? It's likely\n",
    "not the case. To make our model only focus on the most important bits of the images that relate\n",
    "well to their corresponding textual parts we can use \"cross-attention\":\n",
    "\n",
    "```python\n",
    "# Embeddings.\n",
    "vision_projections = vision_encoder([image_1, image_2])\n",
    "text_projections = text_encoder(text_inputs)\n",
    "\n",
    "# Cross-attention (Luong-style).\n",
    "query_value_attention_seq = keras.layers.Attention(use_scale=True, dropout=0.2)(\n",
    "    [vision_projections, text_projections]\n",
    ")\n",
    "# Concatenate.\n",
    "concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
    "contextual = keras.layers.Concatenate()([concatenated, query_value_attention_seq])\n",
    "```\n",
    "\n",
    "To see this in action, refer to\n",
    "[this notebook](https://github.com/sayakpaul/Multimodal-Entailment-Baseline/blob/main/multimodal_entailment_attn.ipynb).\n",
    "\n",
    "**Handling class imbalance**:\n",
    "\n",
    "The dataset suffers from class imbalance. Investigating the confusion matrix of the\n",
    "above model reveals that it performs poorly on the minority classes. If we had used a\n",
    "weighted loss then the training would have been more guided. You can check out\n",
    "[this notebook](https://github.com/sayakpaul/Multimodal-Entailment-Baseline/blob/main/multimodal_entailment.ipynb)\n",
    "that takes class-imbalance into account during model training.\n",
    "\n",
    "**Using only text inputs**:\n",
    "\n",
    "Also, what if we had only incorporated text inputs for the entailment task? Because of\n",
    "the nature of the text inputs encountered on social media platforms, text inputs alone\n",
    "would have hurt the final performance. Under a similar training setup, by only using\n",
    "text inputs we get to 67.14% top-1 accuracy on the same test set. Refer to\n",
    "[this notebook](https://github.com/sayakpaul/Multimodal-Entailment-Baseline/blob/main/text_entailment.ipynb)\n",
    "for details.\n",
    "\n",
    "Finally, here is a table comparing different approaches taken for the entailment task:\n",
    "\n",
    "| Type  | Standard<br>Cross-entropy     | Loss-weighted<br>Cross-entropy    | Focal Loss    |\n",
    "|:---:  |:---:  |:---:    |:---:    |\n",
    "| Multimodal    | 77.86%    | 67.86%    | 86.43%    |\n",
    "| Only text     | 67.14%    | 11.43%    | 37.86%    |\n",
    "\n",
    "You can check out [this repository](https://git.io/JR0HU) to learn more about how the\n",
    "experiments were conducted to obtain these numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Final remarks\n",
    "\n",
    "* The architecture we used in this example is too large for the number of data points\n",
    "available for training. It's going to benefit from more data.\n",
    "* We used a smaller variant of the original BERT model. Chances are high that with a\n",
    "larger variant, this performance will be improved. TensorFlow Hub\n",
    "[provides](https://www.tensorflow.org/text/tutorials/bert_glue#loading_models_from_tensorflow_hub)\n",
    "a number of different BERT models that you can experiment with.\n",
    "* We kept the pre-trained models frozen. Fine-tuning them on the multimodal entailment\n",
    "task would could resulted in better performance.\n",
    "* We built a simple baseline model for the multimodal entailment task. There are various\n",
    "approaches that have been proposed to tackle the entailment problem.\n",
    "[This presentation deck](https://docs.google.com/presentation/d/1mAB31BCmqzfedreNZYn4hsKPFmgHA9Kxz219DzyRY3c/edit?usp=sharing)\n",
    "from the\n",
    "[Recognizing Multimodal Entailment](https://multimodal-entailment.github.io/)\n",
    "tutorial provides a comprehensive overview.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/multimodal-entailment)\n",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/multimodal_entailment)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multimodal_entailment",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}