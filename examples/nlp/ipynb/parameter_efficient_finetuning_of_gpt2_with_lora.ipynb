{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Parameter-efficient fine-tuning of GPT-2 with LoRA\n",
    "\n",
    "**Author:** [Abheesht Sharma](https://github.com/abheesht17/), [Matthew Watson](https://github.com/mattdangerw/)<br>\n",
    "**Date created:** 2023/05/27<br>\n",
    "**Last modified:** 2023/05/27<br>\n",
    "**Description:** Use KerasHub to fine-tune a GPT-2 LLM with LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) have been shown to be effective at a variety of NLP\n",
    "tasks. An LLM is first pre-trained on a large corpus of text in a\n",
    "self-supervised fashion. Pre-training helps LLMs learn general-purpose knowledge,\n",
    "such as statistical relationships between words. An LLM can then be fine-tuned\n",
    "on a downstream task of interest (such as sentiment analysis).\n",
    "\n",
    "However, LLMs are extremely large in size, and we don't need to train all the\n",
    "parameters in the model while fine-tuning, especially because datasets on which\n",
    "the model is fine-tuned are relatively small. Another way of saying this is\n",
    "that LLMs are over-parametrized for fine-tuning. This is where\n",
    "[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685) comes in; it\n",
    "significantly reduces the number of trainable parameters. This results in a\n",
    "decrease in training time and GPU memory usage, while maintaining the quality\n",
    "of the outputs.\n",
    "\n",
    "In this example, we will explain LoRA in technical terms, show how the technical\n",
    "explanation translates to code, hack KerasHub's\n",
    "[GPT-2 model](https://keras.io/api/keras_hub/models/gpt2/) and fine-tune\n",
    "it on the next token prediction task using LoRA. We will compare LoRA GPT-2\n",
    "with a fully fine-tuned GPT-2 in terms of the quality of the generated text,\n",
    "training time and GPU memory usage.\n",
    "\n",
    "Note: This example runs on the TensorFlow backend purely for the\n",
    "`tf.config.experimental.get_memory_info` API to easily plot memory usage.\n",
    "Outside of the memory usage callback, this example will run on `jax` and `torch`\n",
    "backends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before we start implementing the pipeline, let's install and import all the\n",
    "libraries we need. We'll be using the KerasHub library.\n",
    "\n",
    "Secondly, let's enable mixed precision training. This will help us reduce the\n",
    "training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade keras-hub\n",
    "!pip install -q --upgrade keras  # Upgrade to Keras 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's also define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# General hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_BATCHES = 500\n",
    "EPOCHS = 1  # Can be set to a higher value for better results\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "MAX_GENERATION_LENGTH = 200\n",
    "\n",
    "GPT2_PRESET = \"gpt2_base_en\"\n",
    "\n",
    "# LoRA-specific hyperparameters\n",
    "RANK = 4\n",
    "ALPHA = 32.0\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load a Reddit dataset. We will fine-tune both the GPT-2 model and the\n",
    "LoRA GPT-2 model on a subset of this dataset. The aim is to produce text similar\n",
    "in style to Reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The dataset has two fields: `document` and `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for document, title in reddit_ds:\n",
    "    print(document.numpy())\n",
    "    print(title.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We'll now batch the dataset and retain only the `document` field because we are\n",
    "fine-tuning the model on the next word prediction task. Take a subset\n",
    "of the dataset for the purpose of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    reddit_ds.map(lambda document, _: document)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "train_ds = train_ds.take(NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "Before we begin fine-tuning the models, let's define a few helper functions and\n",
    "classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Callback for tracking GPU memory usage\n",
    "\n",
    "We'll define a custom callback function which tracks GPU memory usage. The\n",
    "callback function uses TensorFlow's `tf.config.experimental.get_memory_info`\n",
    "API.\n",
    "\n",
    "Here, we assume that we are using a single GPU, `GPU:0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GPUMemoryCallback(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_batches,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.target_batches = target_batches\n",
    "        self.print_stats = print_stats\n",
    "\n",
    "        self.memory_usage = []\n",
    "        self.labels = []\n",
    "\n",
    "    def _compute_memory_usage(self):\n",
    "        memory_stats = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "        # Convert bytes to GB and store in list.\n",
    "        peak_usage = round(memory_stats[\"peak\"] / (2**30), 3)\n",
    "        self.memory_usage.append(peak_usage)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} start\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if batch in self.target_batches:\n",
    "            self._compute_memory_usage()\n",
    "            self.labels.append(f\"batch {batch}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} end\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Function for text generation\n",
    "\n",
    "Here is a helper function to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, input_text, max_length=200):\n",
    "    start = time.time()\n",
    "\n",
    "    output = model.generate(input_text, max_length=max_length)\n",
    "    print(\"\\nOutput:\")\n",
    "    print(output)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Define optimizer and loss\n",
    "\n",
    "We will use AdamW optimizer and cross-entropy loss for training both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_optimizer_and_loss():\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        epsilon=1e-6,\n",
    "        global_clipnorm=1.0,  # Gradient clipping.\n",
    "    )\n",
    "    # Exclude layernorm and bias terms from weight decay.\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
    "\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    return optimizer, loss\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Fine-tune GPT-2\n",
    "\n",
    "Let's load the model and preprocessor first. We use a sequence length of 128\n",
    "instead of 1024 (which is the default sequence length). This will limit our\n",
    "ability to predict long sequences, but will allow us to run this example quickly\n",
    "on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "preprocessor = keras_hub.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    ")\n",
    "gpt2_lm = keras_hub.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "gpt2_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Initialize the GPU memory tracker callback object, and compile the model. We\n",
    "use the Adam optimizer with a linearly decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "gpu_memory_callback = GPUMemoryCallback(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "optimizer, loss = get_optimizer_and_loss()\n",
    "\n",
    "gpt2_lm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We are all set to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "gpt2_lm.fit(train_ds, epochs=EPOCHS, callbacks=[gpu_memory_callback])\n",
    "gpt2_lm_memory_usage = gpu_memory_callback.memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As a final step, let's generate some text. We will harness the power of XLA. The\n",
    "first call to `generate()` will be slow because of XLA compilation, but\n",
    "subsequent calls will be super-fast. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "generate_text(gpt2_lm, \"I like basketball\", max_length=MAX_GENERATION_LENGTH)\n",
    "generate_text(gpt2_lm, \"That Italian restaurant is\", max_length=MAX_GENERATION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## LoRA GPT-2\n",
    "\n",
    "In this section, we discuss the technical details of LoRA, build a LoRA GPT-2\n",
    "model, fine-tune it and generate text.\n",
    "\n",
    "### What exactly is LoRA?\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning technique for LLMs. It freezes the\n",
    "weights of the LLM, and injects trainable rank-decomposition matrices. Let's\n",
    "understand this more clearly.\n",
    "\n",
    "Assume we have an `n x n` pre-trained dense layer (or weight matrix), `W0`. We\n",
    "initialize two dense layers, `A` and `B`, of shapes `n x rank`, and `rank x n`,\n",
    "respectively. `rank` is much smaller than `n`. In the paper, values between 1\n",
    "and 4 are shown to work well.\n",
    "\n",
    "\n",
    "#### LoRA equation\n",
    "\n",
    "The original equation is `output = W0x + b0`, where `x` is the input, `W0` and\n",
    "`b0` are the weight matrix and bias terms of the original dense layer (frozen).\n",
    "The LoRA equation is: `output = W0x + b0 + BAx`, where `A` and `B` are the\n",
    "rank-decomposition matrices.\n",
    "\n",
    "LoRA is based on the idea that updates to the weights of the pre-trained\n",
    "language model have a low \"intrinsic rank\" since pre-trained language models are\n",
    "over-parametrized. Predictive performance of full fine-tuning can be replicated\n",
    "even by constraining `W0`'s updates to low-rank decomposition matrices.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.imgur.com/f4TFqMi.png\" alt=\"lora_diagram\" height=\"250\"/>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "#### Number of trainable parameters\n",
    "\n",
    "Let's do some quick math. Suppose `n` is 768, and `rank` is 4. `W0` has\n",
    "`768 x 768 = 589,824` parameters, whereas the LoRA layers, `A` and `B` together\n",
    "have `768 x 4 + 4 x 768 = 6,144` parameters. So, for the dense layer, we go from\n",
    "`589,824` trainable parameters to `6,144` trainable parameters!\n",
    "\n",
    "#### Why does LoRA reduce memory footprint?\n",
    "\n",
    "Even though the total number of parameters increase (since we are adding LoRA\n",
    "layers), the memory footprint reduces, because the number of trainable\n",
    "parameters reduces. Let's dive deeper into this.\n",
    "\n",
    "The memory usage of a model can be split into four parts:\n",
    "\n",
    "- Model memory: This is the memory required to store the model weights. This\n",
    "will be slightly higher for LoRA than GPT-2.\n",
    "- Forward pass memory: This mostly depends on batch size, sequence length, etc.\n",
    "We keep this constant for both models for a fair comparison.\n",
    "- Backward pass memory: This is the memory required to store the gradients.\n",
    "Note that the gradients are computed only for the trainable parameters.\n",
    "- Optimizer memory: This is the memory required to store the optimizer state.\n",
    "For example, the Adam optimizer stores the \"1st moment vectors\" and\n",
    "\"2nd moment vectors\" for the trainable parameters.\n",
    "\n",
    "Since, with LoRA, there is a huge reduction in the number of trainable\n",
    "parameters, the optimizer memory and the memory required to store the gradients\n",
    "for LoRA is much less than GPT-2. This is where most of the memory savings\n",
    "happen.\n",
    "\n",
    "#### Why is LoRA so popular?\n",
    "\n",
    "- Reduces GPU memory usage;\n",
    "- Faster training; and\n",
    "- No additional inference latency.\n",
    "\n",
    "### Create LoRA layer\n",
    "\n",
    "According to the technical description above, let's create a LoRA layer. In\n",
    "a transformer model, the LoRA layer is created and injected for the query and\n",
    "value projection matrices. In `keras.layers.MultiHeadAttention`, the query/value\n",
    "projection layers are `keras.layers.EinsumDense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class LoraLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        rank=8,\n",
    "        alpha=32,\n",
    "        trainable=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # We want to keep the name of this layer the same as the original\n",
    "        # dense layer.\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self._scale = alpha / rank\n",
    "\n",
    "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
    "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
    "\n",
    "        # Layers.\n",
    "\n",
    "        # Original dense layer.\n",
    "        self.original_layer = original_layer\n",
    "        # No matter whether we are training the model or are in inference mode,\n",
    "        # this layer should be frozen.\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "        # LoRA dense layers.\n",
    "        self.A = keras.layers.Dense(\n",
    "            units=rank,\n",
    "            use_bias=False,\n",
    "            # Note: the original paper mentions that normal distribution was\n",
    "            # used for initialization. However, the official LoRA implementation\n",
    "            # uses \"Kaiming/He Initialization\".\n",
    "            kernel_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
    "            ),\n",
    "            trainable=trainable,\n",
    "            name=f\"lora_A\",\n",
    "        )\n",
    "        # B has the same `equation` and `output_shape` as the original layer.\n",
    "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
    "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
    "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
    "        # `c` represents `rank`.\n",
    "        self.B = keras.layers.EinsumDense(\n",
    "            equation=original_layer_config[\"equation\"],\n",
    "            output_shape=original_layer_config[\"output_shape\"],\n",
    "            kernel_initializer=\"zeros\",\n",
    "            trainable=trainable,\n",
    "            name=f\"lora_B\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        original_output = self.original_layer(inputs)\n",
    "        if self.trainable:\n",
    "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
    "            # to the original layer's output.\n",
    "            lora_output = self.B(self.A(inputs)) * self._scale\n",
    "            return original_output + lora_output\n",
    "\n",
    "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
    "        # the original layer's weights - more on this in the text generation\n",
    "        # section!\n",
    "        return original_output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inject LoRA layer into the model\n",
    "\n",
    "We will now hack the original GPT-2 model and inject LoRA layers into it. Let's\n",
    "do a couple of things before doing that:\n",
    "\n",
    "- Delete previous model;\n",
    "- Reset \"peak\" GPU memory usage using `tf.config.experimental.reset_memory_stats`;\n",
    "- Load a new GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "del gpt2_lm\n",
    "del optimizer\n",
    "del loss\n",
    "\n",
    "# This resets \"peak\" memory usage to \"current\" memory usage.\n",
    "tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "\n",
    "# Load the original model.\n",
    "preprocessor = keras_hub.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "lora_model = keras_hub.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will now override the original query/value projection matrices with our\n",
    "new LoRA layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for layer_idx in range(lora_model.backbone.num_layers):\n",
    "    # Change query dense layer.\n",
    "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
    "    self_attention_layer = decoder_layer._self_attention_layer\n",
    "    # Allow mutation to Keras layer state.\n",
    "    self_attention_layer._tracker.locked = False\n",
    "\n",
    "    # Change query dense layer.\n",
    "    self_attention_layer._query_dense = LoraLayer(\n",
    "        self_attention_layer._query_dense,\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "        trainable=True,\n",
    "    )\n",
    "\n",
    "    # Change value dense layer.\n",
    "    self_attention_layer._value_dense = LoraLayer(\n",
    "        self_attention_layer._value_dense,\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "        trainable=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's now do a forward pass to make sure we still have a valid chain of\n",
    "computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "lora_model(preprocessor([\"LoRA is very useful for quick LLM finetuning\"])[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Freeze the entire LLM, only the LoRA layers should be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for layer in lora_model._flatten_layers():\n",
    "    lst_of_sublayers = list(layer._flatten_layers())\n",
    "\n",
    "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
    "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Print the model's summary and see if the number of non-trainable parameters and\n",
    "total parameters are correct.\n",
    "\n",
    "In a previous section, we had calculated the number of parameters associated with\n",
    "the LoRA layers to be 6,144. The total trainable parameters in the model should\n",
    "be `num_layers * (query, value) * 6,144 = 12 * 2 * 6,144 = 147,456`. The\n",
    "number of non-trainable parameters should be the same as the total number of\n",
    "parameters in the original GPT-2 model, which is `124,439,808`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "lora_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Fine-tune LoRA GPT-2\n",
    "\n",
    "Now that we have hacked and verified the LoRA GPT-2 model, let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "gpu_memory_callback = GPUMemoryCallback(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "optimizer, loss = get_optimizer_and_loss()\n",
    "\n",
    "lora_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "lora_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[gpu_memory_callback],\n",
    ")\n",
    "lora_model_memory_usage = gpu_memory_callback.memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And we are done fine-tuning the model! Before we generate text, let's compare\n",
    "the training time and memory usage of the two models. The training time of GPT-2\n",
    "on a 16 GB Tesla T4 (Colab) is 7 minutes, and for LoRA, it is 5 minutes, a 30%\n",
    "decrease. The memory usage of LoRA GPT-2 is roughly 35% times less than GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [\"GPT-2\", \"LoRA GPT-2\"],\n",
    "    [max(gpt2_lm_memory_usage), max(lora_model_memory_usage)],\n",
    "    color=[\"red\", \"blue\"],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"GPU Memory Usage (in GB)\")\n",
    "\n",
    "plt.title(\"GPU Memory Usage Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Merge weights and generate text!\n",
    "\n",
    "One of the biggest advantages of LoRA over other adapter methods is that it\n",
    "does not incur any additional inference latency. Let's understand why.\n",
    "\n",
    "Recall our LoRA equation: `output = W0x + b0 + BAx`. We can rewrite this as:\n",
    "`output = = Wx + b0 = (W0 + BA)x + b0`, where `W = W0 + BA`. This means that if\n",
    "we merge the weights of the original model and the adapter, we will be essentially\n",
    "doing the same computation as the original model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for layer_idx in range(lora_model.backbone.num_layers):\n",
    "    self_attention_layer = lora_model.backbone.get_layer(\n",
    "        f\"transformer_layer_{layer_idx}\"\n",
    "    )._self_attention_layer\n",
    "\n",
    "    # Merge query dense layer.\n",
    "    query_lora_layer = self_attention_layer._query_dense\n",
    "\n",
    "    A_weights = query_lora_layer.A.kernel  # (768, 1) (a, b)\n",
    "    B_weights = query_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
    "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
    "    query_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
    "\n",
    "    # Merge value dense layer.\n",
    "    value_lora_layer = self_attention_layer._value_dense\n",
    "\n",
    "    A_weights = value_lora_layer.A.kernel  # (768, 1) (a, b)\n",
    "    B_weights = value_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
    "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
    "    value_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
    "\n",
    "    # Put back in place the original layers with updated weights\n",
    "    self_attention_layer._query_dense = query_lora_layer.original_layer\n",
    "    self_attention_layer._value_dense = value_lora_layer.original_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We are now all set to generate text with our LoRA model :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Freezing weights not necessary during generation since no weights are updated.\n",
    "generate_text(lora_model, \"I like basketball\", max_length=MAX_GENERATION_LENGTH)\n",
    "generate_text(\n",
    "    lora_model, \"That Italian restaurant is\", max_length=MAX_GENERATION_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And we're all done!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "parameter_efficient_finetuning_of_gpt2_with_lora",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}