{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Deep Learning for Customer Lifetime Value\n",
    "\n",
    "**Author:** [Praveen Hosdrug](https://www.linkedin.com/in/praveenhosdrug/)<br>\n",
    "**Date created:** 2024/11/23<br>\n",
    "**Last modified:** 2024/11/27<br>\n",
    "**Description:** A hybrid deep learning architecture for predicting customer purchase patterns and lifetime value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "A hybrid deep learning architecture combining Transformer encoders and LSTM networks\n",
    "for predicting customer purchase patterns and lifetime value using transaction history.\n",
    "While many existing review articles focus on classic parametric models and traditional machine learning algorithms\n",
    ",this implementation leverages recent advancements in Transformer-based models for time series prediction.\n",
    "The approach handles multi-granularity prediction across different temporal scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setting up Libraries for the Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def install_packages(packages):\n",
    "    \"\"\"\n",
    "    Install a list of packages using pip.\n",
    "\n",
    "    Args:\n",
    "        packages (list): A list of package names to install.\n",
    "    \"\"\"\n",
    "    for package in packages:\n",
    "        subprocess.run([\"pip\", \"install\", package], check=True)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## List of Packages to Install\n",
    "\n",
    "1. uciml: For the purpose of the tutorial; we will be using\n",
    "          the UK Retail [Dataset](https://archive.ics.uci.edu/dataset/352/online+retail)\n",
    "2. keras_hub: Access to the transformer encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "packages_to_install = [\"ucimlrepo\", \"keras_hub\"]\n",
    "\n",
    "# Install the packages\n",
    "install_packages(packages_to_install)\n",
    "\n",
    "# Core data processing and numerical libraries\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keras imports\n",
    "from keras import layers\n",
    "from keras import Model\n",
    "from keras import ops\n",
    "from keras_hub.layers import TransformerEncoder\n",
    "from keras import regularizers\n",
    "\n",
    "# UK Retail Dataset\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preprocessing the UK Retail dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_time_series_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess retail transaction data for deep learning.\n",
    "\n",
    "    Args:\n",
    "        data: Raw transaction data containing InvoiceDate, UnitPrice, etc.\n",
    "    Returns:\n",
    "        Processed DataFrame with calculated features\n",
    "    \"\"\"\n",
    "    processed_data = data.copy()\n",
    "\n",
    "    # Essential datetime handling for temporal ordering\n",
    "    processed_data[\"InvoiceDate\"] = pd.to_datetime(processed_data[\"InvoiceDate\"])\n",
    "\n",
    "    # Basic business constraints and calculations\n",
    "    processed_data = processed_data[processed_data[\"UnitPrice\"] > 0]\n",
    "    processed_data[\"Amount\"] = processed_data[\"UnitPrice\"] * processed_data[\"Quantity\"]\n",
    "    processed_data[\"CustomerID\"] = processed_data[\"CustomerID\"].fillna(99999.0)\n",
    "\n",
    "    # Handle outliers in Amount using statistical thresholds\n",
    "    q1 = processed_data[\"Amount\"].quantile(0.25)\n",
    "    q3 = processed_data[\"Amount\"].quantile(0.75)\n",
    "\n",
    "    # Define bounds - using 1.5 IQR rule\n",
    "    lower_bound = q1 - 1.5 * (q3 - q1)\n",
    "    upper_bound = q3 + 1.5 * (q3 - q1)\n",
    "\n",
    "    # Filter outliers\n",
    "    processed_data = processed_data[\n",
    "        (processed_data[\"Amount\"] >= lower_bound)\n",
    "        & (processed_data[\"Amount\"] <= upper_bound)\n",
    "    ]\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Load Data\n",
    "\n",
    "online_retail = fetch_ucirepo(id=352)\n",
    "raw_data = online_retail.data.features\n",
    "transformed_data = prepare_time_series_data(raw_data)\n",
    "\n",
    "\n",
    "def prepare_data_for_modeling(\n",
    "    df: pd.DataFrame,\n",
    "    input_sequence_length: int = 6,\n",
    "    output_sequence_length: int = 6,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Transform retail data into sequence-to-sequence format with separate\n",
    "    temporal and trend components.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Daily aggregation\n",
    "    daily_purchases = (\n",
    "        df.groupby([\"CustomerID\", pd.Grouper(key=\"InvoiceDate\", freq=\"D\")])\n",
    "        .agg({\"Amount\": \"sum\", \"Quantity\": \"sum\", \"Country\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    daily_purchases[\"frequency\"] = np.where(daily_purchases[\"Amount\"] > 0, 1, 0)\n",
    "\n",
    "    # Monthly resampling\n",
    "    monthly_purchases = (\n",
    "        daily_purchases.set_index(\"InvoiceDate\")\n",
    "        .groupby(\"CustomerID\")\n",
    "        .resample(\"M\")\n",
    "        .agg(\n",
    "            {\"Amount\": \"sum\", \"Quantity\": \"sum\", \"frequency\": \"sum\", \"Country\": \"first\"}\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Add cyclical temporal features\n",
    "    def prepare_temporal_features(input_window: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "        month = input_window[\"InvoiceDate\"].dt.month\n",
    "        month_sin = np.sin(2 * np.pi * month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * month / 12)\n",
    "        is_quarter_start = (month % 3 == 1).astype(int)\n",
    "\n",
    "        temporal_features = np.column_stack(\n",
    "            [\n",
    "                month,\n",
    "                input_window[\"InvoiceDate\"].dt.year,\n",
    "                month_sin,\n",
    "                month_cos,\n",
    "                is_quarter_start,\n",
    "            ]\n",
    "        )\n",
    "        return temporal_features\n",
    "\n",
    "    # Prepare trend features with lagged values\n",
    "    def prepare_trend_features(input_window: pd.DataFrame, lag: int = 3) -> np.ndarray:\n",
    "\n",
    "        lagged_data = pd.DataFrame()\n",
    "        for i in range(1, lag + 1):\n",
    "            lagged_data[f\"Amount_lag_{i}\"] = input_window[\"Amount\"].shift(i)\n",
    "            lagged_data[f\"Quantity_lag_{i}\"] = input_window[\"Quantity\"].shift(i)\n",
    "            lagged_data[f\"frequency_lag_{i}\"] = input_window[\"frequency\"].shift(i)\n",
    "\n",
    "        lagged_data = lagged_data.fillna(0)\n",
    "\n",
    "        trend_features = np.column_stack(\n",
    "            [\n",
    "                input_window[\"Amount\"].values,\n",
    "                input_window[\"Quantity\"].values,\n",
    "                input_window[\"frequency\"].values,\n",
    "                lagged_data.values,\n",
    "            ]\n",
    "        )\n",
    "        return trend_features\n",
    "\n",
    "    sequence_containers = {\n",
    "        \"temporal_sequences\": [],\n",
    "        \"trend_sequences\": [],\n",
    "        \"static_features\": [],\n",
    "        \"output_sequences\": [],\n",
    "    }\n",
    "\n",
    "    # Process sequences for each customer\n",
    "    for customer_id, customer_data in monthly_purchases.groupby(\"CustomerID\"):\n",
    "        customer_data = customer_data.sort_values(\"InvoiceDate\")\n",
    "        sequence_ranges = (\n",
    "            len(customer_data) - input_sequence_length - output_sequence_length + 1\n",
    "        )\n",
    "\n",
    "        country = customer_data[\"Country\"].iloc[0]\n",
    "\n",
    "        for i in range(sequence_ranges):\n",
    "            input_window = customer_data.iloc[i : i + input_sequence_length]\n",
    "            output_window = customer_data.iloc[\n",
    "                i\n",
    "                + input_sequence_length : i\n",
    "                + input_sequence_length\n",
    "                + output_sequence_length\n",
    "            ]\n",
    "\n",
    "            if (\n",
    "                len(input_window) == input_sequence_length\n",
    "                and len(output_window) == output_sequence_length\n",
    "            ):\n",
    "                temporal_features = prepare_temporal_features(input_window)\n",
    "                trend_features = prepare_trend_features(input_window)\n",
    "\n",
    "                sequence_containers[\"temporal_sequences\"].append(temporal_features)\n",
    "                sequence_containers[\"trend_sequences\"].append(trend_features)\n",
    "                sequence_containers[\"static_features\"].append(country)\n",
    "                sequence_containers[\"output_sequences\"].append(\n",
    "                    output_window[\"Amount\"].values\n",
    "                )\n",
    "\n",
    "    return {\n",
    "        \"temporal_sequences\": (\n",
    "            np.array(sequence_containers[\"temporal_sequences\"], dtype=np.float32)\n",
    "        ),\n",
    "        \"trend_sequences\": (\n",
    "            np.array(sequence_containers[\"trend_sequences\"], dtype=np.float32)\n",
    "        ),\n",
    "        \"static_features\": np.array(sequence_containers[\"static_features\"]),\n",
    "        \"output_sequences\": (\n",
    "            np.array(sequence_containers[\"output_sequences\"], dtype=np.float32)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Transform data with input and output sequences into a Output dictionary\n",
    "output = prepare_data_for_modeling(\n",
    "    df=transformed_data, input_sequence_length=6, output_sequence_length=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Scaling and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def robust_scale(data):\n",
    "    \"\"\"\n",
    "    Min-Max scaling function since standard deviation is high.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    scaled = (data - data_min) / (data_max - data_min)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def create_temporal_splits_with_scaling(\n",
    "    prepared_data: Dict[str, np.ndarray],\n",
    "    test_ratio: float = 0.2,\n",
    "    val_ratio: float = 0.2,\n",
    "):\n",
    "    total_sequences = len(prepared_data[\"trend_sequences\"])\n",
    "    # Calculate split points\n",
    "    test_size = int(total_sequences * test_ratio)\n",
    "    val_size = int(total_sequences * val_ratio)\n",
    "    train_size = total_sequences - (test_size + val_size)\n",
    "\n",
    "    # Scale trend sequences\n",
    "    trend_shape = prepared_data[\"trend_sequences\"].shape\n",
    "    scaled_trends = np.zeros_like(prepared_data[\"trend_sequences\"])\n",
    "\n",
    "    # Scale each feature independently\n",
    "    for i in range(trend_shape[-1]):\n",
    "        scaled_trends[..., i] = robust_scale(prepared_data[\"trend_sequences\"][..., i])\n",
    "    # Scale output sequences\n",
    "    scaled_outputs = robust_scale(prepared_data[\"output_sequences\"])\n",
    "\n",
    "    # Create splits\n",
    "    train_data = {\n",
    "        \"trend_sequences\": scaled_trends[:train_size],\n",
    "        \"temporal_sequences\": prepared_data[\"temporal_sequences\"][:train_size],\n",
    "        \"static_features\": prepared_data[\"static_features\"][:train_size],\n",
    "        \"output_sequences\": scaled_outputs[:train_size],\n",
    "    }\n",
    "\n",
    "    val_data = {\n",
    "        \"trend_sequences\": scaled_trends[train_size : train_size + val_size],\n",
    "        \"temporal_sequences\": prepared_data[\"temporal_sequences\"][\n",
    "            train_size : train_size + val_size\n",
    "        ],\n",
    "        \"static_features\": prepared_data[\"static_features\"][\n",
    "            train_size : train_size + val_size\n",
    "        ],\n",
    "        \"output_sequences\": scaled_outputs[train_size : train_size + val_size],\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        \"trend_sequences\": scaled_trends[train_size + val_size :],\n",
    "        \"temporal_sequences\": prepared_data[\"temporal_sequences\"][\n",
    "            train_size + val_size :\n",
    "        ],\n",
    "        \"static_features\": prepared_data[\"static_features\"][train_size + val_size :],\n",
    "        \"output_sequences\": scaled_outputs[train_size + val_size :],\n",
    "    }\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# Usage\n",
    "train_data, val_data, test_data = create_temporal_splits_with_scaling(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates RMSE, MAE and R\u00b2\n",
    "    \"\"\"\n",
    "    # Convert inputs to \"float32\"\n",
    "    y_true = ops.cast(y_true, dtype=\"float32\")\n",
    "    y_pred = ops.cast(y_pred, dtype=\"float32\")\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean(np.square(y_true - y_pred)))\n",
    "\n",
    "    # R\u00b2 (coefficient of determination)\n",
    "    ss_res = np.sum(np.square(y_true - y_pred))\n",
    "    ss_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    return {\"mae\": np.mean(np.abs(y_true - y_pred)), \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "\n",
    "def plot_lorenz_analysis(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plots Lorenz curves to show distribution of high and low value users\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays and flatten\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "\n",
    "    # Sort values in descending order (for high-value users analysis)\n",
    "    true_sorted = np.sort(-y_true)\n",
    "    pred_sorted = np.sort(-y_pred)\n",
    "\n",
    "    # Calculate cumulative sums\n",
    "    true_cumsum = np.cumsum(true_sorted)\n",
    "    pred_cumsum = np.cumsum(pred_sorted)\n",
    "\n",
    "    # Normalize to percentages\n",
    "    true_cumsum_pct = true_cumsum / true_cumsum[-1]\n",
    "    pred_cumsum_pct = pred_cumsum / pred_cumsum[-1]\n",
    "\n",
    "    # Generate percentiles for x-axis\n",
    "    percentiles = np.linspace(0, 1, len(y_true))\n",
    "\n",
    "    # Calculate Mutual Gini (area between curves)\n",
    "    mutual_gini = np.abs(\n",
    "        np.trapz(true_cumsum_pct, percentiles) - np.trapz(pred_cumsum_pct, percentiles)\n",
    "    )\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(percentiles, true_cumsum_pct, \"g-\", label=\"True Values\")\n",
    "    plt.plot(percentiles, pred_cumsum_pct, \"r-\", label=\"Predicted Values\")\n",
    "    plt.xlabel(\"Cumulative % of Users (Descending Order)\")\n",
    "    plt.ylabel(\"Cumulative % of LTV\")\n",
    "    plt.title(\"Lorenz Curves: True vs Predicted Values\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(f\"\\nMutual Gini: {mutual_gini:.4f} (lower is better)\")\n",
    "    plt.show()\n",
    "\n",
    "    return mutual_gini\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Hybrid Transformer / LSTM model architecture\n",
    "\n",
    "The hybrid nature of this model is particularly significant because it combines RNN's\n",
    "ability to handle sequential data with Transformer's attention mechanisms for capturing\n",
    "global patterns across countries and seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_hybrid_model(\n",
    "    input_sequence_length: int,\n",
    "    output_sequence_length: int,\n",
    "    num_countries: int,\n",
    "    d_model: int = 8,\n",
    "    num_heads: int = 4,\n",
    "):\n",
    "\n",
    "    keras.utils.set_random_seed(seed=42)\n",
    "\n",
    "    # Inputs\n",
    "    temporal_inputs = layers.Input(\n",
    "        shape=(input_sequence_length, 5), name=\"temporal_inputs\"\n",
    "    )\n",
    "    trend_inputs = layers.Input(shape=(input_sequence_length, 12), name=\"trend_inputs\")\n",
    "    country_inputs = layers.Input(\n",
    "        shape=(num_countries,), dtype=\"int32\", name=\"country_inputs\"\n",
    "    )\n",
    "\n",
    "    # Process country features\n",
    "    country_embedding = layers.Embedding(\n",
    "        input_dim=num_countries,\n",
    "        output_dim=d_model,\n",
    "        mask_zero=False,\n",
    "        name=\"country_embedding\",\n",
    "    )(\n",
    "        country_inputs\n",
    "    )  # Output shape: (batch_size, 1, d_model)\n",
    "\n",
    "    # Flatten the embedding output\n",
    "    country_embedding = layers.Flatten(name=\"flatten_country_embedding\")(\n",
    "        country_embedding\n",
    "    )\n",
    "\n",
    "    # Repeat the country embedding across timesteps\n",
    "    country_embedding_repeated = layers.RepeatVector(\n",
    "        input_sequence_length, name=\"repeat_country_embedding\"\n",
    "    )(country_embedding)\n",
    "\n",
    "    # Projection of temporal inputs to match Transformer dimensions\n",
    "    temporal_projection = layers.Dense(\n",
    "        d_model, activation=\"tanh\", name=\"temporal_projection\"\n",
    "    )(temporal_inputs)\n",
    "\n",
    "    # Combine all features\n",
    "    combined_features = layers.Concatenate()(\n",
    "        [temporal_projection, country_embedding_repeated]\n",
    "    )\n",
    "\n",
    "    transformer_output = combined_features\n",
    "    for _ in range(3):\n",
    "        transformer_output = TransformerEncoder(\n",
    "            intermediate_dim=16, num_heads=num_heads\n",
    "        )(transformer_output)\n",
    "\n",
    "    lstm_output = layers.LSTM(units=64, name=\"lstm_trend\")(trend_inputs)\n",
    "\n",
    "    transformer_flattened = layers.GlobalAveragePooling1D(name=\"flatten_transformer\")(\n",
    "        transformer_output\n",
    "    )\n",
    "    transformer_flattened = layers.Dense(1, activation=\"sigmoid\")(transformer_flattened)\n",
    "    # Concatenate flattened Transformer output with LSTM output\n",
    "    merged_features = layers.Concatenate(name=\"concatenate_transformer_lstm\")(\n",
    "        [transformer_flattened, lstm_output]\n",
    "    )\n",
    "    # Repeat the merged features to match the output sequence length\n",
    "    decoder_initial = layers.RepeatVector(\n",
    "        output_sequence_length, name=\"repeat_merged_features\"\n",
    "    )(merged_features)\n",
    "\n",
    "    decoder_lstm = layers.LSTM(\n",
    "        units=64,\n",
    "        return_sequences=True,\n",
    "        recurrent_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "    )(decoder_initial)\n",
    "\n",
    "    # Output Dense layer\n",
    "    output = layers.Dense(units=1, activation=\"linear\", name=\"output_dense\")(\n",
    "        decoder_lstm\n",
    "    )\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[temporal_inputs, trend_inputs, country_inputs], outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mse\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the hybrid model\n",
    "model = build_hybrid_model(\n",
    "    input_sequence_length=6,\n",
    "    output_sequence_length=6,\n",
    "    num_countries=len(np.unique(train_data[\"static_features\"])) + 1,\n",
    "    d_model=8,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "# Configure StringLookup\n",
    "label_encoder = layers.StringLookup(output_mode=\"one_hot\", num_oov_indices=1)\n",
    "\n",
    "# Adapt and encode\n",
    "label_encoder.adapt(train_data[\"static_features\"])\n",
    "\n",
    "train_static_encoded = label_encoder(train_data[\"static_features\"])\n",
    "val_static_encoded = label_encoder(val_data[\"static_features\"])\n",
    "test_static_encoded = label_encoder(test_data[\"static_features\"])\n",
    "\n",
    "# Convert sequences with proper type casting\n",
    "x_train_seq = np.asarray(train_data[\"trend_sequences\"]).astype(np.float32)\n",
    "x_val_seq = np.asarray(val_data[\"trend_sequences\"]).astype(np.float32)\n",
    "x_train_temporal = np.asarray(train_data[\"temporal_sequences\"]).astype(np.float32)\n",
    "x_val_temporal = np.asarray(val_data[\"temporal_sequences\"]).astype(np.float32)\n",
    "train_outputs = np.asarray(train_data[\"output_sequences\"]).astype(np.float32)\n",
    "val_outputs = np.asarray(val_data[\"output_sequences\"]).astype(np.float32)\n",
    "test_output = np.asarray(test_data[\"output_sequences\"]).astype(np.float32)\n",
    "# Training setup\n",
    "keras.utils.set_random_seed(seed=42)\n",
    "\n",
    "history = model.fit(\n",
    "    [x_train_temporal, x_train_seq, train_static_encoded],\n",
    "    train_outputs,\n",
    "    validation_data=(\n",
    "        [x_val_temporal, x_val_seq, val_static_encoded],\n",
    "        val_data[\"output_sequences\"].astype(np.float32),\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=30,\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(\n",
    "    [\n",
    "        test_data[\"temporal_sequences\"].astype(np.float32),\n",
    "        test_data[\"trend_sequences\"].astype(np.float32),\n",
    "        test_static_encoded,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate the predictions\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Calculate basic metrics\n",
    "hybrid_metrics = calculate_metrics(test_data[\"output_sequences\"], predictions)\n",
    "\n",
    "# Plot Lorenz curves and get Mutual Gini\n",
    "hybrid_mutual_gini = plot_lorenz_analysis(test_data[\"output_sequences\"], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "While LSTMs excel at sequence to sequence learning as demonstrated through the work of Sutskever, I., Vinyals,\n",
    "O., & Le, Q. V. (2014) Sequence to sequence learning with neural networks.\n",
    "The hybrid approach here enhances this foundation. The addition of attention mechanisms allows the model to adaptively\n",
    "focus on relevant temporal/geographical patterns while maintaining the LSTM's inherent strengths in sequence learning.\n",
    "This combination has proven especially effective for handling both periodic patterns and special events in time\n",
    "series forecasting from Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., & Zhang, W. (2021).\n",
    "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting."
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "customer_lifetime_value",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}