{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Class Attention Image Transformers with LayerScale\n",
    "\n",
    "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
    "**Date created:** 2022/09/19<br>\n",
    "**Last modified:** 2022/11/21<br>\n",
    "**Description:** Implementing an image transformer equipped with Class Attention and LayerScale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we implement the CaiT (Class-Attention in Image Transformers)\n",
    "proposed in [Going deeper with Image Transformers](https://arxiv.org/abs/2103.17239) by\n",
    "Touvron et al. Depth scaling, i.e. increasing the model depth for obtaining better\n",
    "performance and generalization has been quite successful for convolutional neural\n",
    "networks ([Tan et al.](https://arxiv.org/abs/1905.11946),\n",
    "[Doll√°r et al.](https://arxiv.org/abs/2103.06877), for example). But applying\n",
    "the same model scaling principles to\n",
    "Vision Transformers ([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)) doesn't\n",
    "translate equally well -- their performance gets saturated quickly with depth scaling.\n",
    "Note that one assumption here is that the underlying pre-training dataset is\n",
    "always kept fixed when performing model scaling.\n",
    "\n",
    "In the CaiT paper, the authors investigate this phenomenon and propose modifications to\n",
    "the vanilla ViT (Vision Transformers) architecture to mitigate this problem.\n",
    "\n",
    "The tutorial is structured like so:\n",
    "\n",
    "* Implementation of the individual blocks of CaiT\n",
    "* Collating all the blocks to create the CaiT model\n",
    "* Loading a pre-trained CaiT model\n",
    "* Obtaining prediction results\n",
    "* Visualization of the different attention layers of CaiT\n",
    "\n",
    "The readers are assumed to be familiar with Vision Transformers already. Here is\n",
    "an implementation of Vision Transformers in Keras:\n",
    "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import io\n",
    "import typing\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## The LayerScale layer\n",
    "\n",
    "We begin by implementing a **LayerScale** layer which is one of the two modifications\n",
    "proposed in the CaiT paper.\n",
    "\n",
    "When increasing the depth of the ViT models, they meet with optimization instability and\n",
    "eventually don't converge. The residual connections within each Transformer block\n",
    "introduce information bottleneck. When there is an increased amount of depth, this\n",
    "bottleneck can quickly explode and deviate the optimization pathway for the underlying\n",
    "model.\n",
    "\n",
    "The following equations denote where residual connections are added within a Transformer\n",
    "block:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://i.ibb.co/jWV5bFb/image.png\"/>\n",
    "</div>\n",
    "\n",
    "where, **SA** stands for self-attention, **FFN** stands for feed-forward network, and\n",
    "**eta** denotes the LayerNorm operator ([Ba et al.](https://arxiv.org/abs/1607.06450)).\n",
    "\n",
    "LayerScale is formally implemented like so:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://i.ibb.co/VYDWNn9/image.png\"/>\n",
    "</div>\n",
    "\n",
    "where, the lambdas are learnable parameters and are initialized with a very small value\n",
    "({0.1, 1e-5, 1e-6}). **diag** represents a diagonal matrix.\n",
    "\n",
    "Intuitively, LayerScale helps control the contribution of the residual branches. The\n",
    "learnable parameters of LayerScale are initialized to a small value to let the branches\n",
    "act like identity functions and then let them figure out the degrees of interactions\n",
    "during the training. The diagonal matrix additionally helps control the contributions\n",
    "of the individual dimensions of the residual inputs as it is applied on a per-channel\n",
    "basis.\n",
    "\n",
    "The practical implementation of LayerScale is simpler than it might sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"LayerScale as introduced in CaiT: https://arxiv.org/abs/2103.17239.\n",
    "\n",
    "    Args:\n",
    "        init_values (float): value to initialize the diagonal matrix of LayerScale.\n",
    "        projection_dim (int): projection dimension used in LayerScale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values: float, projection_dim: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(projection_dim,),\n",
    "            initializer=keras.initializers.Constant(init_values),\n",
    "        )\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        return x * self.gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Stochastic depth layer\n",
    "\n",
    "Since its introduction ([Huang et al.](https://arxiv.org/abs/1603.09382)), Stochastic\n",
    "Depth has become a favorite component in almost all modern neural network architectures.\n",
    "CaiT is no exception. Discussing Stochastic Depth is out of scope for this notebook. You\n",
    "can refer to [this resource](https://paperswithcode.com/method/stochastic-depth) in case\n",
    "you need a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    \"\"\"Stochastic Depth layer (https://arxiv.org/abs/1603.09382).\n",
    "\n",
    "    Reference:\n",
    "        https://github.com/rwightman/pytorch-image-models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob: float, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (ops.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n",
    "            random_tensor = keep_prob + ops.random.uniform(\n",
    "                shape, minval=0, maxval=1, seed=self.seed_generator\n",
    "            )\n",
    "            random_tensor = ops.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Class attention\n",
    "\n",
    "The vanilla ViT uses self-attention (SA) layers for modelling how the image patches and\n",
    "the _learnable_ CLS token interact with each other. The CaiT authors propose to decouple\n",
    "the attention layers responsible for attending to the image patches and the CLS tokens.\n",
    "\n",
    "When using ViTs for any discriminative tasks (classification, for example), we usually\n",
    "take the representations belonging to the CLS token and then pass them to the\n",
    "task-specific heads. This is as opposed to using something like global average pooling as\n",
    "is typically done in convolutional neural networks.\n",
    "\n",
    "The interactions between the CLS token and other image patches are processed uniformly\n",
    "through self-attention layers. As the CaiT authors point out, this setup has got an\n",
    "entangled effect. On one hand, the self-attention layers are responsible for modelling\n",
    "the image patches. On the other hand, they're also responsible for summarizing the\n",
    "modelled information via the CLS token so that it's useful for the learning objective.\n",
    "\n",
    "To help disentangle these two things, the authors propose to:\n",
    "\n",
    "* Introduce the CLS token at a later stage in the network.\n",
    "* Model the interaction between the CLS token and the representations related to the\n",
    "image patches through a separate set of attention layers. The authors call this **Class\n",
    "Attention** (CA).\n",
    "\n",
    "The figure below (taken from the original paper) depicts this idea:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://i.imgur.com/cxeooHr.png\"/ width=350>\n",
    "</div>\n",
    "\n",
    "This is achieved by treating the CLS token embeddings as the queries in the CA layers.\n",
    "CLS token embeddings and the image patch embeddings are fed as keys as well values.\n",
    "\n",
    "**Note** that \"embeddings\" and \"representations\" have been used interchangeably here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ClassAttention(layers.Layer):\n",
    "    \"\"\"Class attention as proposed in CaiT: https://arxiv.org/abs/2103.17239.\n",
    "\n",
    "    Args:\n",
    "        projection_dim (int): projection dimension for the query, key, and value\n",
    "            of attention.\n",
    "        num_heads (int): number of attention heads.\n",
    "        dropout_rate (float): dropout rate to be used for dropout in the attention\n",
    "            scores as well as the final projected outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, projection_dim: int, num_heads: int, dropout_rate: float, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        head_dim = projection_dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.q = layers.Dense(projection_dim)\n",
    "        self.k = layers.Dense(projection_dim)\n",
    "        self.v = layers.Dense(projection_dim)\n",
    "        self.attn_drop = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(projection_dim)\n",
    "        self.proj_drop = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        batch_size, num_patches, num_channels = (\n",
    "            ops.shape(x)[0],\n",
    "            ops.shape(x)[1],\n",
    "            ops.shape(x)[2],\n",
    "        )\n",
    "\n",
    "        # Query projection. `cls_token` embeddings are queries.\n",
    "        q = ops.expand_dims(self.q(x[:, 0]), axis=1)\n",
    "        q = ops.reshape(\n",
    "            q, (batch_size, 1, self.num_heads, num_channels // self.num_heads)\n",
    "        )  # Shape: (batch_size, 1, num_heads, dimension_per_head)\n",
    "        q = ops.transpose(q, axes=[0, 2, 1, 3])\n",
    "        scale = ops.cast(self.scale, dtype=q.dtype)\n",
    "        q = q * scale\n",
    "\n",
    "        # Key projection. Patch embeddings as well the cls embedding are used as keys.\n",
    "        k = self.k(x)\n",
    "        k = ops.reshape(\n",
    "            k, (batch_size, num_patches, self.num_heads, num_channels // self.num_heads)\n",
    "        )  # Shape: (batch_size, num_tokens, num_heads, dimension_per_head)\n",
    "        k = ops.transpose(k, axes=[0, 2, 3, 1])\n",
    "\n",
    "        # Value projection. Patch embeddings as well the cls embedding are used as values.\n",
    "        v = self.v(x)\n",
    "        v = ops.reshape(\n",
    "            v, (batch_size, num_patches, self.num_heads, num_channels // self.num_heads)\n",
    "        )\n",
    "        v = ops.transpose(v, axes=[0, 2, 1, 3])\n",
    "\n",
    "        # Calculate attention scores between cls_token embedding and patch embeddings.\n",
    "        attn = ops.matmul(q, k)\n",
    "        attn = ops.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "\n",
    "        x_cls = ops.matmul(attn, v)\n",
    "        x_cls = ops.transpose(x_cls, axes=[0, 2, 1, 3])\n",
    "        x_cls = ops.reshape(x_cls, (batch_size, 1, num_channels))\n",
    "        x_cls = self.proj(x_cls)\n",
    "        x_cls = self.proj_drop(x_cls, training=training)\n",
    "\n",
    "        return x_cls, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Talking Head Attention\n",
    "\n",
    "The CaiT authors use the Talking Head attention\n",
    "([Shazeer et al.](https://arxiv.org/abs/2003.02436))\n",
    "instead of the vanilla scaled dot-product multi-head attention used in\n",
    "the original Transformer paper\n",
    "([Vaswani et al.](https://papers.nips.cc/paper/7181-attention-is-all-you-need)).\n",
    "They introduce two linear projections before and after the softmax\n",
    "operations for obtaining better results.\n",
    "\n",
    "For a more rigorous treatment of the Talking Head attention and the vanilla attention\n",
    "mechanisms, please refer to their respective papers (linked above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TalkingHeadAttention(layers.Layer):\n",
    "    \"\"\"Talking-head attention as proposed in CaiT: https://arxiv.org/abs/2003.02436.\n",
    "\n",
    "    Args:\n",
    "        projection_dim (int): projection dimension for the query, key, and value\n",
    "            of attention.\n",
    "        num_heads (int): number of attention heads.\n",
    "        dropout_rate (float): dropout rate to be used for dropout in the attention\n",
    "            scores as well as the final projected outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, projection_dim: int, num_heads: int, dropout_rate: float, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        head_dim = projection_dim // self.num_heads\n",
    "\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = layers.Dense(projection_dim * 3)\n",
    "        self.attn_drop = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.proj = layers.Dense(projection_dim)\n",
    "\n",
    "        self.proj_l = layers.Dense(self.num_heads)\n",
    "        self.proj_w = layers.Dense(self.num_heads)\n",
    "\n",
    "        self.proj_drop = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B, N, C = ops.shape(x)[0], ops.shape(x)[1], ops.shape(x)[2]\n",
    "\n",
    "        # Project the inputs all at once.\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # Reshape the projected output so that they're segregated in terms of\n",
    "        # query, key, and value projections.\n",
    "        qkv = ops.reshape(qkv, (B, N, 3, self.num_heads, C // self.num_heads))\n",
    "\n",
    "        # Transpose so that the `num_heads` becomes the leading dimensions.\n",
    "        # Helps to better segregate the representation sub-spaces.\n",
    "        qkv = ops.transpose(qkv, axes=[2, 0, 3, 1, 4])\n",
    "        scale = ops.cast(self.scale, dtype=qkv.dtype)\n",
    "        q, k, v = qkv[0] * scale, qkv[1], qkv[2]\n",
    "\n",
    "        # Obtain the raw attention scores.\n",
    "        attn = ops.matmul(q, ops.transpose(k, axes=[0, 1, 3, 2]))\n",
    "\n",
    "        # Linear projection of the similarities between the query and key projections.\n",
    "        attn = self.proj_l(ops.transpose(attn, axes=[0, 2, 3, 1]))\n",
    "\n",
    "        # Normalize the attention scores.\n",
    "        attn = ops.transpose(attn, axes=[0, 3, 1, 2])\n",
    "        attn = ops.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        # Linear projection on the softmaxed scores.\n",
    "        attn = self.proj_w(ops.transpose(attn, axes=[0, 2, 3, 1]))\n",
    "        attn = ops.transpose(attn, axes=[0, 3, 1, 2])\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "\n",
    "        # Final set of projections as done in the vanilla attention mechanism.\n",
    "        x = ops.matmul(attn, v)\n",
    "        x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
    "        x = ops.reshape(x, (B, N, C))\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x, training=training)\n",
    "\n",
    "        return x, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Feed-forward Network\n",
    "\n",
    "Next, we implement the feed-forward network which is one of the components within a\n",
    "Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, dropout_rate: float, hidden_units: typing.List[int]):\n",
    "    \"\"\"FFN for a Transformer block.\"\"\"\n",
    "    for idx, units in enumerate(hidden_units):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=ops.nn.gelu if idx == 0 else None,\n",
    "            bias_initializer=keras.initializers.RandomNormal(stddev=1e-6),\n",
    "        )(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Other blocks\n",
    "\n",
    "In the next two cells, we implement the remaining blocks as standalone functions:\n",
    "\n",
    "* `LayerScaleBlockClassAttention()` which returns a `keras.Model`. It is a Transformer block\n",
    "equipped with Class Attention, LayerScale, and Stochastic Depth. It operates on the CLS\n",
    "embeddings and the image patch embeddings.\n",
    "* `LayerScaleBlock()` which returns a `keras.model`. It is also a Transformer block that\n",
    "operates only on the embeddings of the image patches. It is equipped with LayerScale and\n",
    "Stochastic Depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def LayerScaleBlockClassAttention(\n",
    "    projection_dim: int,\n",
    "    num_heads: int,\n",
    "    layer_norm_eps: float,\n",
    "    init_values: float,\n",
    "    mlp_units: typing.List[int],\n",
    "    dropout_rate: float,\n",
    "    sd_prob: float,\n",
    "    name: str,\n",
    "):\n",
    "    \"\"\"Pre-norm transformer block meant to be applied to the embeddings of the\n",
    "    cls token and the embeddings of image patches.\n",
    "\n",
    "    Includes LayerScale and Stochastic Depth.\n",
    "\n",
    "    Args:\n",
    "        projection_dim (int): projection dimension to be used in the\n",
    "            Transformer blocks and patch projection layer.\n",
    "        num_heads (int): number of attention heads.\n",
    "        layer_norm_eps (float): epsilon to be used for Layer Normalization.\n",
    "        init_values (float): initial value for the diagonal matrix used in LayerScale.\n",
    "        mlp_units (List[int]): dimensions of the feed-forward network used in\n",
    "            the Transformer blocks.\n",
    "        dropout_rate (float): dropout rate to be used for dropout in the attention\n",
    "            scores as well as the final projected outputs.\n",
    "        sd_prob (float): stochastic depth rate.\n",
    "        name (str): a name identifier for the block.\n",
    "\n",
    "    Returns:\n",
    "        A keras.Model instance.\n",
    "    \"\"\"\n",
    "    x = keras.Input((None, projection_dim))\n",
    "    x_cls = keras.Input((None, projection_dim))\n",
    "    inputs = keras.layers.Concatenate(axis=1)([x_cls, x])\n",
    "\n",
    "    # Class attention (CA).\n",
    "    x1 = layers.LayerNormalization(epsilon=layer_norm_eps)(inputs)\n",
    "    attn_output, attn_scores = ClassAttention(projection_dim, num_heads, dropout_rate)(\n",
    "        x1\n",
    "    )\n",
    "    attn_output = (\n",
    "        LayerScale(init_values, projection_dim)(attn_output)\n",
    "        if init_values\n",
    "        else attn_output\n",
    "    )\n",
    "    attn_output = StochasticDepth(sd_prob)(attn_output) if sd_prob else attn_output\n",
    "    x2 = keras.layers.Add()([x_cls, attn_output])\n",
    "\n",
    "    # FFN.\n",
    "    x3 = layers.LayerNormalization(epsilon=layer_norm_eps)(x2)\n",
    "    x4 = mlp(x3, hidden_units=mlp_units, dropout_rate=dropout_rate)\n",
    "    x4 = LayerScale(init_values, projection_dim)(x4) if init_values else x4\n",
    "    x4 = StochasticDepth(sd_prob)(x4) if sd_prob else x4\n",
    "    outputs = keras.layers.Add()([x2, x4])\n",
    "\n",
    "    return keras.Model([x, x_cls], [outputs, attn_scores], name=name)\n",
    "\n",
    "\n",
    "def LayerScaleBlock(\n",
    "    projection_dim: int,\n",
    "    num_heads: int,\n",
    "    layer_norm_eps: float,\n",
    "    init_values: float,\n",
    "    mlp_units: typing.List[int],\n",
    "    dropout_rate: float,\n",
    "    sd_prob: float,\n",
    "    name: str,\n",
    "):\n",
    "    \"\"\"Pre-norm transformer block meant to be applied to the embeddings of the\n",
    "    image patches.\n",
    "\n",
    "    Includes LayerScale and Stochastic Depth.\n",
    "\n",
    "        Args:\n",
    "            projection_dim (int): projection dimension to be used in the\n",
    "                Transformer blocks and patch projection layer.\n",
    "            num_heads (int): number of attention heads.\n",
    "            layer_norm_eps (float): epsilon to be used for Layer Normalization.\n",
    "            init_values (float): initial value for the diagonal matrix used in LayerScale.\n",
    "            mlp_units (List[int]): dimensions of the feed-forward network used in\n",
    "                the Transformer blocks.\n",
    "            dropout_rate (float): dropout rate to be used for dropout in the attention\n",
    "                scores as well as the final projected outputs.\n",
    "            sd_prob (float): stochastic depth rate.\n",
    "            name (str): a name identifier for the block.\n",
    "\n",
    "    Returns:\n",
    "        A keras.Model instance.\n",
    "    \"\"\"\n",
    "    encoded_patches = keras.Input((None, projection_dim))\n",
    "\n",
    "    # Self-attention.\n",
    "    x1 = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    attn_output, attn_scores = TalkingHeadAttention(\n",
    "        projection_dim, num_heads, dropout_rate\n",
    "    )(x1)\n",
    "    attn_output = (\n",
    "        LayerScale(init_values, projection_dim)(attn_output)\n",
    "        if init_values\n",
    "        else attn_output\n",
    "    )\n",
    "    attn_output = StochasticDepth(sd_prob)(attn_output) if sd_prob else attn_output\n",
    "    x2 = layers.Add()([encoded_patches, attn_output])\n",
    "\n",
    "    # FFN.\n",
    "    x3 = layers.LayerNormalization(epsilon=layer_norm_eps)(x2)\n",
    "    x4 = mlp(x3, hidden_units=mlp_units, dropout_rate=dropout_rate)\n",
    "    x4 = LayerScale(init_values, projection_dim)(x4) if init_values else x4\n",
    "    x4 = StochasticDepth(sd_prob)(x4) if sd_prob else x4\n",
    "    outputs = layers.Add()([x2, x4])\n",
    "\n",
    "    return keras.Model(encoded_patches, [outputs, attn_scores], name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Given all these blocks, we are now ready to collate them into the final CaiT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Putting the pieces together: The CaiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CaiT(keras.Model):\n",
    "    \"\"\"CaiT model.\n",
    "\n",
    "    Args:\n",
    "        projection_dim (int): projection dimension to be used in the\n",
    "            Transformer blocks and patch projection layer.\n",
    "        patch_size (int): patch size of the input images.\n",
    "        num_patches (int): number of patches after extracting the image patches.\n",
    "        init_values (float): initial value for the diagonal matrix used in LayerScale.\n",
    "        mlp_units: (List[int]): dimensions of the feed-forward network used in\n",
    "            the Transformer blocks.\n",
    "        sa_ffn_layers (int): number of self-attention Transformer blocks.\n",
    "        ca_ffn_layers (int): number of class-attention Transformer blocks.\n",
    "        num_heads (int): number of attention heads.\n",
    "        layer_norm_eps (float): epsilon to be used for Layer Normalization.\n",
    "        dropout_rate (float): dropout rate to be used for dropout in the attention\n",
    "            scores as well as the final projected outputs.\n",
    "        sd_prob (float): stochastic depth rate.\n",
    "        global_pool (str): denotes how to pool the representations coming out of\n",
    "            the final Transformer block.\n",
    "        pre_logits (bool): if set to True then don't add a classification head.\n",
    "        num_classes (int): number of classes to construct the final classification\n",
    "            layer with.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        projection_dim: int,\n",
    "        patch_size: int,\n",
    "        num_patches: int,\n",
    "        init_values: float,\n",
    "        mlp_units: typing.List[int],\n",
    "        sa_ffn_layers: int,\n",
    "        ca_ffn_layers: int,\n",
    "        num_heads: int,\n",
    "        layer_norm_eps: float,\n",
    "        dropout_rate: float,\n",
    "        sd_prob: float,\n",
    "        global_pool: str,\n",
    "        pre_logits: bool,\n",
    "        num_classes: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if global_pool not in [\"token\", \"avg\"]:\n",
    "            raise ValueError(\n",
    "                'Invalid value received for `global_pool`, should be either `\"token\"` or `\"avg\"`.'\n",
    "            )\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Responsible for patchifying the input images and the linearly projecting them.\n",
    "        self.projection = keras.Sequential(\n",
    "            [\n",
    "                layers.Conv2D(\n",
    "                    filters=projection_dim,\n",
    "                    kernel_size=(patch_size, patch_size),\n",
    "                    strides=(patch_size, patch_size),\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv_projection\",\n",
    "                    kernel_initializer=\"lecun_normal\",\n",
    "                ),\n",
    "                layers.Reshape(\n",
    "                    target_shape=(-1, projection_dim),\n",
    "                    name=\"flatten_projection\",\n",
    "                ),\n",
    "            ],\n",
    "            name=\"projection\",\n",
    "        )\n",
    "\n",
    "        # CLS token and the positional embeddings.\n",
    "        self.cls_token = self.add_weight(\n",
    "            shape=(1, 1, projection_dim), initializer=\"zeros\"\n",
    "        )\n",
    "        self.pos_embed = self.add_weight(\n",
    "            shape=(1, num_patches, projection_dim), initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "        # Projection dropout.\n",
    "        self.pos_drop = layers.Dropout(dropout_rate, name=\"projection_dropout\")\n",
    "\n",
    "        # Stochastic depth schedule.\n",
    "        dpr = [sd_prob for _ in range(sa_ffn_layers)]\n",
    "\n",
    "        # Self-attention (SA) Transformer blocks operating only on the image patch\n",
    "        # embeddings.\n",
    "        self.blocks = [\n",
    "            LayerScaleBlock(\n",
    "                projection_dim=projection_dim,\n",
    "                num_heads=num_heads,\n",
    "                layer_norm_eps=layer_norm_eps,\n",
    "                init_values=init_values,\n",
    "                mlp_units=mlp_units,\n",
    "                dropout_rate=dropout_rate,\n",
    "                sd_prob=dpr[i],\n",
    "                name=f\"sa_ffn_block_{i}\",\n",
    "            )\n",
    "            for i in range(sa_ffn_layers)\n",
    "        ]\n",
    "\n",
    "        # Class Attention (CA) Transformer blocks operating on the CLS token and image patch\n",
    "        # embeddings.\n",
    "        self.blocks_token_only = [\n",
    "            LayerScaleBlockClassAttention(\n",
    "                projection_dim=projection_dim,\n",
    "                num_heads=num_heads,\n",
    "                layer_norm_eps=layer_norm_eps,\n",
    "                init_values=init_values,\n",
    "                mlp_units=mlp_units,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f\"ca_ffn_block_{i}\",\n",
    "                sd_prob=0.0,  # No Stochastic Depth in the class attention layers.\n",
    "            )\n",
    "            for i in range(ca_ffn_layers)\n",
    "        ]\n",
    "\n",
    "        # Pre-classification layer normalization.\n",
    "        self.norm = layers.LayerNormalization(epsilon=layer_norm_eps, name=\"head_norm\")\n",
    "\n",
    "        # Representation pooling for classification head.\n",
    "        self.global_pool = global_pool\n",
    "\n",
    "        # Classification head.\n",
    "        self.pre_logits = pre_logits\n",
    "        self.num_classes = num_classes\n",
    "        if not pre_logits:\n",
    "            self.head = layers.Dense(num_classes, name=\"classification_head\")\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Notice how CLS token is not added here.\n",
    "        x = self.projection(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # SA+FFN layers.\n",
    "        sa_ffn_attn = {}\n",
    "        for blk in self.blocks:\n",
    "            x, attn_scores = blk(x)\n",
    "            sa_ffn_attn[f\"{blk.name}_att\"] = attn_scores\n",
    "\n",
    "        # CA+FFN layers.\n",
    "        ca_ffn_attn = {}\n",
    "        cls_tokens = ops.tile(self.cls_token, (ops.shape(x)[0], 1, 1))\n",
    "        for blk in self.blocks_token_only:\n",
    "            cls_tokens, attn_scores = blk([x, cls_tokens])\n",
    "            ca_ffn_attn[f\"{blk.name}_att\"] = attn_scores\n",
    "\n",
    "        x = ops.concatenate([cls_tokens, x], axis=1)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Always return the attention scores from the SA+FFN and CA+FFN layers\n",
    "        # for convenience.\n",
    "        if self.global_pool:\n",
    "            x = (\n",
    "                ops.reduce_mean(x[:, 1:], axis=1)\n",
    "                if self.global_pool == \"avg\"\n",
    "                else x[:, 0]\n",
    "            )\n",
    "        return (\n",
    "            (x, sa_ffn_attn, ca_ffn_attn)\n",
    "            if self.pre_logits\n",
    "            else (self.head(x), sa_ffn_attn, ca_ffn_attn)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Having the SA and CA layers segregated this way helps the model to focus on underlying\n",
    "objectives more concretely:\n",
    "\n",
    "* model dependencies in between the image patches\n",
    "* summarize the information from the image patches in a CLS token that can be used for\n",
    "the task at hand\n",
    "\n",
    "Now that we have defined the CaiT model, it's time to test it. We will start by defining\n",
    "a model configuration that will be passed to our `CaiT` class for initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Defining Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_config(\n",
    "    image_size: int = 224,\n",
    "    patch_size: int = 16,\n",
    "    projection_dim: int = 192,\n",
    "    sa_ffn_layers: int = 24,\n",
    "    ca_ffn_layers: int = 2,\n",
    "    num_heads: int = 4,\n",
    "    mlp_ratio: int = 4,\n",
    "    layer_norm_eps=1e-6,\n",
    "    init_values: float = 1e-5,\n",
    "    dropout_rate: float = 0.0,\n",
    "    sd_prob: float = 0.0,\n",
    "    global_pool: str = \"token\",\n",
    "    pre_logits: bool = False,\n",
    "    num_classes: int = 1000,\n",
    ") -> typing.Dict:\n",
    "    \"\"\"Default configuration for CaiT models (cait_xxs24_224).\n",
    "\n",
    "    Reference:\n",
    "        https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/cait.py\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "\n",
    "    # Patchification and projection.\n",
    "    config[\"patch_size\"] = patch_size\n",
    "    config[\"num_patches\"] = (image_size // patch_size) ** 2\n",
    "\n",
    "    # LayerScale.\n",
    "    config[\"init_values\"] = init_values\n",
    "\n",
    "    # Dropout and Stochastic Depth.\n",
    "    config[\"dropout_rate\"] = dropout_rate\n",
    "    config[\"sd_prob\"] = sd_prob\n",
    "\n",
    "    # Shared across different blocks and layers.\n",
    "    config[\"layer_norm_eps\"] = layer_norm_eps\n",
    "    config[\"projection_dim\"] = projection_dim\n",
    "    config[\"mlp_units\"] = [\n",
    "        projection_dim * mlp_ratio,\n",
    "        projection_dim,\n",
    "    ]\n",
    "\n",
    "    # Attention layers.\n",
    "    config[\"num_heads\"] = num_heads\n",
    "    config[\"sa_ffn_layers\"] = sa_ffn_layers\n",
    "    config[\"ca_ffn_layers\"] = ca_ffn_layers\n",
    "\n",
    "    # Representation pooling and task specific parameters.\n",
    "    config[\"global_pool\"] = global_pool\n",
    "    config[\"pre_logits\"] = pre_logits\n",
    "    config[\"num_classes\"] = num_classes\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Most of the configuration variables should sound familiar to you if you already know the\n",
    "ViT architecture. Point of focus is given to `sa_ffn_layers` and `ca_ffn_layers` that\n",
    "control the number of SA-Transformer blocks and CA-Transformer blocks. You can easily\n",
    "amend this `get_config()` method to instantiate a CaiT model for your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "num_channels = 3\n",
    "batch_size = 2\n",
    "\n",
    "config = get_config()\n",
    "cait_xxs24_224 = CaiT(**config)\n",
    "\n",
    "dummy_inputs = ops.ones((batch_size, image_size, image_size, num_channels))\n",
    "_ = cait_xxs24_224(dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can successfully perform inference with the model. But what about implementation\n",
    "correctness? There are many ways to verify it:\n",
    "\n",
    "* Obtain the performance of the model (given it's been populated with the pre-trained\n",
    "parameters) on the ImageNet-1k validation set (as the pretraining dataset was\n",
    "ImageNet-1k).\n",
    "* Fine-tune the model on a different dataset.\n",
    "\n",
    "In order to verify that, we will load another instance of the same model that has been\n",
    "already populated with the pre-trained parameters. Please refer to\n",
    "[this repository](https://github.com/sayakpaul/cait-tf)\n",
    "(developed by the author of this notebook) for more details.\n",
    "Additionally, the repository provides code to verify model performance on the\n",
    "[ImageNet-1k validation set](https://github.com/sayakpaul/cait-tf/tree/main/i1k_eval)\n",
    "as well as\n",
    "[fine-tuning](https://github.com/sayakpaul/cait-tf/blob/main/notebooks/finetune.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model_gcs_path = \"gs://kaggle-tfhub-models-uncompressed/tfhub-modules/sayakpaul/cait_xxs24_224/1/uncompressed\"\n",
    "pretrained_model = keras.Sequential(\n",
    "    [keras.layers.TFSMLayer(model_gcs_path, call_endpoint=\"serving_default\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Inference utilities\n",
    "\n",
    "In the next couple of cells, we develop preprocessing utilities needed to run inference\n",
    "with the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# The preprocessing transformations include center cropping, and normalizing\n",
    "# the pixel values with the ImageNet-1k training stats (mean and standard deviation).\n",
    "crop_layer = keras.layers.CenterCrop(image_size, image_size)\n",
    "norm_layer = keras.layers.Normalization(\n",
    "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_image(image, size=image_size):\n",
    "    image = np.array(image)\n",
    "    image_resized = ops.expand_dims(image, 0)\n",
    "    resize_size = int((256 / image_size) * size)\n",
    "    image_resized = ops.image.resize(\n",
    "        image_resized, (resize_size, resize_size), interpolation=\"bicubic\"\n",
    "    )\n",
    "    image_resized = crop_layer(image_resized)\n",
    "    return norm_layer(image_resized).numpy()\n",
    "\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    image_bytes = io.BytesIO(urlopen(url).read())\n",
    "    image = PIL.Image.open(image_bytes)\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    return image, preprocessed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, we retrieve the ImageNet-1k labels and load them as the model we're\n",
    "loading was pretrained on the ImageNet-1k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# ImageNet-1k class labels.\n",
    "imagenet_labels = (\n",
    "    \"https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\"\n",
    ")\n",
    "label_path = keras.utils.get_file(origin=imagenet_labels)\n",
    "\n",
    "with open(label_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "imagenet_labels = [line.rstrip() for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "img_url = \"https://i.imgur.com/ErgfLTn.jpg\"\n",
    "image, preprocessed_image = load_image_from_url(img_url)\n",
    "\n",
    "# https://unsplash.com/photos/Ho93gVTRWW8\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Obtain Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "outputs = pretrained_model.predict(preprocessed_image)\n",
    "logits = outputs[\"output_1\"]\n",
    "ca_ffn_block_0_att = outputs[\"output_3_ca_ffn_block_0_att\"]\n",
    "ca_ffn_block_1_att = outputs[\"output_3_ca_ffn_block_1_att\"]\n",
    "\n",
    "predicted_label = imagenet_labels[int(np.argmax(logits))]\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we have obtained the predictions (which appear to be as expected), we can\n",
    "further extend our investigation. Following the CaiT authors, we can investigate the\n",
    "attention scores from the attention layers. This helps us to get deeper insights into the\n",
    "modifications introduced in the CaiT paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualizing the Attention Layers\n",
    "\n",
    "We start by inspecting the shape of the attention weights returned by a Class Attention\n",
    "layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# (batch_size, nb_attention_heads, num_cls_token, seq_length)\n",
    "print(\"Shape of the attention scores from a class attention block:\")\n",
    "print(ca_ffn_block_0_att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The shape denotes we have got attention weights for each of the individual attention\n",
    "heads. They quantify the information about how the CLS token is related to itself and the\n",
    "rest of the image patches.\n",
    "\n",
    "Next, we write a utility to:\n",
    "\n",
    "* Visualize what the individual attention heads in the Class Attention layers are\n",
    "focusing on. This helps us to get an idea of how the _spatial-class relationship_ is\n",
    "induced in the CaiT model.\n",
    "* Obtain a saliency map from the first Class Attention layer that helps to understand how\n",
    "CA layer aggregates information from the region(s) of interest in the images.\n",
    "\n",
    "This utility is referred from Figures 6 and 7 of the original\n",
    "[CaiT paper](https://arxiv.org/abs/2103.17239). This is also a part of\n",
    "[this notebook](https://github.com/sayakpaul/cait-tf/blob/main/notebooks/classification.ipynb)\n",
    "(developed by the author of this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://github.com/facebookresearch/dino/blob/main/visualize_attention.py\n",
    "\n",
    "patch_size = 16\n",
    "\n",
    "\n",
    "def get_cls_attention_map(\n",
    "    attention_scores,\n",
    "    return_saliency=False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns attention scores from a particular attention block.\n",
    "\n",
    "    Args:\n",
    "        attention_scores: the attention scores from the attention block to\n",
    "            visualize.\n",
    "        return_saliency: a boolean flag if set to True also returns the salient\n",
    "            representations of the attention block.\n",
    "    \"\"\"\n",
    "    w_featmap = preprocessed_image.shape[2] // patch_size\n",
    "    h_featmap = preprocessed_image.shape[1] // patch_size\n",
    "\n",
    "    nh = attention_scores.shape[1]  # Number of attention heads.\n",
    "\n",
    "    # Taking the representations from CLS token.\n",
    "    attentions = attention_scores[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "    # Reshape the attention scores to resemble mini patches.\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "\n",
    "    if not return_saliency:\n",
    "        attentions = attentions.transpose((1, 2, 0))\n",
    "\n",
    "    else:\n",
    "        attentions = np.mean(attentions, axis=0)\n",
    "        attentions = (attentions - attentions.min()) / (\n",
    "            attentions.max() - attentions.min()\n",
    "        )\n",
    "        attentions = np.expand_dims(attentions, -1)\n",
    "\n",
    "    # Resize the attention patches to 224x224 (224: 14x16)\n",
    "    attentions = ops.image.resize(\n",
    "        attentions,\n",
    "        size=(h_featmap * patch_size, w_featmap * patch_size),\n",
    "        interpolation=\"bicubic\",\n",
    "    )\n",
    "\n",
    "    return attentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the first CA layer, we notice that the model is focusing solely on the region of\n",
    "interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "attentions_ca_block_0 = get_cls_attention_map(ca_ffn_block_0_att)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(13, 13))\n",
    "img_count = 0\n",
    "\n",
    "for i in range(attentions_ca_block_0.shape[-1]):\n",
    "    if img_count < attentions_ca_block_0.shape[-1]:\n",
    "        axes[i].imshow(attentions_ca_block_0[:, :, img_count])\n",
    "        axes[i].title.set_text(f\"Attention head: {img_count}\")\n",
    "        axes[i].axis(\"off\")\n",
    "        img_count += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Whereas in the second CA layer, the model is trying to focus more on the context that\n",
    "contains discriminative signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "attentions_ca_block_1 = get_cls_attention_map(ca_ffn_block_1_att)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(13, 13))\n",
    "img_count = 0\n",
    "\n",
    "for i in range(attentions_ca_block_1.shape[-1]):\n",
    "    if img_count < attentions_ca_block_1.shape[-1]:\n",
    "        axes[i].imshow(attentions_ca_block_1[:, :, img_count])\n",
    "        axes[i].title.set_text(f\"Attention head: {img_count}\")\n",
    "        axes[i].axis(\"off\")\n",
    "        img_count += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, we obtain the saliency map for the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "saliency_attention = get_cls_attention_map(ca_ffn_block_0_att, return_saliency=True)\n",
    "\n",
    "image = np.array(image)\n",
    "image_resized = ops.expand_dims(image, 0)\n",
    "resize_size = int((256 / 224) * image_size)\n",
    "image_resized = ops.image.resize(\n",
    "    image_resized, (resize_size, resize_size), interpolation=\"bicubic\"\n",
    ")\n",
    "image_resized = crop_layer(image_resized)\n",
    "\n",
    "plt.imshow(image_resized.numpy().squeeze().astype(\"int32\"))\n",
    "plt.imshow(saliency_attention.numpy().squeeze(), cmap=\"cividis\", alpha=0.9)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented the CaiT model. It shows how to mitigate the issues in\n",
    "ViTs when trying scale their depth while keeping the pretraining dataset fixed. I hope\n",
    "the additional visualizations provided in the notebook spark excitement in the community\n",
    "and people develop interesting methods to probe what models like ViT learn.\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "Thanks to the ML Developer Programs team at Google providing Google Cloud Platform\n",
    "support."
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "cait",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
