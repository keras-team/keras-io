{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# MoE for MNIST\n",
    "\n",
    "**Author:** [Damoon Shahhosseini](https://www.linkedin.com/in/damoonsh/)<br>\n",
    "**Date created:** 2015/06/19<br>\n",
    "**Last modified:** 2020/04/21<br>\n",
    "**Description:** Showcasing concepts relates to Mixture of Experts (MoE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this example, we implement an adaptation of the Mixture of Experts (MoE) architecture\n",
    "([Shazeer et al.](https://arxiv.org/abs/1701.06538)).\n",
    "The idea is to use conditional computation to increases model capacity without increasing computation.\n",
    "Experts are identical blocks within a layer where each are trained to specialize in different parts of the input space.\n",
    "At each forward pass, a gating network selects a subset of experts to apply to the input.\n",
    "\n",
    "The components to implement are:\n",
    "- Gating network: A dense layer that outputs a probability distribution over the experts.\n",
    "- MoE layer: A layer that applies a different expert to each input in the batch. And a loss function that ensures specialization among the experts.\n",
    "- Model: A simple model that uses the MoE layer.\n",
    "\n",
    "In this example, we will first implement a linear MoE layer and then a CNN-based MoE layer. Lastly we will combine the two using an abstract implementation to showcase its capacties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "NUM_EXPERTS = 5\n",
    "TOP_K = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Base architecture\n",
    "\n",
    "The most basic [MNIST classifier](https://keras.io/examples/vision/mnist_convnet/) consists of a stack of convolutional layers followed by a dense layer. In this tutorial, we will first replace the dense layer with a MoE layer. Then do the same for convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Linear MoE using Dense layers\n",
    "\n",
    "For this layer, we will create multiple dense layers that will be used as experts. Then a simple gating network will select at each step which exerts should be utilized for the current input. We will keep track of the number of times each expert is used. Then the selected experts will be combined using a weighted sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearMoE(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_experts=NUM_EXPERTS,\n",
    "        top_k=TOP_K,\n",
    "    ):\n",
    "        super(LinearMoE, self).__init__()\n",
    "\n",
    "        # Initialize experts\n",
    "        self.experts = [\n",
    "            layers.Dense(\n",
    "                hidden_size,\n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                    mean=0.0, stddev=0.001\n",
    "                ),\n",
    "                bias_initializer=\"zeros\",\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "        # Initialize gating network\n",
    "        self.gating_network = layers.Dense(\n",
    "            NUM_EXPERTS,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.001\n",
    "            ),\n",
    "            bias_initializer=\"zeros\",\n",
    "        )\n",
    "\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        # Keep track of how many times each expert is used\n",
    "        self.expert_usage_count = tf.Variable(\n",
    "            tf.zeros((num_experts,), dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get gating weights\n",
    "        gating_weights = self.gating_network(x)\n",
    "\n",
    "        # Get the top k experts based on the gating weights\n",
    "        top_k_weights, top_k_indices = tf.math.top_k(gating_weights, k=self.top_k)\n",
    "\n",
    "        # Count usage of each expert symbolically\n",
    "        updates = tf.ones_like(tf.reshape(top_k_indices, [-1]), dtype=tf.float32)\n",
    "        # Use tf.tensor_scatter_nd_add to increment the usage count\n",
    "        self.expert_usage_count.assign(\n",
    "            tf.tensor_scatter_nd_add(\n",
    "                self.expert_usage_count, tf.reshape(top_k_indices, [-1, 1]), updates\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Get outputs from only the top-k experts\n",
    "        top_k_expert_outputs = tf.stack(\n",
    "            [\n",
    "                self.experts[expert_index](x)\n",
    "                for expert_index in top_k_indices.numpy()[0]\n",
    "            ],\n",
    "            axis=1,\n",
    "        )  # Stack outputs along axis 1\n",
    "\n",
    "        # Combine outputs using top-k weights\n",
    "        combined_output = tf.einsum(\"ijk,ij->ik\", top_k_expert_outputs, top_k_weights)\n",
    "\n",
    "        return combined_output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Output of the top 3 experts out of 10 for one layer of MoE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sample_data = tf.random.uniform((1, 10))\n",
    "linear_mode = LinearMoE(32, 10, 3)\n",
    "linear_mode(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Routing Collapse\n",
    "\n",
    "Routing collapse is a problem that occurs with MoE layers. The route terminology refers to the selection process of which expert to use for a given input.\n",
    "\n",
    "Route collapse happens when a routing model, early in training, starts favoring just a few experts because they perform slightly better due to random starting conditions. This leads to most examples being sent to these experts, leaving others unused and reducing the model\u2019s overall capacity.\n",
    "\n",
    "Code below demonstrates the randomness of expert selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_expert_usage(runs):\n",
    "    # Running the later multiple times to show randomness of expert selection\n",
    "    for i in range(runs):\n",
    "        sample_data = tf.random.uniform((1, 10))\n",
    "        linear_mode = LinearMoE(10, 5)\n",
    "        _ = linear_mode(sample_data)\n",
    "        print(f\"Run {i}, Expert usage: {linear_mode.expert_usage_count.numpy()}\")\n",
    "\n",
    "\n",
    "check_expert_usage(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Adding loss functions to prevent route collapse\n",
    "To fix this, the authors use extra rules (importance and load losses), ideas borrowed from [Shazeer et al.](https://arxiv.org/abs/1701.06538), to ensure all experts get used evenly.\n",
    "\n",
    "The importance_loss calculates how much the usage of each expert (tracked in batch_importance_sum) deviates from the average usage (mean_importance) by using mean squared error, aiming to balance expert utilization. This helps prevent route collapse by discouraging the model from overloading a few experts, instead promoting an even distribution of examples across all experts to maintain diverse and effective routing.\n",
    "\n",
    "#### Load losses:\n",
    "    - Diversity loss: Diversity loss helps prevent route collapse by encouraging the routing model to evenly distribute examples across all experts, rather than favoring just a few due to their initial performance. It does this by maximizing the entropy of the gating weights, ensuring balanced expert utilization and improving the model's overall capacity.\n",
    "    - Overflow loss: The batch_overflow_sum measures how much the usage of experts exceeds a set capacity by applying ReLU to the difference between usage_counts (how many examples each expert handles) and batch_capacity (the allowed limit), then summing the excesses. This helps prevent route collapse by penalizing situations where certain experts are overused, encouraging a more even spread of examples across all experts to keep the model's capacity balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearMoE(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_experts=NUM_EXPERTS,\n",
    "        top_k=TOP_K,\n",
    "    ):\n",
    "        super(LinearMoE, self).__init__()\n",
    "\n",
    "        # Initialize experts\n",
    "        self.experts = [\n",
    "            layers.Dense(\n",
    "                hidden_size,\n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                    mean=0.0, stddev=0.001\n",
    "                ),\n",
    "                bias_initializer=\"zeros\",\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "        # Initialize gating network\n",
    "        self.gating_network = layers.Dense(\n",
    "            num_experts,  # Match output to num_experts\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.001\n",
    "            ),\n",
    "            bias_initializer=\"zeros\",\n",
    "        )\n",
    "\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        # Keep track of how many times each expert is used as a layer weight\n",
    "        self.expert_usage_count = tf.Variable(\n",
    "            tf.zeros((num_experts,), dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "        self.batch_capacity = BATCH_SIZE // num_experts\n",
    "\n",
    "    def _diversity_loss(self, weights):\n",
    "        entropy = -K.sum(weights * K.log(weights + 1e-10), axis=1)\n",
    "        self.diversity_loss = -K.mean(entropy)\n",
    "\n",
    "    def _importance_loss(self, gating_weights):\n",
    "        batch_importance_sum = K.sum(gating_weights, axis=0)\n",
    "        mean_importance = K.mean(batch_importance_sum)\n",
    "        self.importance_loss = K.mean(\n",
    "            K.square(\n",
    "                batch_importance_sum\n",
    "                - mean_importance * tf.ones_like(batch_importance_sum)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get gating weights and normalize\n",
    "        gating_weights = self.gating_network(x)\n",
    "        gating_weights = K.softmax(gating_weights)  # Ensure weights are probabilities\n",
    "        self._diversity_loss(gating_weights)\n",
    "        self._importance_loss(gating_weights)\n",
    "\n",
    "        # Get the top k experts based on the gating weights\n",
    "        top_k_weights, top_k_indices = tf.math.top_k(gating_weights, k=self.top_k)\n",
    "\n",
    "        # Count usage of each expert symbolically\n",
    "        updates = tf.ones_like(tf.reshape(top_k_indices, [-1]), dtype=tf.float32)\n",
    "        # Use tf.tensor_scatter_nd_add to increment the usage count\n",
    "        self.expert_usage_count.assign(\n",
    "            tf.tensor_scatter_nd_add(\n",
    "                self.expert_usage_count, tf.reshape(top_k_indices, [-1, 1]), updates\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate overflow using updated usage count\n",
    "        self.batch_overflow_sum = K.sum(\n",
    "            K.relu(tf.convert_to_tensor(self.expert_usage_count) - self.batch_capacity)\n",
    "        )\n",
    "\n",
    "        # Compute all expert outputs\n",
    "        expert_outputs = tf.stack(\n",
    "            [expert(x) for expert in self.experts], axis=1\n",
    "        )  # Shape: (batch_size, num_experts, hidden_size)\n",
    "\n",
    "        # Gather the top-k expert outputs using top_k_indices\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        batch_indices = tf.expand_dims(\n",
    "            tf.range(batch_size), 1\n",
    "        )  # Shape: (batch_size, 1)\n",
    "        batch_indices = tf.tile(\n",
    "            batch_indices, [1, self.top_k]\n",
    "        )  # Shape: (batch_size, top_k)\n",
    "\n",
    "        # Create indices for gathering\n",
    "        indices = tf.stack(\n",
    "            [batch_indices, top_k_indices], axis=2\n",
    "        )  # Shape: (batch_size, top_k, 2)\n",
    "        top_k_expert_outputs = tf.gather_nd(\n",
    "            expert_outputs, indices\n",
    "        )  # Shape: (batch_size, top_k, hidden_size)\n",
    "\n",
    "        # Combine outputs using top-k weights\n",
    "        combined_output = tf.reduce_sum(\n",
    "            top_k_expert_outputs * tf.expand_dims(top_k_weights, axis=-1), axis=1\n",
    "        )\n",
    "\n",
    "        return combined_output\n",
    "\n",
    "    def compute_total_loss(self, load_balance_coef=0.01):\n",
    "        return load_balance_coef * (\n",
    "            self.diversity_loss + self.batch_overflow_sum + self.importance_loss\n",
    "        )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## MNIST classification with MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MoEModel(keras.Model):\n",
    "    def __init__(self, input_shape, num_classes, num_experts=NUM_EXPERTS, top_k=TOP_K):\n",
    "        super(MoEModel, self).__init__()\n",
    "\n",
    "        # Define the convolutional block\n",
    "        self.conv_block = keras.Sequential(\n",
    "            [\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dropout(0.5),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # MoE classifier\n",
    "        self.moe_classifier = LinearMoE(\n",
    "            hidden_size=num_classes, num_experts=num_experts, top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Softmax layer\n",
    "        self.softmax = layers.Softmax()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        conv_flatten = self.conv_block(inputs)\n",
    "        moe_output = self.moe_classifier(conv_flatten)\n",
    "        outputs = self.softmax(moe_output)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data  # Unpack input data and labels\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            classification_loss = self.compute_loss(x, y, y_pred)\n",
    "            moe_loss = self.moe_classifier.compute_total_loss(load_balance_coef=0.01)\n",
    "            total_loss = classification_loss + moe_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.trainable_variables)\n",
    "        )  # Update metrics (e.g., accuracy)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict of metrics for monitoring\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"moe_loss\": moe_loss,\n",
    "            **{m.name: m.result() for m in self.metrics},\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        classification_loss = self.compute_loss(x, y, y_pred)\n",
    "        moe_loss = self.moe_classifier.compute_total_loss(load_balance_coef=0.01)\n",
    "        total_loss = classification_loss + moe_loss\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"moe_loss\": moe_loss,\n",
    "            **{m.name: m.result() for m in self.metrics},\n",
    "        }\n",
    "\n",
    "\n",
    "# Instantiate and compile the model\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "model = MoEModel(\n",
    "    input_shape=input_shape, num_classes=num_classes, num_experts=6, top_k=4\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),  # Assumes one-hot encoded labels\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_moe",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}