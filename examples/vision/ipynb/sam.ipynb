{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Segment Anything Model with ðŸ¤—Transformers\n",
    "\n",
    "**Authors:** [Merve Noyan](https://twitter.com/mervenoyann) & [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
    "**Date created:** 2023/07/11<br>\n",
    "**Last modified:** 2023/07/11<br>\n",
    "**Description:** Fine-tuning Segment Anything Model using Keras and ðŸ¤— Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Large language models (LLMs) make it easy for the end users to apply them to various\n",
    "applications through \"prompting\". For example if we wanted an LLM to predict the\n",
    "sentiment of the following sentence -- \"That movie was amazing, I thoroughly enjoyed it\"\n",
    "-- we'd do prompt the LLM with something like:\n",
    "\n",
    "> What's the sentiment of the following sentence: \"That movie was amazing, I thoroughly\n",
    "enjoyed it\"?\n",
    "\n",
    "In return, the LLM would return sentiment token.\n",
    "\n",
    "But when it comes to visual recognition tasks, how can we engineer \"visual\" cues to\n",
    "prompt foundation vision models? For example, we could have an input image and prompt the\n",
    "model with bounding box on that image and ask it to perform segmentation. The bounding\n",
    "box would serve as our visual prompt here.\n",
    "\n",
    "In the [Segment Anything Model](https://segment-anything.com/) (dubbed as SAM),\n",
    "researchers from Meta extended the space of language prompting to visual prompting. SAM\n",
    "is capable of performing zero-shot segmentation with a prompt input, inspired by large\n",
    "language models. The prompt here can be a set of foreground/background points, free text,\n",
    "a box or a mask. There are many downstream segmentation tasks, including semantic\n",
    "segmentation and edge detection. The goal of SAM is to enable all of these downstream\n",
    "segmentation tasks through prompting.\n",
    "\n",
    "In this example, we'll learn how to use the SAM model from ðŸ¤— Transformers for performing\n",
    "inference and fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install -q git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's import everything we need for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from transformers import TFSamModel, SamProcessor\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from PIL import Image\n",
    "import requests\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## SAM in a few words\n",
    "\n",
    "SAM has the following components:\n",
    "\n",
    "| ![](https://imgur.com/oLfdwuB.png) |\n",
    "|:--:|\n",
    "| Image taken from the official [SAM blog post](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The image encoder is responsible for computing image embeddings. When interacting with\n",
    "SAM, we compute the image embedding one time (as the image encoder is heavy) and then\n",
    "reuse it with different prompts mentioned above (points, bounding boxes, masks).\n",
    "\n",
    "Points and boxes (so-called sparse prompts) go through a lightweight prompt encoder,\n",
    "while masks (dense prompts) go through a convolutional layer. We couple the image\n",
    "embedding extracted from the image encoder and the prompt embedding and both go to a\n",
    "lightweight mask decoder. The decoder is responsible for predicting the mask.\n",
    "\n",
    "| ![](https://i.imgur.com/QQ9Ts5T.png) |\n",
    "|:--:|\n",
    "| Figure taken from the [SAM paper](https://arxiv.org/abs/2304.02643) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "SAM was pre-trained to predict a _valid_ mask for any acceptable prompt. This requirement\n",
    "allows SAM to output a valid mask even when the prompt is ambiguous to understand -- this\n",
    "makes SAM ambiguity-aware. Moreover, SAM predicts multiple masks for a single prompt.\n",
    "\n",
    "We highly encourage you to check out the [SAM paper](https://arxiv.org/abs/2304.02643)\n",
    "and the\n",
    "[blog post](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)\n",
    "to learn more about the additional details of SAM and the dataset used to pre-trained it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Running inference with SAM\n",
    "\n",
    "There are three checkpoints for SAM:\n",
    "\n",
    "* [sam-vit-base](https://huggingface.co/facebook/sam-vit-base)\n",
    "* [sam-vit-large](https://huggingface.co/facebook/sam-vit-large)\n",
    "* [sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge).\n",
    "\n",
    "We load `sam-vit-base` in\n",
    "[`TFSamModel`](https://huggingface.co/docs/transformers/main/model_doc/sam#transformers.TFSamModel).\n",
    "We also need `SamProcessor`for the associated checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = TFSamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we write some utility functions for visualization. Most of these functions are\n",
    "taken from\n",
    "[this notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_on_image(raw_image, boxes):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_points_on_image(raw_image, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "        labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "        labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_points_and_boxes_on_image(raw_image, boxes, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "        labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "        labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_points_and_boxes_on_image(raw_image, boxes, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "        labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "        labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_masks_on_image(raw_image, masks, scores):\n",
    "    if len(masks[0].shape) == 4:\n",
    "        final_masks = tf.squeeze(masks[0])\n",
    "    if scores.shape[0] == 1:\n",
    "        final_scores = tf.squeeze(scores)\n",
    "\n",
    "    nb_predictions = scores.shape[-1]\n",
    "    fig, axes = plt.subplots(1, nb_predictions, figsize=(15, 15))\n",
    "\n",
    "    for i, (mask, score) in enumerate(zip(final_masks, final_scores)):\n",
    "        mask = tf.stop_gradient(mask)\n",
    "        axes[i].imshow(np.array(raw_image))\n",
    "        show_mask(mask, axes[i])\n",
    "        axes[i].title.set_text(f\"Mask {i+1}, Score: {score.numpy().item():.3f}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will segment a car image using a point prompt. Make sure to set `return_tensors` to\n",
    "`tf` when calling the processor.\n",
    "\n",
    "Let's load an image of a car and segment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "plt.imshow(raw_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's now define a set of points we will use as the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "input_points = [[[450, 600]]]\n",
    "\n",
    "# Visualize a single point.\n",
    "show_points_on_image(raw_image, input_points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Preprocess the input image.\n",
    "inputs = processor(raw_image, input_points=input_points, return_tensors=\"tf\")\n",
    "\n",
    "# Predict for segmentation with the prompt.\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`outputs` has got two attributes of our interest:\n",
    "\n",
    "* `outputs.pred_masks`: which denotes the predicted masks.\n",
    "* `outputs.iou_scores`: which denotes the IoU scores associated with the masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's post-process the masks and visualize them with their IoU scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks,\n",
    "    inputs[\"original_sizes\"],\n",
    "    inputs[\"reshaped_input_sizes\"],\n",
    "    return_tensors=\"tf\",\n",
    ")\n",
    "\n",
    "show_masks_on_image(raw_image, masks, outputs.iou_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And there we go!\n",
    "\n",
    "As can be noticed, all the masks are _valid_ masks for the point prompt we provided.\n",
    "\n",
    "SAM is flexible enough to support different visual prompts and we encourage you to check\n",
    "out [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb) to know more about them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "We'll use [this dataset](https://huggingface.co/datasets/nielsr/breast-cancer) consisting\n",
    "of breast cancer scans. In the medical imaging domain, being able to segment the cells\n",
    "containing malignancy is an important task.\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Let's first get the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "remote_path = \"https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/breast-cancer-dataset.tar.gz\"\n",
    "dataset_path = keras.utils.get_file(\n",
    "    \"breast-cancer-dataset.tar.gz\", remote_path, untar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's now visualize a sample from the dataset.\n",
    "\n",
    "*(The `show_mask()` utility is taken from\n",
    "[this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "# Load all the image and label paths.\n",
    "image_paths = sorted(glob.glob(os.path.join(dataset_path, \"images/*.png\")))\n",
    "label_paths = sorted(glob.glob(os.path.join(dataset_path, \"labels/*.png\")))\n",
    "\n",
    "# Load the image and label.\n",
    "idx = 15\n",
    "image = Image.open(image_paths[idx])\n",
    "label = Image.open(label_paths[idx])\n",
    "image = np.array(image)\n",
    "ground_truth_seg = np.array(label)\n",
    "\n",
    "# Display.\n",
    "fig, axes = plt.subplots()\n",
    "axes.imshow(image)\n",
    "show_mask(ground_truth_seg, axes)\n",
    "axes.title.set_text(f\"Ground truth mask\")\n",
    "axes.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "tf.shape(ground_truth_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing `tf.data.Dataset`\n",
    "\n",
    "We now write a generator class to prepare the images and the segmentation masks using the\n",
    "`processor` utilized above. We will leverage this generator class to create a\n",
    "`tf.data.Dataset` object for our training set by using\n",
    "`tf.data.Dataset.from_generator()`. Utilities of this class have been adapted from\n",
    "[this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb).\n",
    "\n",
    "The generator is responsible for yielding the preprocessed images and the segmentation\n",
    "masks, and some other metadata needed by the SAM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Generator:\n",
    "    \"\"\"Generator class for processing the images and the masks for SAM fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, processor):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.image_paths = sorted(\n",
    "            glob.glob(os.path.join(self.dataset_path, \"images/*.png\"))\n",
    "        )\n",
    "        self.label_paths = sorted(\n",
    "            glob.glob(os.path.join(self.dataset_path, \"labels/*.png\"))\n",
    "        )\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self):\n",
    "        for image_path, label_path in zip(self.image_paths, self.label_paths):\n",
    "            image = np.array(Image.open(image_path))\n",
    "            ground_truth_mask = np.array(Image.open(label_path))\n",
    "\n",
    "            # get bounding box prompt\n",
    "            prompt = self.get_bounding_box(ground_truth_mask)\n",
    "\n",
    "            # prepare image and prompt for the model\n",
    "            inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"np\")\n",
    "\n",
    "            # remove batch dimension which the processor adds by default\n",
    "            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "            # add ground truth segmentation\n",
    "            inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "            yield inputs\n",
    "\n",
    "    def get_bounding_box(self, ground_truth_map):\n",
    "        # get bounding box from mask\n",
    "        y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = ground_truth_map.shape\n",
    "        x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "        bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        return bbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`get_bounding_box()` is responsible for turning the ground-truth segmentation maps into\n",
    "bounding boxes. These bounding boxes are fed to SAM as prompts (along with the original\n",
    "images) during fine-tuning and SAM is then trained to predict valid masks.\n",
    "\n",
    "The advantage of this first creating a generator and then using it to create a\n",
    "`tf.data.Dataset` is the flexbility. Sometimes, we may need to use utitlities from other\n",
    "libraries ([`albumentations`](https://albumentations.ai/), for example) which may not\n",
    "come in native TensorFlow implementations. By using this workflow, we can easily\n",
    "accommodate such use case.\n",
    "\n",
    "But the non-TF counterparts might introduce performance bottlenecks, though. However, for\n",
    "our example, it should work just fine.\n",
    "\n",
    "Now, we prepare the `tf.data.Dataset` from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define the output signature of the generator class.\n",
    "output_signature = {\n",
    "    \"pixel_values\": tf.TensorSpec(shape=(3, None, None), dtype=tf.float32),\n",
    "    \"original_sizes\": tf.TensorSpec(shape=(None,), dtype=tf.int64),\n",
    "    \"reshaped_input_sizes\": tf.TensorSpec(shape=(None,), dtype=tf.int64),\n",
    "    \"input_boxes\": tf.TensorSpec(shape=(None, None), dtype=tf.float64),\n",
    "    \"ground_truth_mask\": tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "}\n",
    "\n",
    "# Prepare the dataset object.\n",
    "train_dataset_gen = Generator(dataset_path, processor)\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    train_dataset_gen, output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we configure the dataset for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "auto = tf.data.AUTOTUNE\n",
    "batch_size = 2\n",
    "shuffle_buffer = 4\n",
    "\n",
    "train_ds = (\n",
    "    train_ds.cache()\n",
    "    .shuffle(shuffle_buffer)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=auto)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Take a single batch of data and inspect the shapes of the elements present inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_ds))\n",
    "for k in sample:\n",
    "    print(k, sample[k].shape, sample[k].dtype, isinstance(sample[k], tf.Tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will now write DICE loss. This implementation is based on\n",
    "[MONAI DICE loss](https://docs.monai.io/en/stable/losses.html#diceloss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-5):\n",
    "    y_pred = tf.sigmoid(y_pred)\n",
    "    reduce_axis = list(range(2, len(y_pred.shape)))\n",
    "    if batch_size > 1:\n",
    "        # reducing spatial dimensions and batch\n",
    "        reduce_axis = [0] + reduce_axis\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=reduce_axis)\n",
    "    y_true_sq = tf.math.pow(y_true, 2)\n",
    "    y_pred_sq = tf.math.pow(y_pred, 2)\n",
    "\n",
    "    ground_o = tf.reduce_sum(y_true_sq, axis=reduce_axis)\n",
    "    pred_o = tf.reduce_sum(y_pred_sq, axis=reduce_axis)\n",
    "    denominator = ground_o + pred_o\n",
    "    # calculate DICE coefficient\n",
    "    loss = 1.0 - (2.0 * intersection + 1e-5) / (denominator + 1e-5)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "##Â Fine-tuning SAM\n",
    "\n",
    "We will now fine-tune SAM's decoder part. We will freeze the vision encoder and prompt\n",
    "encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# initialize SAM model and optimizer\n",
    "sam = TFSamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "optimizer = keras.optimizers.Adam(1e-5)\n",
    "\n",
    "for layer in sam.layers:\n",
    "    if layer.name in [\"vision_encoder\", \"prompt_encoder\"]:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # pass inputs to SAM model\n",
    "        outputs = sam(\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            input_boxes=inputs[\"input_boxes\"],\n",
    "            multimask_output=False,\n",
    "            training=True,\n",
    "        )\n",
    "\n",
    "        predicted_masks = tf.squeeze(outputs.pred_masks, 1)\n",
    "        ground_truth_masks = tf.cast(inputs[\"ground_truth_mask\"], tf.float32)\n",
    "\n",
    "        # calculate loss over predicted and ground truth masks\n",
    "        loss = dice_loss(tf.expand_dims(ground_truth_masks, 1), predicted_masks)\n",
    "        # update trainable variables\n",
    "        trainable_vars = sam.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can now run the training for three epochs. We might have a warning about gradients\n",
    "not existing on IoU prediction head of mask decoder, we can safely ignore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# run training\n",
    "for epoch in range(3):\n",
    "    for inputs in train_ds:\n",
    "        loss = train_step(inputs)\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Serialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We serialized the model and pushed for you below. `push_to_hub` method serializes model,\n",
    "generates a model card and pushes it to Hugging Face Hub, so that other people can load\n",
    "the model using `from_pretrained` method to infer or further fine-tune. We also need to\n",
    "push the same preprocessor in the repository. Find the model and the preprocessor\n",
    "[here](https://huggingface.co/merve/sam-finetuned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# sam.push_to_hub(\"merve/sam-finetuned\")\n",
    "# processor.push_to_hub(\"merve/sam-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can now infer with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load another image for inference.\n",
    "idx = 20\n",
    "raw_image_inference = Image.open(image_paths[idx])\n",
    "\n",
    "# process the image and infer\n",
    "preprocessed_img = processor(raw_image_inference)\n",
    "outputs = sam(preprocessed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Lastly, we can visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "infer_masks = outputs[\"pred_masks\"]\n",
    "iou_scores = outputs[\"iou_scores\"]\n",
    "show_masks_on_image(raw_image_inference, masks=infer_masks, scores=iou_scores)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sam",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
