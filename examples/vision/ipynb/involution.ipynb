{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Involutional neural networks\n",
    "\n",
    "**Author:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)<br>\n",
    "**Date created:** 2021/07/25<br>\n",
    "**Last modified:** 2021/07/25<br>\n",
    "**Description:** Deep dive into location-specific and channel-agnostic \"involution\" kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Convolution has been the basis of most modern neural\n",
    "networks for computer vision. A convolution kernel is\n",
    "spatial-agnostic and channel-specific. Because of this, it isn't able\n",
    "to adapt to different visual patterns with respect to\n",
    "different spatial locations. Along with location-related problems, the\n",
    "receptive field of convolution creates challenges with regard to capturing\n",
    "long-range spatial interactions.\n",
    "\n",
    "To address the above issues, Li et. al. rethink the properties\n",
    "of convolution in\n",
    "[Involution: Inverting the Inherence of Convolution for VisualRecognition](https://arxiv.org/abs/2103.06255).\n",
    "The authors propose the \"involution kernel\", that is location-specific and\n",
    "channel-agnostic. Due to the location-specific nature of the operation,\n",
    "the authors say that self-attention falls under the design paradigm of\n",
    "involution.\n",
    "\n",
    "This example describes the involution kernel, compares two image\n",
    "classification models, one with convolution and the other with\n",
    "involution, and also tries drawing a parallel with the self-attention\n",
    "layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Convolution\n",
    "\n",
    "Convolution remains the mainstay of deep neural networks for computer vision.\n",
    "To understand Involution, it is necessary to talk about the\n",
    "convolution operation.\n",
    "\n",
    "![Imgur](https://i.imgur.com/MSKLsm5.png)\n",
    "\n",
    "Consider an input tensor **X** with dimensions **H**, **W** and\n",
    "**C_in**. We take a collection of **C_out** convolution kernels each of\n",
    "shape **K**, **K**, **C_in**. With the multiply-add operation between\n",
    "the input tensor and the kernels we obtain an output tensor **Y** with\n",
    "dimensions **H**, **W**, **C_out**.\n",
    "\n",
    "In the diagram above `C_out=3`. This makes the output tensor of shape H,\n",
    "W and 3. One can notice that the convoltuion kernel does not depend on\n",
    "the spatial position of the input tensor which makes it\n",
    "**location-agnostic**. On the other hand, each channel in the output\n",
    "tensor is based on a specific convolution filter which makes is\n",
    "**channel-specific**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Involution\n",
    "\n",
    "The idea is to have an operation that is both **location-specific**\n",
    "and **channel-agnostic**. Trying to implement these specific properties poses\n",
    "a challenge. With a fixed number of involution kernels (for each\n",
    "spatial position) we will **not** be able to process variable-resolution\n",
    "input tensors.\n",
    "\n",
    "To solve this problem, the authors have considered *generating* each\n",
    "kernel conditioned on specific spatial positions. With this method, we\n",
    "should be able to process variable-resolution input tensors with ease.\n",
    "The diagram below provides an intuition on this kernel generation\n",
    "method.\n",
    "\n",
    "![Imgur](https://i.imgur.com/jtrGGQg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Involution(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, channel, group_number, kernel_size, stride, reduction_ratio, name\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # Initialize the parameters.\n",
    "        self.channel = channel\n",
    "        self.group_number = group_number\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Get the shape of the input.\n",
    "        (_, height, width, num_channels) = input_shape\n",
    "\n",
    "        # Scale the height and width with respect to the strides.\n",
    "        height = height // self.stride\n",
    "        width = width // self.stride\n",
    "\n",
    "        # Define a layer that average pools the input tensor\n",
    "        # if stride is more than 1.\n",
    "        self.stride_layer = (\n",
    "            keras.layers.AveragePooling2D(\n",
    "                pool_size=self.stride, strides=self.stride, padding=\"same\"\n",
    "            )\n",
    "            if self.stride > 1\n",
    "            else tf.identity\n",
    "        )\n",
    "        # Define the kernel generation layer.\n",
    "        self.kernel_gen = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Conv2D(\n",
    "                    filters=self.channel // self.reduction_ratio, kernel_size=1\n",
    "                ),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.Conv2D(\n",
    "                    filters=self.kernel_size * self.kernel_size * self.group_number,\n",
    "                    kernel_size=1,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # Define reshape layers\n",
    "        self.kernel_reshape = keras.layers.Reshape(\n",
    "            target_shape=(\n",
    "                height,\n",
    "                width,\n",
    "                self.kernel_size * self.kernel_size,\n",
    "                1,\n",
    "                self.group_number,\n",
    "            )\n",
    "        )\n",
    "        self.input_patches_reshape = keras.layers.Reshape(\n",
    "            target_shape=(\n",
    "                height,\n",
    "                width,\n",
    "                self.kernel_size * self.kernel_size,\n",
    "                num_channels // self.group_number,\n",
    "                self.group_number,\n",
    "            )\n",
    "        )\n",
    "        self.output_reshape = keras.layers.Reshape(\n",
    "            target_shape=(height, width, num_channels)\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Generate the kernel with respect to the input tensor.\n",
    "        # B, H, W, K*K*G\n",
    "        kernel_input = self.stride_layer(x)\n",
    "        kernel = self.kernel_gen(kernel_input)\n",
    "\n",
    "        # reshape the kerenl\n",
    "        # B, H, W, K*K, 1, G\n",
    "        kernel = self.kernel_reshape(kernel)\n",
    "\n",
    "        # Extract input patches.\n",
    "        # B, H, W, K*K*C\n",
    "        input_patches = tf.image.extract_patches(\n",
    "            images=x,\n",
    "            sizes=[1, self.kernel_size, self.kernel_size, 1],\n",
    "            strides=[1, self.stride, self.stride, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "\n",
    "        # Reshape the input patches to align with later operations.\n",
    "        # B, H, W, K*K, C//G, G\n",
    "        input_patches = self.input_patches_reshape(input_patches)\n",
    "\n",
    "        # Compute the multiply-add operation of kernels and patches.\n",
    "        # B, H, W, K*K, C//G, G\n",
    "        output = tf.multiply(kernel, input_patches)\n",
    "        # B, H, W, C//G, G\n",
    "        output = tf.reduce_sum(output, axis=3)\n",
    "\n",
    "        # Reshape the output kernel.\n",
    "        # B, H, W, C\n",
    "        output = self.output_reshape(output)\n",
    "\n",
    "        # Return the output tensor and the kernel.\n",
    "        return output, kernel\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Testing the Involution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define the input tensor.\n",
    "input_tensor = tf.random.normal((32, 256, 256, 3))\n",
    "\n",
    "# Compute involution with stride 1.\n",
    "output_tensor, _ = Involution(\n",
    "    channel=3, group_number=1, kernel_size=5, stride=1, reduction_ratio=1, name=\"inv_1\"\n",
    ")(input_tensor)\n",
    "print(f\"with stride 1 ouput shape: {output_tensor.shape}\")\n",
    "\n",
    "# Compute involution with stride 2.\n",
    "output_tensor, _ = Involution(\n",
    "    channel=3, group_number=1, kernel_size=5, stride=2, reduction_ratio=1, name=\"inv_2\"\n",
    ")(input_tensor)\n",
    "print(f\"with stride 2 ouput shape: {output_tensor.shape}\")\n",
    "\n",
    "# Compute involution with stride 1, channel 16 and reduction ratio 2.\n",
    "output_tensor, _ = Involution(\n",
    "    channel=16, group_number=1, kernel_size=5, stride=1, reduction_ratio=2, name=\"inv_3\"\n",
    ")(input_tensor)\n",
    "print(\n",
    "    \"with channel 16 and reduction ratio 2 ouput shape: {}\".format(output_tensor.shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Image Classification\n",
    "\n",
    "In this section, we will build an image-classifier model. There will\n",
    "be two models one with convolutions and the other with involutions.\n",
    "\n",
    "The image-classification model is heavily inspired by this\n",
    "[Convolutional Neural Network (CNN)](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "tutorial from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Get the CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset.\n",
    "print(\"loading the CIFAR10 dataset...\")\n",
    "(train_images, train_labels), (\n",
    "    test_images,\n",
    "    test_labels,\n",
    ") = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1.\n",
    "(train_images, test_images) = (train_images / 255.0, test_images / 255.0)\n",
    "\n",
    "# Shuffle and batch the dataset.\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "    .shuffle(256)\n",
    "    .batch(256)\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Build the conv model.\n",
    "print(\"building the convolution model...\")\n",
    "conv_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding=\"same\"),\n",
    "        keras.layers.ReLU(name=\"relu1\"),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n",
    "        keras.layers.ReLU(name=\"relu2\"),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n",
    "        keras.layers.ReLU(name=\"relu3\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the mode with the necessary loss function and optimizer.\n",
    "print(\"compiling the convolution model...\")\n",
    "conv_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "print(\"conv model training...\")\n",
    "conv_hist = conv_model.fit(train_ds, epochs=20, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Involutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Build the involution model.\n",
    "print(\"building the involution model...\")\n",
    "\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x, _ = Involution(\n",
    "    channel=3, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"inv_1\"\n",
    ")(inputs)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "x, _ = Involution(\n",
    "    channel=3, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"inv_2\"\n",
    ")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "x, _ = Involution(\n",
    "    channel=3, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"inv_3\"\n",
    ")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(10)(x)\n",
    "\n",
    "inv_model = keras.Model(inputs=[inputs], outputs=[outputs], name=\"inv_model\")\n",
    "\n",
    "# Compile the mode with the necessary loss function and optimizer.\n",
    "print(\"compiling the involution model...\")\n",
    "inv_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "print(\"inv model training...\")\n",
    "inv_hist = inv_model.fit(train_ds, epochs=20, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Comparisons\n",
    "\n",
    "In this section, we will be looking at both the models and compare a\n",
    "few pointers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Parameters\n",
    "\n",
    "One can see that with a similar architecture the parameters in a CNN\n",
    "is much larger than that of an INN (Involutional Neural Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_model.summary()\n",
    "\n",
    "inv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Loss and Accuracy Plots\n",
    "\n",
    "Here, the loss and the accuracy plots demonstrate that INNs are slow\n",
    "learners (with lower parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Convolution Loss\")\n",
    "plt.plot(conv_hist.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(conv_hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Involution Loss\")\n",
    "plt.plot(inv_hist.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(inv_hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Convolution Accuracy\")\n",
    "plt.plot(conv_hist.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(conv_hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Involution Accuracy\")\n",
    "plt.plot(inv_hist.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(inv_hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualizing Involution Kernels\n",
    "\n",
    "To visualize the kernels, we take the sum of **K\u00d7K** values from each\n",
    "involution kernel. **All the representatives at different spatial\n",
    "locations frame the corresponding heat map.**\n",
    "\n",
    "The authors mention:\n",
    "\n",
    "\"Our proposed involution is reminiscent of self-attention and\n",
    "essentially could become a generalized version of it.\"\n",
    "\n",
    "With the visualization of the kernel we can indeed obtain an attention\n",
    "map of the image. The learned involution kernels provides attention to\n",
    "individual spatial positions of the input tensor. The\n",
    "**location-specific** property makes involution a generic space of models\n",
    "in which self-attention belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "layer_names = [\"inv_1\", \"inv_2\", \"inv_3\"]\n",
    "outputs = [inv_model.get_layer(name).output for name in layer_names]\n",
    "vis_model = keras.Model(inv_model.input, outputs)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=4, figsize=(10, 30))\n",
    "\n",
    "for ax, test_image in zip(axes, test_images[:10]):\n",
    "    (inv1_out, inv2_out, inv3_out) = vis_model.predict(test_image[None, ...])\n",
    "\n",
    "    _, inv1_kernel = inv1_out\n",
    "    _, inv2_kernel = inv2_out\n",
    "    _, inv3_kernel = inv3_out\n",
    "\n",
    "    inv1_kernel = tf.reduce_sum(inv1_kernel, axis=[-1, -2, -3])\n",
    "    inv2_kernel = tf.reduce_sum(inv2_kernel, axis=[-1, -2, -3])\n",
    "    inv3_kernel = tf.reduce_sum(inv3_kernel, axis=[-1, -2, -3])\n",
    "\n",
    "    ax[0].imshow(keras.preprocessing.image.array_to_img(test_image))\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "\n",
    "    ax[1].imshow(keras.preprocessing.image.array_to_img(inv1_kernel[0, ..., None]))\n",
    "    ax[1].set_title(\"Involution Kernel 1\")\n",
    "\n",
    "    ax[2].imshow(keras.preprocessing.image.array_to_img(inv2_kernel[0, ..., None]))\n",
    "    ax[2].set_title(\"Involution Kernel 2\")\n",
    "\n",
    "    ax[3].imshow(keras.preprocessing.image.array_to_img(inv3_kernel[0, ..., None]))\n",
    "    ax[3].set_title(\"Involution Kernel 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In this example, the main focus was to build an `Involution` layer which\n",
    "can be easily reused. While our comparisons were based on a specific\n",
    "task, feel free to use the layer for different tasks and report your\n",
    "results.\n",
    "\n",
    "According to me, the key take-away of involution is its\n",
    "relationship with self-attention. The intuition behind location-specific\n",
    "and channel-spefic processing makes sense in a lot of tasks.\n",
    "\n",
    "Moving forward one can:\n",
    "\n",
    "- Look at [Yannick's video](https://youtu.be/pH2jZun8MoY) on\n",
    "    involution for a better understanding.\n",
    "- Experiment with the various hyperparameters of the involution layer.\n",
    "- Build different models with the involution layer.\n",
    "- Try building a different kernel generation method altogether.\n",
    "\n"
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/involution) ",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/involution)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "involution",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}