{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# 3D Multimodal Brain Tumor Segmentation\n",
    "\n",
    "**Author:** [Mohammed Innat](https://www.linkedin.com/in/innat2k14/)<br>\n",
    "**Date created:** 2026/02/02<br>\n",
    "**Last modified:** 2026/02/02<br>\n",
    "**Description:** Implementing 3D semantic segmentation pipeline for medical imaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Brain tumor segmentation is a core task in medical image analysis, where the goal is to automatically identify and label different tumor sub-regions from 3D MRI scans. Accurate segmentation helps clinicians with diagnosis, treatment planning, and disease monitoring. In this tutorial, we focus on multimodal MRI-based brain tumor segmentation using the widely adopted **BraTS** (**Brain Tumor Segmentation**) dataset.\n",
    "\n",
    "## The BraTS Dataset\n",
    "\n",
    "The **BraTS** dataset provides multimodal 3D brain MRI scans, released as NIfTI files (`.nii.gz`). For each patient, four MRI modalities are available:\n",
    "\n",
    "- **T1** \u2013 native T1-weighted MRI\n",
    "- **T1Gd** \u2013 post-contrast T1-weighted MRI\n",
    "- **T2** \u2013 T2-weighted MRI\n",
    "- **T2-FLAIR** \u2013 Fluid Attenuated Inversion Recovery MRI\n",
    "\n",
    "These scans are collected using different scanners and clinical protocols from 19 institutions, making the dataset diverse and realistic. More details about the dataset can be found in the official [BraTS documentation](https://www.med.upenn.edu/cbica/brats2020/data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Segmentation Labels\n",
    "\n",
    "Each scan is manually annotated by **one to four expert raters**, following a standardized annotation protocol and reviewed by experienced neuroradiologists. The segmentation masks contain the following tumor sub-regions:\n",
    "\n",
    "- **NCR / NET (label 1)** \u2013 Necrotic and non-enhancing tumor core\n",
    "- **ED (label 2)** \u2013 Peritumoral edema\n",
    "- **ET (label 4)** \u2013 GD-enhancing tumor\n",
    "- **0** \u2013 Background (non-tumor tissue)\n",
    "\n",
    "The data are released after preprocessing:\n",
    "\n",
    "- All modalities are **co-registered**\n",
    "- Resampled to `1 mm\u00b3` isotropic resolution\n",
    "- **Skull-stripped** for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset Format and TFRecord Conversion\n",
    "\n",
    "The original BraTS scans are provided in `.nii` format and can be accessed from Kaggle [here](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation/). To enable **efficient training pipelines**, we convert the NIfTI files into **TFRecord** format:\n",
    "\n",
    "- The conversion process is documented [here](https://www.kaggle.com/code/ipythonx/brats-nii-to-tfrecord)\n",
    "- The preprocessed TFRecord dataset is available [here](https://www.kaggle.com/datasets/ipythonx/brats2020)\n",
    "- Each TFRecord file contains **up to 20 cases**\n",
    "\n",
    "Since BraTS does not provide publicly available ground-truth labels for validation or test sets, we will **hold out a subset of TFRecord files** from training for validation purposes.\n",
    "\n",
    "\n",
    "# What This Tutorial Covers\n",
    "\n",
    "In this tutorial, we provide a step-by-step, end-to-end workflow for brain tumor segmentation using [medicai](https://github.com/innat/medic-ai), a Keras-based medical imaging library with multi-backend support. We will walk through:\n",
    "\n",
    "1. **Loading the Dataset**\n",
    "    - Read TFRecord files that contain `image`, `label`, and `affine` matrix information.\n",
    "    - Build efficient data pipelines using the `tf.data` API for training and evaluation.\n",
    "2. **Medical Image Preprocessing**\n",
    "    - Apply image transformations provided by `medicai` to prepare the data for model input.\n",
    "3. **Model Building**\n",
    "    - Construct a 3D segmentation model with [`SwinUNETR`](https://arxiv.org/abs/2201.01266) You can also experiment with other available 3D architectures, including [`UNETR`](https://arxiv.org/abs/2103.10504), [`SegFormer`](https://arxiv.org/abs/2404.10156), and [`UNETR++`](https://ieeexplore.ieee.org/document/10526382)..\n",
    "4. **Loss and Metrics Definition**\n",
    "    - Using Dice-based loss functions and segmentation metrics tailored for medical imaging\n",
    "5. **Model Evaluation**\n",
    "    - Performing inference on large 3D volumes using **sliding window inference**\n",
    "    - Computing per-class evaluation metrics\n",
    "6. **Visualization of Results**\n",
    "    - Visualizing predicted segmentation masks for qualitative analysis\n",
    "\n",
    "By the end of this tutorial, you will have a complete brain tumor segmentation pipeline, from data loading and preprocessing to model training, evaluation, and visualization using modern 3D deep learning techniques and the `medicai` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Installation\n",
    "\n",
    "We will install the following packages: [`kagglehub`](https://github.com/Kaggle/kagglehub) for downloading the dataset from\n",
    "Kaggle, and [`medicai`](https://github.com/innat/medic-ai) for accessing specialized methods for medical imaging, including 3D transformations, model architectures, loss functions, metrics, and other essential components.\n",
    "\n",
    "```shell\n",
    "!pip install kagglehub -qU\n",
    "!pip install git+https://github.com/innat/medic-ai.git -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
    "    kagglehub.login()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Download the dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset_id = \"ipythonx/brats2020\"\n",
    "destination_path = \"brats2020_subset\"\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Download the 3 shards: 0 and 1st for training set, 36th for validation set.\n",
    "for i in [0, 1, 36]:\n",
    "    filename = f\"training_shard_{i}.tfrec\"\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    path = kagglehub.dataset_download(dataset_id, path=filename)\n",
    "    shutil.move(path, destination_path)\n",
    "\n",
    "# Commented out to observe the progress bar\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # tensorflow, torch, jax\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from medicai.callbacks import SlidingWindowInferenceCallback\n",
    "from medicai.losses import BinaryDiceCELoss\n",
    "from medicai.metrics import BinaryDiceMetric\n",
    "from medicai.models import SwinUNETR\n",
    "from medicai.transforms import (\n",
    "    Compose,\n",
    "    CropForeground,\n",
    "    NormalizeIntensity,\n",
    "    RandFlip,\n",
    "    RandShiftIntensity,\n",
    "    RandSpatialCrop,\n",
    "    TensorBundle,\n",
    ")\n",
    "from medicai.utils.inference import SlidingWindowInference\n",
    "\n",
    "# enable mixed precision\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# reproducibility\n",
    "keras.utils.set_random_seed(101)\n",
    "\n",
    "print(\n",
    "    f\"keras backend: {keras.config.backend()}\\n\"\n",
    "    f\"keras version: {keras.version()}\\n\"\n",
    "    f\"tensorflow version: {tf.__version__}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Create Multi-label Brain Tumor Labels\n",
    "\n",
    "The BraTS segmentation task involves multiple tumor sub-regions, and it is formulated as a multi-label segmentation problem. The label combinations are used to define the following clinical regions of interest:\n",
    "\n",
    "```shell\n",
    "- Tumor Core (TC): label = 1 or 4\n",
    "- Whole Tumor (WT): label = 1 or 2 or 4\n",
    "- Enhancing Tumor (ET): label = 4\n",
    "```\n",
    "\n",
    "These region-wise groupings allow for evaluation across different tumor structures relevant for clinical assessment and treatment planning. A sample view is shown below, figure taken from [BraTS-benchmark](https://arxiv.org/abs/2107.02314) paper.\n",
    "\n",
    "![](https://i.imgur.com/Agnwpxm.png)\n",
    "\n",
    "## Managing Multi-Label Outputs with `TensorBundle`\n",
    "\n",
    "To organize and manage these multi-label segmentation targets, we will implement a custom transformation using [**TensorBundle**](https://github.com/innat/medic-ai/blob/2d2139020531acd1c2d41b07a10daf04ceb150f4/medicai/transforms/tensor_bundle.py#L9) from `medicai`. The `TensorBundle` is a lightweight container class designed to hold:\n",
    "\n",
    "- A dictionary of tensors (e.g., images, labels)\n",
    "- Optional metadata associated with those tensors (e.g., affine matrices, spacing, original shapes)\n",
    "\n",
    "This design allows data and metadata to be passed together through the transformation pipeline in a structured and consistent way. Each `medicai` transformation expects inputs to be organized as `key:value` pairs, for example:\n",
    "\n",
    "```shell\n",
    "meta = {\"affine\": affine}\n",
    "data = {\"image\": image, \"label\": label}\n",
    "```\n",
    "\n",
    "Using `TensorBundle` makes it easier to apply complex medical imaging transformations while preserving spatial and anatomical information throughout preprocessing and model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvertToMultiChannelBasedOnBratsClasses:\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on BRATS classes using TensorFlow.\n",
    "\n",
    "    Label definitions:\n",
    "    - 1: necrotic and non-enhancing tumor core\n",
    "    - 2: peritumoral edema\n",
    "    - 4: GD-enhancing tumor\n",
    "\n",
    "    Output channels:\n",
    "    - Channel 0 (TC): Tumor core (labels 1 or 4)\n",
    "    - Channel 1 (WT): Whole tumor (labels 1, 2, or 4)\n",
    "    - Channel 2 (ET): Enhancing tumor (label 4)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = TensorBundle(inputs)\n",
    "\n",
    "        for key in self.keys:\n",
    "            data = inputs[key]\n",
    "\n",
    "            # TC: label == 1 or 4\n",
    "            tc = tf.logical_or(tf.equal(data, 1), tf.equal(data, 4))\n",
    "\n",
    "            # WT: label == 1 or 2 or 4\n",
    "            wt = tf.logical_or(tc, tf.equal(data, 2))\n",
    "\n",
    "            # ET: label == 4\n",
    "            et = tf.equal(data, 4)\n",
    "\n",
    "            stacked = tf.stack(\n",
    "                [\n",
    "                    tf.cast(tc, tf.float32),\n",
    "                    tf.cast(wt, tf.float32),\n",
    "                    tf.cast(et, tf.float32),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "            inputs[key] = stacked\n",
    "        return inputs\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Transformation\n",
    "\n",
    "Each `medicai` transformation expects the input to have the shape `(depth, height, width, channel)`. The original `.nii` (and converted `.tfrecord`) format contains the input shape of `(height, width, depth)`. To make it compatible with `medicai`, we need to re-arrange the shape axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rearrange_shape(sample):\n",
    "    # unpack sample\n",
    "    image = sample[\"image\"]\n",
    "    label = sample[\"label\"]\n",
    "    affine = sample[\"affine\"]\n",
    "\n",
    "    # special case\n",
    "    image = tf.transpose(image, perm=[2, 1, 0, 3])  # whdc -> dhwc\n",
    "    label = tf.transpose(label, perm=[2, 1, 0])  # whd -> dhw\n",
    "    cols = tf.gather(affine, [2, 1, 0], axis=1)  # (whd) -> (dhw)\n",
    "    affine = tf.concat([cols, affine[:, 3:]], axis=1)\n",
    "\n",
    "    # update sample with new / updated tensor\n",
    "    sample[\"image\"] = image\n",
    "    sample[\"label\"] = label\n",
    "    sample[\"affine\"] = affine\n",
    "    return sample\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Each transformation class of `medicai` expects input as either a dictionary or a `TensorBundle` object, as discussed earlier. When a dictionary of input data (along with metadata) is passed, it is automatically wrapped into a `TensorBundle` instance. The examples below demonstrate how transformations are used in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "epochs = 4\n",
    "input_shape = (96, 96, 96, 4)\n",
    "\n",
    "\n",
    "def train_transformation(sample):\n",
    "    meta = {\"affine\": sample[\"affine\"]}\n",
    "    data = {\"image\": sample[\"image\"], \"label\": sample[\"label\"]}\n",
    "\n",
    "    pipeline = Compose(\n",
    "        [\n",
    "            ConvertToMultiChannelBasedOnBratsClasses(keys=[\"label\"]),\n",
    "            CropForeground(\n",
    "                keys=(\"image\", \"label\"),\n",
    "                source_key=\"image\",\n",
    "                k_divisible=input_shape[:3],\n",
    "            ),\n",
    "            RandSpatialCrop(\n",
    "                keys=[\"image\", \"label\"], roi_size=input_shape[:3], random_size=False\n",
    "            ),\n",
    "            RandFlip(keys=[\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
    "            RandFlip(keys=[\"image\", \"label\"], spatial_axis=[1], prob=0.5),\n",
    "            RandFlip(keys=[\"image\", \"label\"], spatial_axis=[2], prob=0.5),\n",
    "            NormalizeIntensity(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            RandShiftIntensity(keys=[\"image\"], offsets=0.10, prob=1.0),\n",
    "        ]\n",
    "    )\n",
    "    result = pipeline(data, meta)\n",
    "    return result[\"image\"], result[\"label\"]\n",
    "\n",
    "\n",
    "def val_transformation(sample):\n",
    "    meta = {\"affine\": sample[\"affine\"]}\n",
    "    data = {\"image\": sample[\"image\"], \"label\": sample[\"label\"]}\n",
    "\n",
    "    pipeline = Compose(\n",
    "        [\n",
    "            ConvertToMultiChannelBasedOnBratsClasses(keys=[\"label\"]),\n",
    "            NormalizeIntensity(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "        ]\n",
    "    )\n",
    "    result = pipeline(data, meta)\n",
    "    return result[\"image\"], result[\"label\"]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## The `tfrecord` parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_tfrecord_fn(example_proto):\n",
    "    feature_description = {\n",
    "        # Image raw data\n",
    "        \"flair_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"t1_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"t1ce_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"t2_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        # Image shape\n",
    "        \"flair_shape\": tf.io.FixedLenFeature([3], tf.int64),\n",
    "        \"t1_shape\": tf.io.FixedLenFeature([3], tf.int64),\n",
    "        \"t1ce_shape\": tf.io.FixedLenFeature([3], tf.int64),\n",
    "        \"t2_shape\": tf.io.FixedLenFeature([3], tf.int64),\n",
    "        \"label_shape\": tf.io.FixedLenFeature([3], tf.int64),\n",
    "        # Affine matrices (4x4 = 16 values)\n",
    "        \"flair_affine\": tf.io.FixedLenFeature([16], tf.float32),\n",
    "        \"t1_affine\": tf.io.FixedLenFeature([16], tf.float32),\n",
    "        \"t1ce_affine\": tf.io.FixedLenFeature([16], tf.float32),\n",
    "        \"t2_affine\": tf.io.FixedLenFeature([16], tf.float32),\n",
    "        \"label_affine\": tf.io.FixedLenFeature([16], tf.float32),\n",
    "        # Voxel Spacing (pixdim)\n",
    "        \"flair_pixdim\": tf.io.FixedLenFeature([8], tf.float32),\n",
    "        \"t1_pixdim\": tf.io.FixedLenFeature([8], tf.float32),\n",
    "        \"t1ce_pixdim\": tf.io.FixedLenFeature([8], tf.float32),\n",
    "        \"t2_pixdim\": tf.io.FixedLenFeature([8], tf.float32),\n",
    "        \"label_pixdim\": tf.io.FixedLenFeature([8], tf.float32),\n",
    "        # Filenames\n",
    "        \"flair_filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"t1_filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"t1ce_filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"t2_filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label_filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    # Decode image and label data\n",
    "    flair = tf.io.decode_raw(example[\"flair_raw\"], tf.float32)\n",
    "    t1 = tf.io.decode_raw(example[\"t1_raw\"], tf.float32)\n",
    "    t1ce = tf.io.decode_raw(example[\"t1ce_raw\"], tf.float32)\n",
    "    t2 = tf.io.decode_raw(example[\"t2_raw\"], tf.float32)\n",
    "    label = tf.io.decode_raw(example[\"label_raw\"], tf.float32)\n",
    "\n",
    "    # Reshape to original dimensions\n",
    "    flair = tf.reshape(flair, example[\"flair_shape\"])\n",
    "    t1 = tf.reshape(t1, example[\"t1_shape\"])\n",
    "    t1ce = tf.reshape(t1ce, example[\"t1ce_shape\"])\n",
    "    t2 = tf.reshape(t2, example[\"t2_shape\"])\n",
    "    label = tf.reshape(label, example[\"label_shape\"])\n",
    "\n",
    "    # Decode affine matrices\n",
    "    flair_affine = tf.reshape(example[\"flair_affine\"], (4, 4))\n",
    "    t1_affine = tf.reshape(example[\"t1_affine\"], (4, 4))\n",
    "    t1ce_affine = tf.reshape(example[\"t1ce_affine\"], (4, 4))\n",
    "    t2_affine = tf.reshape(example[\"t2_affine\"], (4, 4))\n",
    "    label_affine = tf.reshape(example[\"label_affine\"], (4, 4))\n",
    "\n",
    "    # add channel axis\n",
    "    flair = flair[..., None]\n",
    "    t1 = t1[..., None]\n",
    "    t1ce = t1ce[..., None]\n",
    "    t2 = t2[..., None]\n",
    "    image = tf.concat([flair, t1, t1ce, t2], axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"image\": image,\n",
    "        \"label\": label,\n",
    "        \"affine\": flair_affine,  # Since affine is the same for all\n",
    "    }\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_tfrecord_dataset(tfrecord_datalist, batch_size=1, shuffle=True):\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_datalist)\n",
    "    dataset = dataset.shuffle(buffer_size=100) if shuffle else dataset\n",
    "    dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(rearrange_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.map(train_transformation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        dataset = dataset.map(val_transformation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The training batch size can be set to more than 1 depending on the environment and available resources. However, we intentionally keep the validation batch size as 1 to handle variable-sized samples more flexibly. While padded or ragged batches are alternative options, a batch size of 1 ensures simplicity and consistency during evaluation, especially for 3D medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tfrecord_pattern = \"brats2020_subset/training_shard_*.tfrec\"\n",
    "datalist = sorted(\n",
    "    tf.io.gfile.glob(tfrecord_pattern),\n",
    "    key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]),\n",
    ")\n",
    "\n",
    "train_datalist = datalist[:-1]\n",
    "val_datalist = datalist[-1:]\n",
    "print(len(train_datalist), len(val_datalist))\n",
    "\n",
    "train_ds = load_tfrecord_dataset(train_datalist, batch_size=1, shuffle=True)\n",
    "val_ds = load_tfrecord_dataset(val_datalist, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**sanity check**: Fetch a single validation sample to inspect its shape and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "val_x, val_y = next(iter(val_ds))\n",
    "test_image = val_x.numpy().squeeze()\n",
    "test_mask = val_y.numpy().squeeze()\n",
    "print(test_image.shape, test_mask.shape, np.unique(test_mask))\n",
    "print(test_image.min(), test_image.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**sanity check**: Visualize the middle slice of the image and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "slice_no = test_image.shape[0] // 2\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(test_image[slice_no], cmap=\"gray\")\n",
    "ax1.set_title(f\"Image shape: {test_image.shape}\")\n",
    "ax2.imshow(test_mask[slice_no])\n",
    "ax2.set_title(f\"Label shape: {test_mask.shape}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**sanity check**: Visualize sample image and label channels at middle slice index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(f\"image shape: {test_image.shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"image channel {i}\")\n",
    "    plt.imshow(test_image[slice_no, :, :, i], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"label shape: {test_mask.shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"label channel {i}\")\n",
    "    plt.imshow(test_mask[slice_no, :, :, i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model\n",
    "\n",
    "We will be using the 3D model architecture Swin UNEt TRansformers, i.e., [`SwinUNETR`](https://arxiv.org/abs/2201.01266). It was used in the BraTS 2021 segmentation challenge by NVIDIA. The model was among the top-performing methods. It uses a Swin Transformer encoder to extract features at five different resolutions. A CNN-based decoder is connected to each resolution using skip connections.\n",
    "\n",
    "The BraTS dataset provides four input modalities: `flair`, `t1`, `t1ce`, and `t2` and three multi-label outputs: `tumor-core`, `whole-tumor`, and `enhancing-tumor`. Accordingly, we will initiate the model with `4` input channels and `3` output channels.\n",
    "\n",
    "![](https://i.imgur.com/OInMRGp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "```shell\n",
    "# # check available models\n",
    "# medicai.models.list_models()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = SwinUNETR(\n",
    "    encoder_name=\"swin_tiny_v2\",\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes,\n",
    "    classifier_activation=None,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-5,\n",
    "    ),\n",
    "    loss=BinaryDiceCELoss(\n",
    "        from_logits=True,\n",
    "        num_classes=num_classes,\n",
    "    ),\n",
    "    metrics=[\n",
    "        BinaryDiceMetric(\n",
    "            from_logits=True,\n",
    "            ignore_empty=True,\n",
    "            num_classes=num_classes,\n",
    "            name=\"dice\",\n",
    "        ),\n",
    "        BinaryDiceMetric(\n",
    "            from_logits=True,\n",
    "            ignore_empty=True,\n",
    "            target_class_ids=[0],\n",
    "            num_classes=num_classes,\n",
    "            name=\"dice_tc\",\n",
    "        ),\n",
    "        BinaryDiceMetric(\n",
    "            from_logits=True,\n",
    "            ignore_empty=True,\n",
    "            target_class_ids=[1],\n",
    "            num_classes=num_classes,\n",
    "            name=\"dice_wt\",\n",
    "        ),\n",
    "        BinaryDiceMetric(\n",
    "            from_logits=True,\n",
    "            ignore_empty=True,\n",
    "            target_class_ids=[2],\n",
    "            num_classes=num_classes,\n",
    "            name=\"dice_et\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ALERT: This `instance_describe` attributes available in medicai.\n",
    "try:\n",
    "    print(model.instance_describe())\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Callback\n",
    "\n",
    "We will be using sliding window inference callback from `medicai` to perform validation at certain interval or epoch during training. Based on the number of epoch size, we should set `interval` accordingly. For example, if epoch is set 15 and we want to evaluate model on validation set every 5 epoch, then we should set `interval` to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "swi_callback_metric = BinaryDiceMetric(\n",
    "    from_logits=True,\n",
    "    ignore_empty=True,\n",
    "    num_classes=num_classes,\n",
    "    name=\"val_dice\",\n",
    ")\n",
    "\n",
    "swi_callback = SlidingWindowInferenceCallback(\n",
    "    model,\n",
    "    dataset=val_ds,\n",
    "    metrics=swi_callback_metric,\n",
    "    num_classes=num_classes,\n",
    "    interval=2,\n",
    "    overlap=0.5,\n",
    "    roi_size=input_shape[:3],\n",
    "    sw_batch_size=4,\n",
    "    mode=\"gaussian\",\n",
    "    save_path=\"brats.model.weights.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training\n",
    "\n",
    "Set more epoch for better optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=epochs, callbacks=[swi_callback])\n",
    "\n",
    "# Commented out to observe the progress bar\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let\u2019s take a quick look at how our model performed during training. We will first print the available metrics recorded in the training history, save them to a CSV file for future reference, and then visualize them to better understand the model\u2019s learning progress over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(history_df):\n",
    "    metrics = history_df.columns\n",
    "    n_metrics = len(metrics)\n",
    "\n",
    "    n_rows = 2\n",
    "    n_cols = (n_metrics + 1) // 2  # ceiling division for columns\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 5 * n_rows))\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        plt.plot(history_df[metric], label=metric, marker=\"o\")\n",
    "        plt.title(metric)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(model.history.history.keys())\n",
    "his_csv = pd.DataFrame(model.history.history)\n",
    "his_csv.to_csv(\"brats.history.csv\")\n",
    "plot_training_history(his_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "In this [Kaggle notebook](https://www.kaggle.com/code/ipythonx/3d-brats-segmentation-in-keras-multi-gpu/) (version 5), we trained the model on the entire dataset for approximately `30` epochs. The resulting weights will be used for further evaluation. Note that the validation set used in both here and Kaggle notebook are the same: `training_shard_36.tfrec`, which contains `8` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model_weight = kagglehub.model_download(\n",
    "    \"ipythonx/bratsmodel/keras/default\", path=\"brats.model.weights.h5\"\n",
    ")\n",
    "print(\"\\nPath to model files:\", model_weight)\n",
    "\n",
    "model.load_weights(model_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In this section, we perform sliding window inference on the validation dataset and compute Dice scores for overall segmentation quality as well as specific tumor subregions:\n",
    " - Tumor Core (TC)\n",
    " - Whole Tumor (WT)\n",
    " - Enhancing Tumor (ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "swi = SlidingWindowInference(\n",
    "    model,\n",
    "    num_classes=num_classes,\n",
    "    roi_size=input_shape[:3],\n",
    "    sw_batch_size=4,\n",
    "    overlap=0.5,\n",
    "    mode=\"gaussian\",\n",
    ")\n",
    "\n",
    "dice = BinaryDiceMetric(\n",
    "    from_logits=True,\n",
    "    ignore_empty=True,\n",
    "    num_classes=num_classes,\n",
    "    name=\"dice\",\n",
    ")\n",
    "dice_tc = BinaryDiceMetric(\n",
    "    from_logits=True,\n",
    "    ignore_empty=True,\n",
    "    target_class_ids=[0],\n",
    "    num_classes=num_classes,\n",
    "    name=\"dice_tc\",\n",
    ")\n",
    "dice_wt = BinaryDiceMetric(\n",
    "    from_logits=True,\n",
    "    ignore_empty=True,\n",
    "    target_class_ids=[1],\n",
    "    num_classes=num_classes,\n",
    "    name=\"dice_wt\",\n",
    ")\n",
    "dice_et = BinaryDiceMetric(\n",
    "    from_logits=True,\n",
    "    ignore_empty=True,\n",
    "    target_class_ids=[2],\n",
    "    num_classes=num_classes,\n",
    "    name=\"dice_et\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Due to the variable size, and larger size of the validation data, we iterate over the validation dataloader. The sliding window inference handles input patches and computes the predictions for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dice.reset_state()\n",
    "dice_tc.reset_state()\n",
    "dice_wt.reset_state()\n",
    "dice_et.reset_state()\n",
    "\n",
    "for sample in val_ds:\n",
    "    x, y = sample\n",
    "    output = swi(x)\n",
    "    dice.update_state(y, output)\n",
    "    dice_tc.update_state(y, output)\n",
    "    dice_wt.update_state(y, output)\n",
    "    dice_et.update_state(y, output)\n",
    "\n",
    "dice_score = float(ops.convert_to_numpy(dice.result()))\n",
    "dice_score_tc = float(ops.convert_to_numpy(dice_tc.result()))\n",
    "dice_score_wt = float(ops.convert_to_numpy(dice_wt.result()))\n",
    "dice_score_et = float(ops.convert_to_numpy(dice_et.result()))\n",
    "\n",
    "# Commented out to observe the progress bar\n",
    "clear_output()\n",
    "\n",
    "print(f\"Dice Score: {dice_score:.4f}\")\n",
    "print(f\"Dice Score on tumor core (TC): {dice_score_tc:.4f}\")\n",
    "print(f\"Dice Score on whole tumor (WT): {dice_score_wt:.4f}\")\n",
    "print(f\"Dice Score on enhancing tumor (ET): {dice_score_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Analyse and Visualize\n",
    "\n",
    "Let's analyse the model predictions and visualize them. First, we will implement the test transformation pipeline. This is same as validation transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_transformation(sample):\n",
    "    return val_transformation(sample)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's load the `tfrecord` file and check its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "dataset = tf.data.TFRecordDataset(val_datalist[index])\n",
    "dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.map(rearrange_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "sample = next(iter(dataset))\n",
    "orig_image = sample[\"image\"]\n",
    "orig_label = sample[\"label\"]\n",
    "print(orig_image.shape, orig_label.shape, np.unique(orig_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Run the transformation to prepare the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "pre_image, pre_label = test_transformation(sample)\n",
    "print(pre_image.shape, pre_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Pass the preprocessed sample to the inference object, ensuring that a batch axis is added to the input beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_pred = swi(pre_image[None, ...])\n",
    "\n",
    "# Commented out to observe the progress bar\n",
    "clear_output()\n",
    "\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "After running inference, we remove the batch dimension and apply a `sigmoid` activation to obtain class probabilities. We then threshold the probabilities at `0.5` to generate the final binary segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_pred_logits = y_pred.squeeze(axis=0)\n",
    "y_pred_prob = ops.convert_to_numpy(ops.sigmoid(y_pred_logits))\n",
    "segment = (y_pred_prob > 0.5).astype(int)\n",
    "print(segment.shape, np.unique(segment))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We compare the ground truth (`pre_label`) and the predicted segmentation (`segment`) for each tumor sub-region. Each sub-plot shows a specific channel corresponding to a tumor type: TC, WT, and ET. Here we visualize the `80th` axial slice across the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "label_map = {0: \"TC\", 1: \"WT\", 2: \"ET\"}\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(pre_label.shape[-1]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"label channel {label_map[i]}\")\n",
    "    plt.imshow(pre_label[80, :, :, i])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"pred channel {label_map[i]}\")\n",
    "    plt.imshow(segment[80, :, :, i])\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The predicted output is a multi-channel binary map, where each channel corresponds to a specific tumor region. To visualize it against the original ground truth, we convert it into a single-channel label map. Here we assign:\n",
    "    - Label 1 for Tumor Core (TC)\n",
    "    - Label 2 for Whole Tumor (WT)\n",
    "    - Label 4 for Enhancing Tumor (ET)\n",
    "The label values are chosen to match typical conventions used in medical segmentation benchmarks like BraTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "prediction = np.zeros(\n",
    "    (segment.shape[0], segment.shape[1], segment.shape[2]), dtype=\"float32\"\n",
    ")\n",
    "prediction[segment[..., 1] == 1] = 2\n",
    "prediction[segment[..., 0] == 1] = 1\n",
    "prediction[segment[..., 2] == 1] = 4\n",
    "\n",
    "print(\"label \", orig_label.shape, np.unique(orig_label))\n",
    "print(\"predicted \", prediction.shape, np.unique(prediction))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's begin by examining the original input slices from the MRI scan. The input contains four channels corresponding to different MRI modalities:\n",
    "    - FLAIR\n",
    "    - T1\n",
    "    - T1CE (T1 with contrast enhancement)\n",
    "    - T2\n",
    "We display the same slice number across all modalities for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "slice_map = {0: \"flair\", 1: \"t1\", 2: \"t1ce\", 3: \"t2\"}\n",
    "slice_num = 75\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(orig_image.shape[-1]):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"Original channel: {slice_map[i]}\")\n",
    "    plt.imshow(orig_image[slice_num, :, :, i], cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we compare this input with the ground truth label and the predicted segmentation on the same slice. This provides visual insight into how well the model has localized and segmented the tumor regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_channels = orig_image.shape[-1]\n",
    "plt.figure(\"image\", (15, 15))\n",
    "\n",
    "# plotting image, label and prediction\n",
    "plt.subplot(3, num_channels, num_channels + 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(orig_image[slice_num, :, :, 0], cmap=\"gray\")\n",
    "\n",
    "plt.subplot(3, num_channels, num_channels + 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(orig_label[slice_num, :, :])\n",
    "\n",
    "plt.subplot(3, num_channels, num_channels + 3)\n",
    "plt.title(\"prediction\")\n",
    "plt.imshow(prediction[slice_num, :, :])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, create a clean GIF visualizer showing the input image, ground-truth label, and model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# The input volume contains large black margins, so we crop\n",
    "# the foreground region of interest (ROI).\n",
    "crop_forground = CropForeground(\n",
    "    keys=(\"image\", \"label\", \"prediction\"), source_key=\"image\"\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"image\": orig_image,\n",
    "    \"label\": orig_label[..., None],\n",
    "    \"prediction\": prediction[..., None],\n",
    "}\n",
    "results = crop_forground(data)\n",
    "crop_orig_image = results[\"image\"]\n",
    "crop_orig_label = results[\"label\"]\n",
    "crop_prediction = results[\"prediction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Prepare a visualization-friendly prediction map by remapping label values to a compact index range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "viz_pred = np.zeros_like(crop_prediction, dtype=\"uint8\")\n",
    "viz_pred[crop_prediction == 1] = 1\n",
    "viz_pred[crop_prediction == 2] = 2\n",
    "viz_pred[crop_prediction == 4] = 3\n",
    "\n",
    "# Colormap for background, tumor core, edema, and enhancing regions\n",
    "cmap = ListedColormap(\n",
    "    [\n",
    "        \"#000000\",  # background\n",
    "        \"#E57373\",  # muted red\n",
    "        \"#64B5F6\",  # muted blue\n",
    "        \"#81C784\",  # muted green\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create side-by-side views for input, label, and prediction\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "ax_img, ax_lbl, ax_pred = axes\n",
    "\n",
    "img_im = ax_img.imshow(crop_orig_image[0, :, :, 0], cmap=\"gray\")\n",
    "lbl_im = ax_lbl.imshow(\n",
    "    crop_orig_label[0], vmin=0, vmax=3, cmap=cmap, interpolation=\"nearest\"\n",
    ")\n",
    "pred_im = ax_pred.imshow(\n",
    "    viz_pred[0], vmin=0, vmax=3, cmap=cmap, interpolation=\"nearest\"\n",
    ")\n",
    "\n",
    "# Tight layout for a compact GIF\n",
    "plt.subplots_adjust(left=0.01, right=0.99, bottom=0.02, top=0.9, wspace=0.01)\n",
    "\n",
    "for ax, t in zip(axes, [\"FLAIR\", \"Label\", \"Prediction\"]):\n",
    "    ax.set_title(t, fontsize=19, pad=10)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_adjustable(\"box\")\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    img_im.set_data(crop_orig_image[i, :, :, 0])\n",
    "    lbl_im.set_data(crop_orig_label[i])\n",
    "    pred_im.set_data(crop_prediction[i])\n",
    "    fig.suptitle(f\"Slice {i}\", fontsize=14)\n",
    "    return img_im, lbl_im, pred_im\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update, frames=crop_orig_image.shape[0], interval=120\n",
    ")\n",
    "ani.save(\n",
    "    \"segmentation_slices.gif\",\n",
    "    writer=\"pillow\",\n",
    "    dpi=100,\n",
    ")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "When you open the saved GIF, you should see a visualization similar to this.\n",
    "\n",
    "![](https://i.imgur.com/CbaQGf2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [BraTS Segmentation on Multi-GPU](https://www.kaggle.com/code/ipythonx/3d-brats-segmentation-in-keras-multi-gpu)\n",
    "- [BraTS Segmentation on TPU-VM](https://www.kaggle.com/code/ipythonx/3d-brats-segmentation-in-keras-tpu-vm)\n",
    "- [BraTS .nii to TFRecord](https://www.kaggle.com/code/ipythonx/brats-nii-to-tfrecord)\n",
    "- [Covid-19 Segmentation](https://www.kaggle.com/code/ipythonx/medicai-covid-19-3d-image-segmentation)\n",
    "- [3D Multi-organ Segmentation](https://www.kaggle.com/code/ipythonx/medicai-3d-btcv-segmentation-in-keras)\n",
    "- [Spleen 3D segmentation](https://www.kaggle.com/code/ipythonx/medicai-spleen-3d-segmentation-in-keras)\n",
    "- [3D Medical Image Transformation](https://www.kaggle.com/code/ipythonx/medicai-3d-medical-image-transformation)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brain_tumor_segmentation",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}