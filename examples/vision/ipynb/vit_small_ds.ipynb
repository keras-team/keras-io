{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Train a Vision Transformer on small datasets\n",
    "\n",
    "**Author:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)<br>\n",
    "**Date created:** 2022/01/07<br>\n",
    "**Last modified:** 2022/01/10<br>\n",
    "**Description:** Training a ViT from scratch on smaller datasets with shifted patch tokenization and locality self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In the academic paper\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),\n",
    "the authors mention that Vision Transformers (ViT) are data-hungry. Therefore,\n",
    "pretraining a ViT on a large-sized dataset like JFT300M and fine-tuning\n",
    "it on medium-sized datasets (like ImageNet) is the only way to beat\n",
    "state-of-the-art Convolutional Neural Network models.\n",
    "\n",
    "The self-attention layer of ViT lacks **locality inductive bias** (the notion that\n",
    "image pixels are locally correlated and that their correlation maps are translation-invariant).\n",
    "This is the reason why ViTs need more data. On the other hand, CNNs look at images through\n",
    "spatial sliding windows, which helps them get better results with smaller datasets.\n",
    "\n",
    "In the academic paper\n",
    "[Vision Transformer for Small-Size Datasets](https://arxiv.org/abs/2112.13492v1),\n",
    "the authors set out to tackle the problem of locality inductive bias in ViTs.\n",
    "\n",
    "The main ideas are:\n",
    "\n",
    "- **Shifted Patch Tokenization**\n",
    "- **Locality Self Attention**\n",
    "\n",
    "This example implements the ideas of the paper. A large part of this\n",
    "example is inspired from\n",
    "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
    "\n",
    "_Note_: This example requires TensorFlow 2.6 or higher, as well as\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons), which can be\n",
    "installed using the following command:\n",
    "\n",
    "```python\n",
    "pip install -qq -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Setting seed for reproducibiltiy\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 100\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configure the hyperparameters\n",
    "\n",
    "The hyperparameters are different from the paper. Feel free to tune\n",
    "the hyperparameters yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 72\n",
    "PATCH_SIZE = 6\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 50\n",
    "\n",
    "# ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "TRANSFORMER_LAYERS = 8\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "MLP_HEAD_UNITS = [2048, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Use data augmentation\n",
    "\n",
    "A snippet from the paper:\n",
    "\n",
    "*\"According to DeiT, various techniques are required to effectively\n",
    "train ViTs. Thus, we applied data augmentations such as CutMix, Mixup,\n",
    "Auto Augment, Repeated Augment to all models.\"*\n",
    "\n",
    "In this example, we will focus solely on the novelty of the approach\n",
    "and not on reproducing the paper results. For this reason, we\n",
    "don't use the mentioned data augmentation schemes. Please feel\n",
    "free to add to or remove from the augmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement Shifted Patch Tokenization\n",
    "\n",
    "In a ViT pipeline, the input images are divided into patches that are\n",
    "then linearly projected into tokens. Shifted patch tokenization (STP)\n",
    "is introduced to combat the low receptive field of ViTs. The steps\n",
    "for Shifted Patch Tokenization are as follows:\n",
    "\n",
    "- Start with an image.\n",
    "- Shift the image in diagonal directions.\n",
    "- Concat the diagonally shifted images with the original image.\n",
    "- Extract patches of the concatenated images.\n",
    "- Flatten the spatial dimension of all patches.\n",
    "- Layer normalize the flattened patches and then project it.\n",
    "\n",
    "| ![Shifted Patch Toekenization](https://i.imgur.com/bUnHxd0.png) |\n",
    "| :--: |\n",
    "| Shifted Patch Tokenization [Source](https://arxiv.org/abs/2112.13492v1) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ShiftedPatchTokenization(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        projection_dim=PROJECTION_DIM,\n",
    "        vanilla=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch = patch_size // 2\n",
    "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "    def crop_shift_pad(self, images, mode):\n",
    "        # Build the diagonally shifted images\n",
    "        if mode == \"left-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = 0\n",
    "            shift_width = 0\n",
    "        elif mode == \"left-down\":\n",
    "            crop_height = 0\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = 0\n",
    "        elif mode == \"right-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = 0\n",
    "            shift_height = 0\n",
    "            shift_width = self.half_patch\n",
    "        else:\n",
    "            crop_height = 0\n",
    "            crop_width = 0\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = self.half_patch\n",
    "\n",
    "        # Crop the shifted images and pad them\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            images,\n",
    "            offset_height=crop_height,\n",
    "            offset_width=crop_width,\n",
    "            target_height=self.image_size - self.half_patch,\n",
    "            target_width=self.image_size - self.half_patch,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=shift_height,\n",
    "            offset_width=shift_width,\n",
    "            target_height=self.image_size,\n",
    "            target_width=self.image_size,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, images):\n",
    "        if not self.vanilla:\n",
    "            # Concat the shifted images with the original image\n",
    "            images = tf.concat(\n",
    "                [\n",
    "                    images,\n",
    "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "        # Patchify the images and flatten it\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        flat_patches = self.flatten_patches(patches)\n",
    "        if not self.vanilla:\n",
    "            # Layer normalize the flat patches and linearly project it\n",
    "            tokens = self.layer_norm(flat_patches)\n",
    "            tokens = self.projection(tokens)\n",
    "        else:\n",
    "            # Linearly project the flat patches\n",
    "            tokens = self.projection(flat_patches)\n",
    "        return (tokens, patches)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualize the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Get a random image from the training dataset\n",
    "# and resize the image\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    ")\n",
    "\n",
    "# Vanilla patch maker: This takes an image and divides into\n",
    "# patches as in the original ViT paper\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "count = 1\n",
    "plt.figure(figsize=(4, 4))\n",
    "for row in range(n):\n",
    "    for col in range(n):\n",
    "        plt.subplot(n, n, count)\n",
    "        count = count + 1\n",
    "        image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Shifted Patch Tokenization: This layer takes the image, shifts it\n",
    "# diagonally and then extracts patches from the concatinated images\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "shifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\n",
    "for index, name in enumerate(shifted_images):\n",
    "    print(name)\n",
    "    count = 1\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for row in range(n):\n",
    "        for col in range(n):\n",
    "            plt.subplot(n, n, count)\n",
    "            count = count + 1\n",
    "            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))\n",
    "            plt.imshow(image[..., 3 * index : 3 * index + 3])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "This layer accepts projected patches and then adds positional\n",
    "information to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_patches = encoded_patches + encoded_positions\n",
    "        return encoded_patches\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement Locality Self Attention\n",
    "\n",
    "The regular attention equation is stated below.\n",
    "\n",
    "| ![Equation of attention](https://miro.medium.com/max/396/1*P9sV1xXM10t943bXy_G9yg.png) |\n",
    "| :--: |\n",
    "| [Source](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634) |\n",
    "\n",
    "The attention module takes a query, key, and value. First, we compute the\n",
    "similarity between the query and key via a dot product. Then, the result\n",
    "is scaled by the square root of the key dimension. The scaling prevents\n",
    "the softmax function from having an overly small gradient. Softmax is then\n",
    "applied to the scaled dot product to produce the attention weights.\n",
    "The value is then modulated via the attention weights.\n",
    "\n",
    "In self-attention, query, key and value come from the same input.\n",
    "The dot product would result in large self-token relations rather than\n",
    "inter-token relations. This also means that the softmax gives higher\n",
    "probabilities to self-token relations than the inter-token relations.\n",
    "To combat this, the authors propose masking the diagonal of the dot product.\n",
    "This way, we force the attention module to pay more attention to the\n",
    "inter-token relations.\n",
    "\n",
    "The scaling factor is a constant in the regular attention module.\n",
    "This acts like a temperature term that can modulate the softmax function.\n",
    "The authors suggest a learnable temperature term instead of a constant.\n",
    "\n",
    "| ![Implementation of LSA](https://i.imgur.com/GTV99pk.png) |\n",
    "| :--: |\n",
    "| Locality Self Attention [Source](https://arxiv.org/abs/2112.13492v1) |\n",
    "\n",
    "The above two pointers make the Locality Self Attention. We have subclassed the\n",
    "[`layers.MultiHeadAttention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)\n",
    "and implemented the trainable temperature. The attention mask is built\n",
    "at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is\n",
    "        # the square root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Build the diagonal attention mask\n",
    "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build the ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier(vanilla=False):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder()(tokens)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        if not vanilla:\n",
    "            attention_output = MultiHeadAttentionLSA(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        else:\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(NUM_CLASSES)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Some code is taken from:\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
    "    warmup_epoch_percentage = 0.10\n",
    "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "    scheduled_lrs = WarmUpCosine(\n",
    "        learning_rate_base=LEARNING_RATE,\n",
    "        total_steps=total_steps,\n",
    "        warmup_learning_rate=0.0,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.1,\n",
    "    )\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Run experiments with the vanilla ViT\n",
    "vit = create_vit_classifier(vanilla=True)\n",
    "history = run_experiment(vit)\n",
    "\n",
    "# Run experiments with the Shifted Patch Tokenization and\n",
    "# Locality Self Attention modified ViT\n",
    "vit_sl = create_vit_classifier(vanilla=False)\n",
    "history = run_experiment(vit_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Final Notes\n",
    "\n",
    "With the help of Shifted Patch Tokenization and Locality Self Attention,\n",
    "we were able to get ~**3-4%** top-1 accuracy gains on CIFAR100.\n",
    "\n",
    "The ideas on Shifted Patch Tokenization and Locality Self Attention\n",
    "are very intuitive and easy to implement. The authors also ablates of\n",
    "different shifting strategies for Shifted Patch Tokenization in the\n",
    "supplementary of the paper.\n",
    "\n",
    "I would like to thank [Jarvislabs.ai](https://jarvislabs.ai/) for\n",
    "generously helping with GPU credits.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/vit_small_ds_v2) ",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/vit-small-ds)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vit_small_ds",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}