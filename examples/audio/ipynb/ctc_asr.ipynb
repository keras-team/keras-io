{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Automatic Speech Recognition using CTC\n",
    "\n",
    "**Authors:** [Mohamed Reda Bouadjenek](https://rbouadjenek.github.io/) and [Ngoc Dung Huynh](https://www.linkedin.com/in/parkerhuynh/)<br>\n",
    "**Date created:** 2021/09/26<br>\n",
    "**Last modified:** 2021/09/26<br>\n",
    "**Description:** Training a CTC-based model for automatic speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Speech recognition is an interdisciplinary subfield of computer science\n",
    "and computational linguistics that develops methodologies and technologies\n",
    "that enable the recognition and translation of spoken language into text\n",
    "by computers. It is also known as automatic speech recognition (ASR),\n",
    "computer speech recognition or speech to text (STT). It incorporates\n",
    "knowledge and research in the computer science, linguistics and computer\n",
    "engineering fields.\n",
    "\n",
    "This demonstration shows how to combine a 2D CNN, RNN and a Connectionist\n",
    "Temporal Classification (CTC) loss to build an ASR. CTC is an algorithm\n",
    "used to train deep neural networks in speech recognition, handwriting\n",
    "recognition and other sequence problems. CTC is used when  we donâ€™t know\n",
    "how the input aligns with the output (how the characters in the transcript\n",
    "align to the audio). The model we create is similar to\n",
    "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html).\n",
    "\n",
    "We will use the LJSpeech dataset from the\n",
    "[LibriVox](https://librivox.org/) project. It consists of short\n",
    "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
    "\n",
    "We will evaluate the quality of the model using\n",
    "[Word Error Rate (WER)](https://en.wikipedia.org/wiki/Word_error_rate).\n",
    "WER is obtained by adding up\n",
    "the substitutions, insertions, and deletions that occur in a sequence of\n",
    "recognized words. Divide that number by the total number of words originally\n",
    "spoken. The result is the WER. To get the WER score you need to install the\n",
    "[jiwer](https://pypi.org/project/jiwer/) package. You can use the following command line:\n",
    "\n",
    "```\n",
    "pip install jiwer\n",
    "```\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n",
    "- [Speech recognition](https://en.wikipedia.org/wiki/Speech_recognition)\n",
    "- [Sequence Modeling With CTC](https://distill.pub/2017/ctc/)\n",
    "- [DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from jiwer import wer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the LJSpeech Dataset\n",
    "\n",
    "Let's download the [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/).\n",
    "The dataset contains 13,100 audio files as `wav` files in the `/wavs/` folder.\n",
    "The label (transcript) for each audio file is a string\n",
    "given in the `metadata.csv` file. The fields are:\n",
    "\n",
    "- **ID**: this is the name of the corresponding .wav file\n",
    "- **Transcription**: words spoken by the reader (UTF-8)\n",
    "- **Normalized transcription**: transcription with numbers,\n",
    "ordinals, and monetary units expanded into full words (UTF-8).\n",
    "\n",
    "For this demo we will use on the \"Normalized transcription\" field.\n",
    "\n",
    "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22,050 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\n",
    "wavs_path = data_path + \"/wavs/\"\n",
    "metadata_path = data_path + \"/metadata.csv\"\n",
    "\n",
    "\n",
    "# Read metadata file and parse it\n",
    "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
    "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
    "metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
    "metadata_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We now split the data into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "split = int(len(metadata_df) * 0.90)\n",
    "df_train = metadata_df[:split]\n",
    "df_val = metadata_df[split:]\n",
    "\n",
    "print(f\"Size of the training set: {len(df_train)}\")\n",
    "print(f\"Size of the training set: {len(df_val)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "We first prepare the vocabulary to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# The set of characters accepted in the transcription.\n",
    "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
    "# Mapping characters to integers\n",
    "char_to_num = layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we create the function that describes the transformation that we apply to each\n",
    "element of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# An integer scalar Tensor. The window length in samples.\n",
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 384\n",
    "\n",
    "\n",
    "def encode_single_sample(wav_file, label):\n",
    "    ###########################################\n",
    "    ##  Process the Audio\n",
    "    ##########################################\n",
    "    # 1. Read wav file\n",
    "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
    "    # 2. Decode the wav file\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = keras.ops.squeeze(audio, axis=-1)\n",
    "    # 3. Change type to float\n",
    "    audio = keras.ops.cast(audio, tf.float32)\n",
    "    # 4. Get the spectrogram\n",
    "    spectrogram = tf.signal.stft(\n",
    "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
    "    )\n",
    "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "    # 6. normalisation\n",
    "    means = keras.ops.mean(spectrogram, 1, keepdims=True)\n",
    "    stddevs = keras.ops.std(spectrogram, 1, keepdims=True)\n",
    "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "    ###########################################\n",
    "    ##  Process the label\n",
    "    ##########################################\n",
    "    # 7. Convert label to Lower case\n",
    "    label = tf.strings.lower(label)\n",
    "    # 8. Split the label\n",
    "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "    # 9. Map the characters in label to numbers\n",
    "    label = char_to_num(label)\n",
    "    # 10. Return a dict as our model is expecting two inputs\n",
    "    return spectrogram, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Creating `Dataset` objects\n",
    "\n",
    "We create a `tf.data.Dataset` object that yields\n",
    "the transformed elements, in the same order as they\n",
    "appeared in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Define the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualize the data\n",
    "\n",
    "Let's visualize an example in our dataset, including the\n",
    "audio clip, the spectrogram and the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    spectrogram = batch[0][0].numpy()\n",
    "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
    "    label = batch[1][0]\n",
    "    # Spectrogram\n",
    "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.imshow(spectrogram, vmax=1)\n",
    "    ax.set_title(label)\n",
    "    ax.axis(\"off\")\n",
    "    # Wav\n",
    "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = audio.numpy()\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    plt.plot(audio)\n",
    "    ax.set_title(\"Signal Wave\")\n",
    "    ax.set_xlim(0, len(audio))\n",
    "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model\n",
    "\n",
    "We now define our model. We will define a model similar to\n",
    "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
    "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
    "    # Model's input\n",
    "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
    "    # Expand the dimension to use 2D CNN.\n",
    "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
    "    # Convolution layer 1\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 41],\n",
    "        strides=[2, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_1\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "    # Convolution layer 2\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 21],\n",
    "        strides=[1, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_2\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "    # Reshape the resulted volume to feed the RNNs layers\n",
    "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
    "    # RNN layers\n",
    "    for i in range(1, rnn_layers + 1):\n",
    "        recurrent = layers.GRU(\n",
    "            units=rnn_units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            reset_after=True,\n",
    "            name=f\"gru_{i}\",\n",
    "        )\n",
    "        x = layers.Bidirectional(\n",
    "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
    "        )(x)\n",
    "        if i < rnn_layers:\n",
    "            x = layers.Dropout(rate=0.5)(x)\n",
    "    # Dense layer\n",
    "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
    "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    # Classification layer\n",
    "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
    "    # Model\n",
    "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt, loss=keras.losses.CTC())\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=512,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/keras-team/keras/blob/ec67b760ba25e1ccc392d288f7d8c6e9e153eea2/keras/legacy/backend.py#L715-L739\n",
    "\n",
    "\n",
    "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n",
    "    input_shape = tf.shape(y_pred)\n",
    "    num_samples, num_steps = input_shape[0], input_shape[1]\n",
    "    y_pred = tf.math.log(tf.transpose(y_pred, perm=[1, 0, 2]) + keras.backend.epsilon())\n",
    "    input_length = tf.cast(input_length, tf.int32)\n",
    "\n",
    "    if greedy:\n",
    "        (decoded, log_prob) = tf.nn.ctc_greedy_decoder(\n",
    "            inputs=y_pred, sequence_length=input_length\n",
    "        )\n",
    "    else:\n",
    "        (decoded, log_prob) = tf.compat.v1.nn.ctc_beam_search_decoder(\n",
    "            inputs=y_pred,\n",
    "            sequence_length=input_length,\n",
    "            beam_width=beam_width,\n",
    "            top_paths=top_paths,\n",
    "        )\n",
    "    decoded_dense = []\n",
    "    for st in decoded:\n",
    "        st = tf.SparseTensor(st.indices, st.values, (num_samples, num_steps))\n",
    "        decoded_dense.append(tf.sparse.to_dense(sp_input=st, default_value=-1))\n",
    "    return (decoded_dense, log_prob)\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for result in results:\n",
    "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(result)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch in self.dataset:\n",
    "            X, y = batch\n",
    "            batch_predictions = model.predict(X)\n",
    "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "            predictions.extend(batch_predictions)\n",
    "            for label in y:\n",
    "                label = (\n",
    "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "                )\n",
    "                targets.append(label)\n",
    "        wer_score = wer(targets, predictions)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "        print(\"-\" * 100)\n",
    "        for i in np.random.randint(0, len(predictions), 2):\n",
    "            print(f\"Target    : {targets[i]}\")\n",
    "            print(f\"Prediction: {predictions[i]}\")\n",
    "            print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs.\n",
    "epochs = 1\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(validation_dataset)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's check results on more validation samples\n",
    "predictions = []\n",
    "targets = []\n",
    "for batch in validation_dataset:\n",
    "    X, y = batch\n",
    "    batch_predictions = model.predict(X)\n",
    "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "    predictions.extend(batch_predictions)\n",
    "    for label in y:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        targets.append(label)\n",
    "wer_score = wer(targets, predictions)\n",
    "print(\"-\" * 100)\n",
    "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "print(\"-\" * 100)\n",
    "for i in np.random.randint(0, len(predictions), 5):\n",
    "    print(f\"Target    : {targets[i]}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In practice, you should train for around 50 epochs or more. Each epoch\n",
    "takes approximately 5-6mn using a `GeForce RTX 2080 Ti` GPU.\n",
    "The model we trained at 50 epochs has a `Word Error Rate (WER) â‰ˆ 16% to 17%`.\n",
    "\n",
    "Some of the transcriptions around epoch 50:\n",
    "\n",
    "**Audio file: LJ017-0009.wav**\n",
    "```\n",
    "- Target    : sir thomas overbury was undoubtedly poisoned by lord rochester in the reign\n",
    "of james the first\n",
    "- Prediction: cer thomas overbery was undoubtedly poisoned by lordrochester in the reign\n",
    "of james the first\n",
    "```\n",
    "\n",
    "**Audio file: LJ003-0340.wav**\n",
    "```\n",
    "- Target    : the committee does not seem to have yet understood that newgate could be\n",
    "only and properly replaced\n",
    "- Prediction: the committee does not seem to have yet understood that newgate could be\n",
    "only and proberly replace\n",
    "```\n",
    "\n",
    "**Audio file: LJ011-0136.wav**\n",
    "```\n",
    "- Target    : still no sentence of death was carried out for the offense and in eighteen\n",
    "thirtytwo\n",
    "- Prediction: still no sentence of death was carried out for the offense and in eighteen\n",
    "thirtytwo\n",
    "```\n",
    "\n",
    "Example available on HuggingFace.\n",
    "| Trained Model | Demo |\n",
    "| :--: | :--: |\n",
    "| [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Model-CTC%20ASR-black.svg)](https://huggingface.co/keras-io/ctc_asr) | [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-CTC%20ASR-black.svg)](https://huggingface.co/spaces/keras-io/ctc_asr) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ctc_asr",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
